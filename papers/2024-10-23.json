{
  "date": "2024-10-23",
  "papers": [
    {
      "title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction",
      "url": "https://huggingface.co/papers/2410.17247",
      "authors": [
        "Long Xing",
        "Xiaoyi Dong",
        "Jiajie Lu",
        "Pan Zhang",
        "Feng Wu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17247.pdf",
      "abstract": "In large vision-language models (LVLMs), images serve as inputs that carry a\nwealth of information. As the idiom \"A picture is worth a thousand words\"\nimplies, representing a single image in current LVLMs can require hundreds or\neven thousands of tokens. This results in significant computational costs,\nwhich grow quadratically as input image resolution increases, thereby severely\nimpacting the efficiency of both training and inference. Previous approaches\nhave attempted to reduce the number of image tokens either before or within the\nearly layers of LVLMs. However, these strategies inevitably result in the loss\nof crucial image information, ultimately diminishing model performance. To\naddress this challenge, we conduct an empirical study revealing that all visual\ntokens are necessary for LVLMs in the shallow layers, and token redundancy\nprogressively increases in the deeper layers of the model. To this end, we\npropose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost\ntheir efficiency in both training and inference with neglectable performance\nloss. Specifically, we partition the LVLM into several stages and drop part of\nthe image tokens at the end of each stage with a pre-defined ratio, creating\npyramid-like visual tokens across model layers. The dropping is based on a\nlightweight similarity calculation with a negligible time overhead. Extensive\nexperiments demonstrate that PyramidDrop can achieve a 40% training time and\n55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance.\nBesides, the PyramidDrop could also serve as a plug-and-play strategy for\ninference acceleration without training, with better performance and lower\ninference cost than counterparts. We hope that the insights and approach\nintroduced by PyramidDrop will inspire future research to further investigate\nthe role of image tokens in LVLMs.",
      "upvotes": 43
    },
    {
      "title": "SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes",
      "url": "https://huggingface.co/papers/2410.17249",
      "authors": [
        "Cheng-De Fan",
        "Yi-Ruei Liu",
        "Jiun-Long Huang",
        "Yu-Chee Tseng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17249.pdf",
      "abstract": "We present SpectroMotion, a novel approach that combines 3D Gaussian\nSplatting (3DGS) with physically-based rendering (PBR) and deformation fields\nto reconstruct dynamic specular scenes. Previous methods extending 3DGS to\nmodel dynamic scenes have struggled to accurately represent specular surfaces.\nOur method addresses this limitation by introducing a residual correction\ntechnique for accurate surface normal computation during deformation,\ncomplemented by a deformable environment map that adapts to time-varying\nlighting conditions. We implement a coarse-to-fine training strategy that\nsignificantly enhances both scene geometry and specular color prediction. We\ndemonstrate that our model outperforms prior methods for view synthesis of\nscenes containing dynamic specular objects and that it is the only existing\n3DGS method capable of synthesizing photorealistic real-world dynamic specular\nscenes, outperforming state-of-the-art methods in rendering complex, dynamic,\nand specular scenes.",
      "upvotes": 39
    },
    {
      "title": "Aligning Large Language Models via Self-Steering Optimization",
      "url": "https://huggingface.co/papers/2410.17131",
      "authors": [
        "Bowen Yu",
        "Le Sun"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17131.pdf",
      "abstract": "Automated alignment develops alignment systems with minimal human\nintervention. The key to automated alignment lies in providing learnable and\naccurate preference signals for preference learning without human annotation.\nIn this paper, we introduce Self-Steering Optimization (SSO), an algorithm\nthat autonomously generates high-quality preference signals based on predefined\nprinciples during iterative training, eliminating the need for manual\nannotation. SSO maintains the accuracy of signals by ensuring a consistent\ngap between chosen and rejected responses while keeping them both on-policy to\nsuit the current policy model's learning capacity. SSO can benefit the online\nand offline training of the policy model, as well as enhance the training of\nreward models. We validate the effectiveness of SSO with two foundation\nmodels, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy\npreference signals throughout iterative training. Without any manual annotation\nor external models, SSO leads to significant performance improvements across\nsix subjective or objective benchmarks. Besides, the preference data generated\nby SSO significantly enhanced the performance of the reward model on\nRewardbench. Our work presents a scalable approach to preference optimization,\npaving the way for more efficient and effective automated alignment.",
      "upvotes": 19
    },
    {
      "title": "Improve Vision Language Model Chain-of-thought Reasoning",
      "url": "https://huggingface.co/papers/2410.16198",
      "authors": [
        "Bowen Zhang",
        "Yanghao Li",
        "Zhiqing Sun",
        "Zhe Gan",
        "Yinfei Yang",
        "Ruoming Pang",
        "Yiming Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16198.pdf",
      "abstract": "Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial\nfor improving interpretability and trustworthiness. However, current training\nrecipes lack robust CoT reasoning data, relying on datasets dominated by short\nannotations with minimal rationales. In this work, we show that training VLM on\nshort answers does not generalize well to reasoning tasks that require more\ndetailed responses. To address this, we propose a two-fold approach. First, we\ndistill rationales from GPT-4o model to enrich the training data and fine-tune\nVLMs, boosting their CoT performance. Second, we apply reinforcement learning\nto further calibrate reasoning quality. Specifically, we construct positive\n(correct) and negative (incorrect) pairs of model-generated reasoning chains,\nby comparing their predictions with annotated short answers. Using this\npairwise data, we apply the Direct Preference Optimization algorithm to refine\nthe model's reasoning abilities. Our experiments demonstrate significant\nimprovements in CoT reasoning on benchmark datasets and better generalization\nto direct answer prediction as well. This work emphasizes the importance of\nincorporating detailed rationales in training and leveraging reinforcement\nlearning to strengthen the reasoning capabilities of VLMs.",
      "upvotes": 17
    },
    {
      "title": "xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs",
      "url": "https://huggingface.co/papers/2410.16267",
      "authors": [
        "Honglu Zhou",
        "Shrikant Kendre",
        "Can Qin",
        "Le Xue",
        "Manli Shu",
        "Silvio Savarese",
        "Ran Xu",
        "Caiming Xiong",
        "Juan Carlos Niebles"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16267.pdf",
      "abstract": "We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for\nvideos, particularly designed to efficiently capture temporal information over\nmultiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in\naddition to the conventional visual tokenizer, which maps a sequence of tokens\nover multiple frames into a compact set of visual tokens. This enables\nBLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32\nvs. 4608 tokens). We explore different types of temporal encoders, including\nlearnable spatio-temporal pooling as well as sequential models like Token\nTuring Machines. We experimentally confirm that BLIP-3-Video obtains video\nquestion-answering accuracies comparable to much larger state-of-the-art models\n(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using\nfewer visual tokens. The project website is at\nhttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html",
      "upvotes": 14
    },
    {
      "title": "Mitigating Object Hallucination via Concentric Causal Attention",
      "url": "https://huggingface.co/papers/2410.15926",
      "authors": [
        "Yiheng Li",
        "Ivan Laptev",
        "Shijian Lu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15926.pdf",
      "abstract": "Recent Large Vision Language Models (LVLMs) present remarkable zero-shot\nconversational and reasoning capabilities given multimodal queries.\nNevertheless, they suffer from object hallucination, a phenomenon where LVLMs\nare prone to generate textual responses not factually aligned with image\ninputs. Our pilot study reveals that object hallucination is closely tied with\nRotary Position Encoding (RoPE), a widely adopted positional dependency\nmodeling design in existing LVLMs. Due to the long-term decay in RoPE, LVLMs\ntend to hallucinate more when relevant visual cues are distant from instruction\ntokens in the multimodal input sequence. Additionally, we observe a similar\neffect when reversing the sequential order of visual tokens during multimodal\nalignment. Our tests indicate that long-term decay in RoPE poses challenges to\nLVLMs while capturing visual-instruction interactions across long distances. We\npropose Concentric Causal Attention (CCA), a simple yet effective positional\nalignment strategy that mitigates the impact of RoPE long-term decay in LVLMs\nby naturally reducing relative distance between visual and instruction tokens.\nWith CCA, visual tokens can better interact with instruction tokens, thereby\nenhancing model's perception capability and alleviating object hallucination.\nWithout bells and whistles, our positional alignment method surpasses existing\nhallucination mitigation strategies by large margins on multiple object\nhallucination benchmarks.",
      "upvotes": 14
    },
    {
      "title": "LLM-based Optimization of Compound AI Systems: A Survey",
      "url": "https://huggingface.co/papers/2410.16392",
      "authors": [
        "Matthieu Lin",
        "Jenny Sheng",
        "Andrew Zhao",
        "Yiran Wu",
        "Huan Liu",
        "Jun Liu",
        "Gao Huang",
        "Yong-Jin Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16392.pdf",
      "abstract": "In a compound AI system, components such as an LLM call, a retriever, a code\ninterpreter, or tools are interconnected. The system's behavior is primarily\ndriven by parameters such as instructions or tool definitions. Recent\nadvancements enable end-to-end optimization of these parameters using an LLM.\nNotably, leveraging an LLM as an optimizer is particularly efficient because it\navoids gradient computation and can generate complex code and instructions.\nThis paper presents a survey of the principles and emerging trends in LLM-based\noptimization of compound AI systems. It covers archetypes of compound AI\nsystems, approaches to LLM-based end-to-end optimization, and insights into\nfuture directions and broader impacts. Importantly, this survey uses concepts\nfrom program analysis to provide a unified view of how an LLM optimizer is\nprompted to optimize a compound AI system. The exhaustive list of paper is\nprovided at\nhttps://github.com/linyuhongg/LLM-based-Optimization-of-Compound-AI-Systems.",
      "upvotes": 13
    },
    {
      "title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation",
      "url": "https://huggingface.co/papers/2410.17250",
      "authors": [
        "Kazuki Egashira",
        "Xiang Yue",
        "Kiyoharu Aizawa"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17250.pdf",
      "abstract": "Accelerating research on Large Multimodal Models (LMMs) in non-English\nlanguages is crucial for enhancing user experiences across broader populations.\nIn this paper, we introduce JMMMU (Japanese MMMU), the first large-scale\nJapanese benchmark designed to evaluate LMMs on expert-level tasks based on the\nJapanese cultural context. To facilitate comprehensive culture-aware\nevaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)\nsubset, where the culture-independent subjects (e.g., Math) are selected and\ntranslated into Japanese, enabling one-to-one comparison with its English\ncounterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly\ncrafted subjects that reflect Japanese cultural context. Using the CA subset,\nwe observe performance drop in many LMMs when evaluated in Japanese, which is\npurely attributable to language variation. Using the CS subset, we reveal their\ninadequate Japanese cultural understanding. Further, by combining both subsets,\nwe identify that some LMMs perform well on the CA subset but not on the CS\nsubset, exposing a shallow understanding of the Japanese language that lacks\ndepth in cultural understanding. We hope this work will not only help advance\nLMM performance in Japanese but also serve as a guideline to create\nhigh-standard, culturally diverse benchmarks for multilingual LMM development.\nThe project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.",
      "upvotes": 12
    },
    {
      "title": "MiniPLM: Knowledge Distillation for Pre-Training Language Models",
      "url": "https://huggingface.co/papers/2410.17215",
      "authors": [
        "Hao Zhou",
        "Jie Zhou",
        "Minlie Huang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17215.pdf",
      "abstract": "Knowledge distillation (KD) is widely used to train small, high-performing\nstudent language models (LMs) using large teacher LMs. While effective in\nfine-tuning, KD during pre-training faces challenges in efficiency,\nflexibility, and effectiveness. Existing methods either incur high\ncomputational costs due to online teacher inference, require tokenization\nmatching between teacher and student LMs, or risk losing the difficulty and\ndiversity of the teacher-generated training data. To address these issues, we\npropose MiniPLM, a KD framework for pre-training LMs by refining the training\ndata distribution with the teacher's knowledge. For efficiency, MiniPLM\nperforms offline teacher LM inference, allowing KD for multiple student LMs\nwithout adding training-time costs. For flexibility, MiniPLM operates solely on\nthe training corpus, enabling KD across model families. For effectiveness,\nMiniPLM leverages the differences between large and small LMs to enhance the\ndifficulty and diversity of the training data, helping student LMs acquire\nversatile and sophisticated knowledge. Extensive experiments demonstrate that\nMiniPLM boosts the student LMs' performance on 9 widely used downstream tasks,\nimproves the language modeling capabilities, and reduces pre-training\ncomputation. The benefit of MiniPLM extends to large pre-training scales,\nevidenced by the extrapolation of the scaling curves. Further analysis reveals\nthat MiniPLM supports KD across model families and enhances the utilization of\npre-training data. Our model, code, and data are available at\nhttps://github.com/thu-coai/MiniPLM.",
      "upvotes": 12
    },
    {
      "title": "EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search",
      "url": "https://huggingface.co/papers/2410.14649",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.14649.pdf",
      "abstract": "The high computational costs of large language models (LLMs) have led to a\nflurry of research on LLM compression, via methods such as quantization,\nsparsification, or structured pruning. A new frontier in this area is given by\ndynamic, non-uniform compression methods, which adjust the compression\nlevels (e.g., sparsity) per-block or even per-layer in order to minimize\naccuracy loss, while guaranteeing a global compression threshold. Yet, current\nmethods rely on heuristics for identifying the \"importance\" of a given layer\ntowards the loss, based on assumptions such as error monotonicity, i.e.\nthat the end-to-end model compression error is proportional to the sum of\nlayer-wise errors. In this paper, we revisit this area, and propose a new and\ngeneral approach for dynamic compression that is provably optimal in a given\ninput range. We begin from the motivating observation that, in general,\nerror monotonicity does not hold for LLMs: compressed models with lower\nsum of per-layer errors can perform worse than models with higher error\nsums. To address this, we propose a new general evolutionary framework for\ndynamic LLM compression called EvoPress, which has provable convergence, and\nlow sample and evaluation complexity. We show that these theoretical guarantees\nlead to highly competitive practical performance for dynamic compression of\nLlama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art\nresults across all compression approaches: structural pruning (block/layer\ndropping), unstructured sparsity, as well as quantization with dynamic\nbitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress.",
      "upvotes": 7
    },
    {
      "title": "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes",
      "url": "https://huggingface.co/papers/2410.16930",
      "authors": [
        "Jonathan Kropko",
        "Thomas Hartvigsen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16930.pdf",
      "abstract": "Math reasoning is a highly active area of Large Language Model (LLM) research\nbecause it is a hallmark of artificial intelligence. However, few works have\nexplored how math reasoning is encoded within LLM parameters and if it is a\nskill that can be isolated within a model. Doing so could allow targeted\nintervention to improve math performance without altering non-math behavior and\nfoster understanding of how models encode math reasoning. We introduce Math\nNeurosurgery (MathNeuro), a method for isolating math-specific parameters in\nLLMs using only forward passes. MathNeuro builds on existing work by using\nweights and activations to calculate parameter importance, but isolates\nmath-specific parameters by removing those important for general language\ntasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning\nability without destroying its general language ability. Scaling these\nparameters by a small constant improves a pretrained or instruction-tuned LLM's\nperformance by 4-17% on GSM8K while leaving non-math behavior unaltered.\nMathNeuro is also data efficient: most of its effectiveness holds when\nidentifying math-specific parameters using a single sample. MathNeuro\nhighlights the potential for future work to intervene on math-specific\nparameters.",
      "upvotes": 5
    },
    {
      "title": "Frontiers in Intelligent Colonoscopy",
      "url": "https://huggingface.co/papers/2410.17241",
      "authors": [
        "Ge-Peng Ji",
        "Jingyi Liu",
        "Nick Barnes",
        "Fahad Shahbaz Khan",
        "Salman Khan",
        "Deng-Ping Fan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17241.pdf",
      "abstract": "Colonoscopy is currently one of the most sensitive screening methods for\ncolorectal cancer. This study investigates the frontiers of intelligent\ncolonoscopy techniques and their prospective implications for multimodal\nmedical applications. With this goal, we begin by assessing the current\ndata-centric and model-centric landscapes through four tasks for colonoscopic\nscene perception, including classification, detection, segmentation, and\nvision-language understanding. This assessment enables us to identify\ndomain-specific challenges and reveals that multimodal research in colonoscopy\nremains open for further exploration. To embrace the coming multimodal era, we\nestablish three foundational initiatives: a large-scale multimodal instruction\ntuning dataset ColonINST, a colonoscopy-designed multimodal language model\nColonGPT, and a multimodal benchmark. To facilitate ongoing monitoring of this\nrapidly evolving field, we provide a public website for the latest updates:\nhttps://github.com/ai4colonoscopy/IntelliScope.",
      "upvotes": 2
    },
    {
      "title": "3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors",
      "url": "https://huggingface.co/papers/2410.16266",
      "authors": [
        "Siyu Huang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16266.pdf",
      "abstract": "Novel-view synthesis aims to generate novel views of a scene from multiple\ninput images or videos, and recent advancements like 3D Gaussian splatting\n(3DGS) have achieved notable success in producing photorealistic renderings\nwith efficient pipelines. However, generating high-quality novel views under\nchallenging settings, such as sparse input views, remains difficult due to\ninsufficient information in under-sampled areas, often resulting in noticeable\nartifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing\nthe representation quality of 3DGS representations. We leverage 2D video\ndiffusion priors to address the challenging 3D view consistency problem,\nreformulating it as achieving temporal consistency within a video generation\nprocess. 3DGS-Enhancer restores view-consistent latent features of rendered\nnovel views and integrates them with the input views through a spatial-temporal\ndecoder. The enhanced views are then used to fine-tune the initial 3DGS model,\nsignificantly improving its rendering performance. Extensive experiments on\nlarge-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields\nsuperior reconstruction performance and high-fidelity rendering results\ncompared to state-of-the-art methods. The project webpage is\nhttps://xiliu8006.github.io/3DGS-Enhancer-project .",
      "upvotes": 2
    }
  ]
}