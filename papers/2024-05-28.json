{
  "date": "2024-05-28",
  "papers": [
    {
      "title": "An Introduction to Vision-Language Modeling",
      "url": "https://huggingface.co/papers/2405.17247",
      "authors": [
        "Alexander C. Li",
        "Adrien Bardes",
        "Suzanne Petryk",
        "Mark Ibrahim",
        "Srihari Jayakumar",
        "Hu Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.17247.pdf",
      "abstract": "Following the recent popularity of Large Language Models (LLMs), several\nattempts have been made to extend them to the visual domain. From having a\nvisual assistant that could guide us through unfamiliar environments to\ngenerative models that produce images using only a high-level text description,\nthe vision-language model (VLM) applications will significantly impact our\nrelationship with technology. However, there are many challenges that need to\nbe addressed to improve the reliability of those models. While language is\ndiscrete, vision evolves in a much higher dimensional space in which concepts\ncannot always be easily discretized. To better understand the mechanics behind\nmapping vision to language, we present this introduction to VLMs which we hope\nwill help anyone who would like to enter the field. First, we introduce what\nVLMs are, how they work, and how to train them. Then, we present and discuss\napproaches to evaluate VLMs. Although this work primarily focuses on mapping\nimages to language, we also discuss extending VLMs to videos.",
      "upvotes": 85
    },
    {
      "title": "Transformers Can Do Arithmetic with the Right Embeddings",
      "url": "https://huggingface.co/papers/2405.17399",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2405.17399.pdf",
      "abstract": "The poor performance of transformers on arithmetic tasks seems to stem in\nlarge part from their inability to keep track of the exact position of each\ndigit inside of a large span of digits. We mend this problem by adding an\nembedding to each digit that encodes its position relative to the start of the\nnumber. In addition to the boost these embeddings provide on their own, we show\nthat this fix enables architectural modifications such as input injection and\nrecurrent layers to improve performance even further.\n  With positions resolved, we can study the logical extrapolation ability of\ntransformers. Can they solve arithmetic problems that are larger and more\ncomplex than those in their training data? We find that training on only 20\ndigit numbers with a single GPU for one day, we can reach state-of-the-art\nperformance, achieving up to 99% accuracy on 100 digit addition problems.\nFinally, we show that these gains in numeracy also unlock improvements on other\nmulti-step reasoning tasks including sorting and multiplication.",
      "upvotes": 51
    },
    {
      "title": "Matryoshka Multimodal Models",
      "url": "https://huggingface.co/papers/2405.17430",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2405.17430.pdf",
      "abstract": "Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in\nvisual-linguistic reasoning. These models first embed images into a fixed large\nnumber of visual tokens and then feed them into a Large Language Model (LLM).\nHowever, this design causes an excessive number of tokens for dense visual\nscenarios such as high-resolution images and videos, leading to great\ninefficiency. While token pruning/merging methods do exist, they produce a\nsingle length output for each image and do not afford flexibility in trading\noff information density v.s. efficiency. Inspired by the concept of Matryoshka\nDolls, we propose M3: Matryoshka Multimodal Models, which learns to represent\nvisual content as nested sets of visual tokens that capture information across\nmultiple coarse-to-fine granularities. Our approach offers several unique\nbenefits for LMMs: (1) One can explicitly control the visual granularity per\ntest instance during inference, e.g. , adjusting the number of tokens used to\nrepresent an image based on the anticipated complexity or simplicity of the\ncontent; (2) M3 provides a framework for analyzing the granularity needed for\nexisting datasets, where we find that COCO-style benchmarks only need around ~9\nvisual tokens to obtain accuracy similar to that of using all 576 tokens; (3)\nOur approach provides a foundation to explore the best trade-off between\nperformance and visual token length at sample level, where our investigation\nreveals that a large gap exists between the oracle upper bound and current\nfixed-scale representations.",
      "upvotes": 30
    },
    {
      "title": "Zamba: A Compact 7B SSM Hybrid Model",
      "url": "https://huggingface.co/papers/2405.16712",
      "authors": [
        "James Whittington"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16712.pdf",
      "abstract": "In this technical report, we present Zamba, a novel 7B SSM-transformer hybrid\nmodel which achieves competitive performance against leading open-weight models\nat a comparable scale. Zamba is trained on 1T tokens from openly available\ndatasets and is the best non-transformer model at this scale. Zamba pioneers a\nunique architecture combining a Mamba backbone with a single shared attention\nmodule, thus obtaining the benefits of attention at minimal parameter cost. Due\nto its architecture, Zamba is significantly faster at inference than comparable\ntransformer models and requires substantially less memory for generation of\nlong sequences. Zamba is pretrained in two phases: the first phase is based on\nexisting web datasets, while the second one consists of annealing the model\nover high-quality instruct and synthetic datasets, and is characterized by a\nrapid learning rate decay. We open-source the weights and all checkpoints for\nZamba, through both phase 1 and annealing phases.",
      "upvotes": 21
    },
    {
      "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models",
      "url": "https://huggingface.co/papers/2405.17428",
      "authors": [
        "Rajarshi Roy"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.17428.pdf",
      "abstract": "Decoder-only large language model (LLM)-based embedding models are beginning\nto outperform BERT or T5-based embedding models in general-purpose text\nembedding tasks, including dense vector-based retrieval. In this work, we\nintroduce the NV-Embed model with a variety of architectural designs and\ntraining procedures to significantly enhance the performance of LLM as a\nversatile embedding model, while maintaining its simplicity and\nreproducibility. For model architecture, we propose a latent attention layer to\nobtain pooled embeddings, which consistently improves retrieval and downstream\ntask accuracy compared to mean pooling or using the last <EOS> token embedding\nfrom LLMs. To enhance representation learning, we remove the causal attention\nmask of LLMs during contrastive training. For model training, we introduce a\ntwo-stage contrastive instruction-tuning method. It first applies contrastive\ntraining with instructions on retrieval datasets, utilizing in-batch negatives\nand curated hard negative examples. At stage-2, it blends various non-retrieval\ndatasets into instruction tuning, which not only enhances non-retrieval task\naccuracy but also improves retrieval performance. Combining these techniques,\nour NV-Embed model, using only publicly available data, has achieved a\nrecord-high score of 69.32, ranking No. 1 on the Massive Text Embedding\nBenchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval,\nreranking, classification, clustering, and semantic textual similarity tasks.\nNotably, our model also attains the highest score of 59.36 on 15 retrieval\ntasks in the MTEB benchmark (also known as BEIR). We will open-source the model\nat: https://huggingface.co/nvidia/NV-Embed-v1.",
      "upvotes": 17
    },
    {
      "title": "I2VEdit: First-Frame-Guided Video Editing via Image-to-Video Diffusion Models",
      "url": "https://huggingface.co/papers/2405.16537",
      "authors": [
        "Lei Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16537.pdf",
      "abstract": "The remarkable generative capabilities of diffusion models have motivated\nextensive research in both image and video editing. Compared to video editing\nwhich faces additional challenges in the time dimension, image editing has\nwitnessed the development of more diverse, high-quality approaches and more\ncapable software like Photoshop. In light of this gap, we introduce a novel and\ngeneric solution that extends the applicability of image editing tools to\nvideos by propagating edits from a single frame to the entire video using a\npre-trained image-to-video model. Our method, dubbed I2VEdit, adaptively\npreserves the visual and motion integrity of the source video depending on the\nextent of the edits, effectively handling global edits, local edits, and\nmoderate shape changes, which existing methods cannot fully achieve. At the\ncore of our method are two main processes: Coarse Motion Extraction to align\nbasic motion patterns with the original video, and Appearance Refinement for\nprecise adjustments using fine-grained attention matching. We also incorporate\na skip-interval strategy to mitigate quality degradation from auto-regressive\ngeneration across multiple video clips. Experimental results demonstrate our\nframework's superior performance in fine-grained video editing, proving its\ncapability to produce high-quality, temporally consistent outputs.",
      "upvotes": 15
    },
    {
      "title": "Looking Backward: Streaming Video-to-Video Translation with Feature Banks",
      "url": "https://huggingface.co/papers/2405.15757",
      "authors": [
        "Akio Kodaira",
        "Masayoshi Tomizuka",
        "Diana Marculescu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15757.pdf",
      "abstract": "This paper introduces StreamV2V, a diffusion model that achieves real-time\nstreaming video-to-video (V2V) translation with user prompts. Unlike prior V2V\nmethods using batches to process limited frames, we opt to process frames in a\nstreaming fashion, to support unlimited frames. At the heart of StreamV2V lies\na backward-looking principle that relates the present to the past. This is\nrealized by maintaining a feature bank, which archives information from past\nframes. For incoming frames, StreamV2V extends self-attention to include banked\nkeys and values and directly fuses similar past features into the output. The\nfeature bank is continually updated by merging stored and new features, making\nit compact but informative. StreamV2V stands out for its adaptability and\nefficiency, seamlessly integrating with image diffusion models without\nfine-tuning. It can run 20 FPS on one A100 GPU, being 15x, 46x, 108x, and 158x\nfaster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively. Quantitative\nmetrics and user studies confirm StreamV2V's exceptional ability to maintain\ntemporal consistency.",
      "upvotes": 14
    },
    {
      "title": "$\\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning",
      "url": "https://huggingface.co/papers/2405.17258",
      "authors": [
        "David Cox",
        "Diego Antognini",
        "Aude Oliva"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.17258.pdf",
      "abstract": "Low-rank adapters (LoRA) and their variants are popular parameter-efficient\nfine-tuning (PEFT) techniques that closely match full model fine-tune\nperformance while requiring only a small number of additional parameters. These\nadditional LoRA parameters are specific to the base model being adapted. When\nthe base model needs to be deprecated and replaced with a new one, all the\nassociated LoRA modules need to be re-trained. Such re-training requires access\nto the data used to train the LoRA for the original base model. This is\nespecially problematic for commercial cloud applications where the LoRA modules\nand the base models are hosted by service providers who may not be allowed to\nhost proprietary client task data. To address this challenge, we propose\nTrans-LoRA -- a novel method for lossless, nearly data-free transfer\nof LoRAs across base models. Our approach relies on synthetic data to transfer\nLoRA modules. Using large language models, we design a synthetic data generator\nto approximate the data-generating process of the observed task data\nsubset. Training on the resulting synthetic dataset transfers LoRA modules to\nnew models. We show the effectiveness of our approach using both LLama and\nGemma model families. Our approach achieves lossless (mostly improved) LoRA\ntransfer between models within and across different base model families, and\neven between different PEFT methods, on a wide variety of tasks.",
      "upvotes": 14
    },
    {
      "title": "Human4DiT: Free-view Human Video Generation with 4D Diffusion Transformer",
      "url": "https://huggingface.co/papers/2405.17405",
      "authors": [
        "Youxin Pang",
        "Zerong Zheng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.17405.pdf",
      "abstract": "We present a novel approach for generating high-quality, spatio-temporally\ncoherent human videos from a single image under arbitrary viewpoints. Our\nframework combines the strengths of U-Nets for accurate condition injection and\ndiffusion transformers for capturing global correlations across viewpoints and\ntime. The core is a cascaded 4D transformer architecture that factorizes\nattention across views, time, and spatial dimensions, enabling efficient\nmodeling of the 4D space. Precise conditioning is achieved by injecting human\nidentity, camera parameters, and temporal signals into the respective\ntransformers. To train this model, we curate a multi-dimensional dataset\nspanning images, videos, multi-view data and 3D/4D scans, along with a\nmulti-dimensional training strategy. Our approach overcomes the limitations of\nprevious methods based on GAN or UNet-based diffusion models, which struggle\nwith complex motions and viewpoint changes. Through extensive experiments, we\ndemonstrate our method's ability to synthesize realistic, coherent and\nfree-view human videos, paving the way for advanced multimedia applications in\nareas such as virtual reality and animation. Our project website is\nhttps://human4dit.github.io.",
      "upvotes": 14
    },
    {
      "title": "Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels",
      "url": "https://huggingface.co/papers/2405.16822",
      "authors": [
        "Fuchun Sun",
        "Jun Zhu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16822.pdf",
      "abstract": "Video generative models are receiving particular attention given their\nability to generate realistic and imaginative frames. Besides, these models are\nalso observed to exhibit strong 3D consistency, significantly enhancing their\npotential to act as world simulators. In this work, we present Vidu4D, a novel\nreconstruction model that excels in accurately reconstructing 4D (i.e.,\nsequential 3D) representations from single generated videos, addressing\nchallenges associated with non-rigidity and frame distortion. This capability\nis pivotal for creating high-fidelity virtual contents that maintain both\nspatial and temporal coherence. At the core of Vidu4D is our proposed Dynamic\nGaussian Surfels (DGS) technique. DGS optimizes time-varying warping functions\nto transform Gaussian surfels (surface elements) from a static state to a\ndynamically warped state. This transformation enables a precise depiction of\nmotion and deformation over time. To preserve the structural integrity of\nsurface-aligned Gaussian surfels, we design the warped-state geometric\nregularization based on continuous warping fields for estimating normals.\nAdditionally, we learn refinements on rotation and scaling parameters of\nGaussian surfels, which greatly alleviates texture flickering during the\nwarping process and enhances the capture of fine-grained appearance details.\nVidu4D also contains a novel initialization state that provides a proper start\nfor the warping fields in DGS. Equipping Vidu4D with an existing video\ngenerative model, the overall framework demonstrates high-fidelity text-to-4D\ngeneration in both appearance and geometry.",
      "upvotes": 11
    },
    {
      "title": "LoGAH: Predicting 774-Million-Parameter Transformers using Graph HyperNetworks with 1/100 Parameters",
      "url": "https://huggingface.co/papers/2405.16287",
      "authors": [
        "Xinyu Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16287.pdf",
      "abstract": "A good initialization of deep learning models is essential since it can help\nthem converge better and faster. However, pretraining large models is\nunaffordable for many researchers, which makes a desired prediction for initial\nparameters more necessary nowadays. Graph HyperNetworks (GHNs), one approach to\npredicting model parameters, have recently shown strong performance in\ninitializing large vision models. Unfortunately, predicting parameters of very\nwide networks relies on copying small chunks of parameters multiple times and\nrequires an extremely large number of parameters to support full prediction,\nwhich greatly hinders its adoption in practice. To address this limitation, we\npropose LoGAH (Low-rank GrAph Hypernetworks), a GHN with a low-rank parameter\ndecoder that expands to significantly wider networks without requiring as\nexcessive increase of parameters as in previous attempts. LoGAH allows us to\npredict the parameters of 774-million large neural networks in a\nmemory-efficient manner. We show that vision and language models (i.e., ViT and\nGPT-2) initialized with LoGAH achieve better performance than those initialized\nrandomly or using existing hypernetworks. Furthermore, we show promising\ntransfer learning results w.r.t. training LoGAH on small datasets and using the\npredicted parameters to initialize for larger tasks. We provide the codes in\nhttps://github.com/Blackzxy/LoGAH .",
      "upvotes": 10
    },
    {
      "title": "EM Distillation for One-step Diffusion Models",
      "url": "https://huggingface.co/papers/2405.16852",
      "authors": [
        "Ying Nian Wu",
        "Kevin Patrick Murphy",
        "Tim Salimans"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16852.pdf",
      "abstract": "While diffusion models can learn complex distributions, sampling requires a\ncomputationally expensive iterative process. Existing distillation methods\nenable efficient sampling, but have notable limitations, such as performance\ndegradation with very few sampling steps, reliance on training data access, or\nmode-seeking optimization that may fail to capture the full distribution. We\npropose EM Distillation (EMD), a maximum likelihood-based approach that\ndistills a diffusion model to a one-step generator model with minimal loss of\nperceptual quality. Our approach is derived through the lens of\nExpectation-Maximization (EM), where the generator parameters are updated using\nsamples from the joint distribution of the diffusion teacher prior and inferred\ngenerator latents. We develop a reparametrized sampling scheme and a noise\ncancellation technique that together stabilizes the distillation process. We\nfurther reveal an interesting connection of our method with existing methods\nthat minimize mode-seeking KL. EMD outperforms existing one-step generative\nmethods in terms of FID scores on ImageNet-64 and ImageNet-128, and compares\nfavorably with prior work on distilling text-to-image diffusion models.",
      "upvotes": 10
    },
    {
      "title": "Part123: Part-aware 3D Reconstruction from a Single-view Image",
      "url": "https://huggingface.co/papers/2405.16888",
      "authors": [
        "Anran Liu",
        "Cheng Lin",
        "Hao-Xiang Guo",
        "Ping Luo",
        "Wenping Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16888.pdf",
      "abstract": "Recently, the emergence of diffusion models has opened up new opportunities\nfor single-view reconstruction. However, all the existing methods represent the\ntarget object as a closed mesh devoid of any structural information, thus\nneglecting the part-based structure, which is crucial for many downstream\napplications, of the reconstructed shape. Moreover, the generated meshes\nusually suffer from large noises, unsmooth surfaces, and blurry textures,\nmaking it challenging to obtain satisfactory part segments using 3D\nsegmentation techniques. In this paper, we present Part123, a novel framework\nfor part-aware 3D reconstruction from a single-view image. We first use\ndiffusion models to generate multiview-consistent images from a given image,\nand then leverage Segment Anything Model (SAM), which demonstrates powerful\ngeneralization ability on arbitrary objects, to generate multiview segmentation\nmasks. To effectively incorporate 2D part-based information into 3D\nreconstruction and handle inconsistency, we introduce contrastive learning into\na neural rendering framework to learn a part-aware feature space based on the\nmultiview segmentation masks. A clustering-based algorithm is also developed to\nautomatically derive 3D part segmentation results from the reconstructed\nmodels. Experiments show that our method can generate 3D models with\nhigh-quality segmented parts on various objects. Compared to existing\nunstructured reconstruction methods, the part-aware 3D models from our method\nbenefit some important applications, including feature-preserving\nreconstruction, primitive fitting, and 3D shape editing.",
      "upvotes": 10
    },
    {
      "title": "Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control",
      "url": "https://huggingface.co/papers/2405.17414",
      "authors": [
        "Zhengfei Kuang",
        "Shengqu Cai",
        "Hao He",
        "Leonidas Guibas",
        "Gordon Wetzstein"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.17414.pdf",
      "abstract": "Research on video generation has recently made tremendous progress, enabling\nhigh-quality videos to be generated from text prompts or images. Adding control\nto the video generation process is an important goal moving forward and recent\napproaches that condition video generation models on camera trajectories make\nstrides towards it. Yet, it remains challenging to generate a video of the same\nscene from multiple different camera trajectories. Solutions to this\nmulti-video generation problem could enable large-scale 3D scene generation\nwith editable camera trajectories, among other applications. We introduce\ncollaborative video diffusion (CVD) as an important step towards this vision.\nThe CVD framework includes a novel cross-video synchronization module that\npromotes consistency between corresponding frames of the same video rendered\nfrom different camera poses using an epipolar attention mechanism. Trained on\ntop of a state-of-the-art camera-control module for video generation, CVD\ngenerates multiple videos rendered from different camera trajectories with\nsignificantly better consistency than baselines, as shown in extensive\nexperiments. Project page: https://collaborativevideodiffusion.github.io/.",
      "upvotes": 10
    },
    {
      "title": "Greedy Growing Enables High-Resolution Pixel-Based Diffusion Models",
      "url": "https://huggingface.co/papers/2405.16759",
      "authors": [
        "Cristina N. Vasconcelos",
        "Abdullah Rashwan Austin Waters",
        "Trevor Walker",
        "Keyang Xu",
        "Jimmy Yan",
        "Rui Qian",
        "Shixin Luo",
        "Zarana Parekh",
        "Andrew Bunner",
        "Mandy Guo",
        "Ivana Kajic",
        "Henna Nandwani",
        "Su Wang",
        "Wenlei Zhou",
        "David J. Fleet"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16759.pdf",
      "abstract": "We address the long-standing problem of how to learn effective pixel-based\nimage diffusion models at scale, introducing a remarkably simple greedy growing\nmethod for stable training of large-scale, high-resolution models. without the\nneeds for cascaded super-resolution components. The key insight stems from\ncareful pre-training of core components, namely, those responsible for\ntext-to-image alignment {\\it vs.} high-resolution rendering. We first\ndemonstrate the benefits of scaling a {\\it Shallow UNet}, with no\ndown(up)-sampling enc(dec)oder. Scaling its deep core layers is shown to\nimprove alignment, object structure, and composition. Building on this core\nmodel, we propose a greedy algorithm that grows the architecture into\nhigh-resolution end-to-end models, while preserving the integrity of the\npre-trained representation, stabilizing training, and reducing the need for\nlarge high-resolution datasets. This enables a single stage model capable of\ngenerating high-resolution images without the need of a super-resolution\ncascade. Our key results rely on public datasets and show that we are able to\ntrain non-cascaded models up to 8B parameters with no further regularization\nschemes. Vermeer, our full pipeline model trained with internal datasets to\nproduce 1024x1024 images, without cascades, is preferred by 44.0% vs. 21.4%\nhuman evaluators over SDXL.",
      "upvotes": 7
    }
  ]
}