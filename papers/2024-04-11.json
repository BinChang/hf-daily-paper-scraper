{
  "date": "2024-04-11",
  "papers": [
    {
      "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention",
      "url": "https://huggingface.co/papers/2404.07143",
      "authors": [
        "Tsendsuren Munkhdalai",
        "Manaal Faruqui",
        "Siddharth Gopal"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.07143.pdf",
      "abstract": "This work introduces an efficient method to scale Transformer-based Large\nLanguage Models (LLMs) to infinitely long inputs with bounded memory and\ncomputation. A key component in our proposed approach is a new attention\ntechnique dubbed Infini-attention. The Infini-attention incorporates a\ncompressive memory into the vanilla attention mechanism and builds in both\nmasked local attention and long-term linear attention mechanisms in a single\nTransformer block. We demonstrate the effectiveness of our approach on\nlong-context language modeling benchmarks, 1M sequence length passkey context\nblock retrieval and 500K length book summarization tasks with 1B and 8B LLMs.\nOur approach introduces minimal bounded memory parameters and enables fast\nstreaming inference for LLMs.",
      "upvotes": 103
    },
    {
      "title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
      "url": "https://huggingface.co/papers/2404.07413",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2404.07413.pdf",
      "abstract": "Large Language Models (LLMs) have achieved remarkable results, but their\nincreasing resource demand has become a major obstacle to the development of\npowerful and accessible super-human intelligence. This report introduces\nJetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens\nfrom carefully mixed open-source corpora and 30,000 H100 GPU hours. Despite its\nlow cost, the JetMoE-8B demonstrates impressive performance, with JetMoE-8B\noutperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the\nLlama2-13B-Chat model. These results suggest that LLM training can be much more\ncost-effective than generally thought. JetMoE-8B is based on an efficient\nSparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention\nand feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B\nto have 8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B. Moreover, JetMoE-8B\nis highly open and academia-friendly, using only public datasets and training\ncode. All training parameters and data mixtures have been detailed in this\nreport to facilitate future efforts in the development of open foundation\nmodels. This transparency aims to encourage collaboration and further\nadvancements in the field of accessible and efficient LLMs. The model weights\nare publicly available at https://github.com/myshell-ai/JetMoE.",
      "upvotes": 36
    },
    {
      "title": "RULER: What's the Real Context Size of Your Long-Context Language Models?",
      "url": "https://huggingface.co/papers/2404.06654",
      "authors": [
        "Dima Rekesh",
        "Fei Jia",
        "Boris Ginsburg"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06654.pdf",
      "abstract": "The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve\na piece of information (the \"needle\") from long distractor texts (the\n\"haystack\"), has been widely adopted to evaluate long-context language models\n(LMs). However, this simple retrieval-based test is indicative of only a\nsuperficial form of long-context understanding. To provide a more comprehensive\nevaluation of long-context LMs, we create a new synthetic benchmark RULER with\nflexible configurations for customized sequence length and task complexity.\nRULER expands upon the vanilla NIAH test to encompass variations with diverse\ntypes and quantities of needles. Moreover, RULER introduces new task categories\nmulti-hop tracing and aggregation to test behaviors beyond searching from\ncontext. We evaluate ten long-context LMs with 13 representative tasks in\nRULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, all\nmodels exhibit large performance drops as the context length increases. While\nthese models all claim context sizes of 32K tokens or greater, only four models\n(GPT-4, Command-R, Yi-34B, and Mixtral) can maintain satisfactory performance\nat the length of 32K. Our analysis of Yi-34B, which supports context length of\n200K, reveals large room for improvement as we increase input length and task\ncomplexity. We open source RULER to spur comprehensive evaluation of\nlong-context LMs.",
      "upvotes": 33
    },
    {
      "title": "RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion",
      "url": "https://huggingface.co/papers/2404.07199",
      "authors": [
        "Ravi Ramamoorthi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.07199.pdf",
      "abstract": "We introduce RealmDreamer, a technique for generation of general\nforward-facing 3D scenes from text descriptions. Our technique optimizes a 3D\nGaussian Splatting representation to match complex text prompts. We initialize\nthese splats by utilizing the state-of-the-art text-to-image generators,\nlifting their samples into 3D, and computing the occlusion volume. We then\noptimize this representation across multiple views as a 3D inpainting task with\nimage-conditional diffusion models. To learn correct geometric structure, we\nincorporate a depth diffusion model by conditioning on the samples from the\ninpainting model, giving rich geometric structure. Finally, we finetune the\nmodel using sharpened samples from image generators. Notably, our technique\ndoes not require video or multi-view data and can synthesize a variety of\nhigh-quality 3D scenes in different styles, consisting of multiple objects. Its\ngenerality additionally allows 3D synthesis from a single image.",
      "upvotes": 25
    },
    {
      "title": "From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples",
      "url": "https://huggingface.co/papers/2404.07544",
      "authors": [
        "Vlad-Andrei Negru",
        "Vasile Suciu",
        "Mihai Surdeanu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.07544.pdf",
      "abstract": "We analyze how well pre-trained large language models (e.g., Llama2, GPT-4,\nClaude 3, etc) can do linear and non-linear regression when given in-context\nexamples, without any additional training or gradient updates. Our findings\nreveal that several large language models (e.g., GPT-4, Claude 3) are able to\nperform regression tasks with a performance rivaling (or even outperforming)\nthat of traditional supervised methods such as Random Forest, Bagging, or\nGradient Boosting. For example, on the challenging Friedman #2 regression\ndataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM,\nRandom Forest, KNN, or Gradient Boosting. We then investigate how well the\nperformance of large language models scales with the number of in-context\nexemplars. We borrow from the notion of regret from online learning and\nempirically show that LLMs are capable of obtaining a sub-linear regret.",
      "upvotes": 18
    },
    {
      "title": "DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting",
      "url": "https://huggingface.co/papers/2404.06903",
      "authors": [
        "Haoran Chang",
        "Tejas Bharadwaj",
        "Suya You",
        "Zhangyang Wang",
        "Achuta Kadambi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06903.pdf",
      "abstract": "The increasing demand for virtual reality applications has highlighted the\nsignificance of crafting immersive 3D assets. We present a text-to-3D\n360^{circ} scene generation pipeline that facilitates the creation of\ncomprehensive 360^{circ} scenes for in-the-wild environments in a matter of\nminutes. Our approach utilizes the generative power of a 2D diffusion model and\nprompt self-refinement to create a high-quality and globally coherent panoramic\nimage. This image acts as a preliminary \"flat\" (2D) scene representation.\nSubsequently, it is lifted into 3D Gaussians, employing splatting techniques to\nenable real-time exploration. To produce consistent 3D geometry, our pipeline\nconstructs a spatially coherent structure by aligning the 2D monocular depth\ninto a globally optimized point cloud. This point cloud serves as the initial\nstate for the centroids of 3D Gaussians. In order to address invisible issues\ninherent in single-view inputs, we impose semantic and geometric constraints on\nboth synthesized and input camera views as regularizations. These guide the\noptimization of Gaussians, aiding in the reconstruction of unseen regions. In\nsummary, our method offers a globally consistent 3D scene within a\n360^{circ} perspective, providing an enhanced immersive experience over\nexisting techniques. Project website at: http://dreamscene360.github.io/",
      "upvotes": 18
    },
    {
      "title": "BRAVE: Broadening the visual encoding of vision-language models",
      "url": "https://huggingface.co/papers/2404.07204",
      "authors": [
        "Achin Kulshrestha",
        "Amir Zamir",
        "Federico Tombari"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.07204.pdf",
      "abstract": "Vision-language models (VLMs) are typically composed of a vision encoder,\ne.g. CLIP, and a language model (LM) that interprets the encoded features to\nsolve downstream tasks. Despite remarkable progress, VLMs are subject to\nseveral shortcomings due to the limited capabilities of vision encoders, e.g.\n\"blindness\" to certain image features, visual hallucination, etc. To address\nthese issues, we study broadening the visual encoding capabilities of VLMs. We\nfirst comprehensively benchmark several vision encoders with different\ninductive biases for solving VLM tasks. We observe that there is no single\nencoding configuration that consistently achieves top performance across\ndifferent tasks, and encoders with different biases can perform surprisingly\nsimilarly. Motivated by this, we introduce a method, named BRAVE, that\nconsolidates features from multiple frozen encoders into a more versatile\nrepresentation that can be directly fed as the input to a frozen LM. BRAVE\nachieves state-of-the-art performance on a broad range of captioning and VQA\nbenchmarks and significantly reduces the aforementioned issues of VLMs, while\nrequiring a smaller number of trainable parameters than existing methods and\nhaving a more compressed representation. Our results highlight the potential of\nincorporating different visual biases for a more broad and contextualized\nvisual understanding of VLMs.",
      "upvotes": 18
    },
    {
      "title": "Adapting LLaMA Decoder to Vision Transformer",
      "url": "https://huggingface.co/papers/2404.06773",
      "authors": [
        "Yong Liu",
        "Kai Chen",
        "Ping Luo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06773.pdf",
      "abstract": "This work examines whether decoder-only Transformers such as LLaMA, which\nwere originally designed for large language models (LLMs), can be adapted to\nthe computer vision field. We first \"LLaMAfy\" a standard ViT step-by-step to\nalign with LLaMA's architecture, and find that directly applying a casual mask\nto the self-attention brings an attention collapse issue, resulting in the\nfailure to the network training. We suggest to reposition the class token\nbehind the image tokens with a post-sequence class token technique to overcome\nthis challenge, enabling causal self-attention to efficiently capture the\nentire image's information. Additionally, we develop a soft mask strategy that\ngradually introduces a casual mask to the self-attention at the onset of\ntraining to facilitate the optimization behavior. The tailored model, dubbed as\nimage LLaMA (iLLaMA), is akin to LLaMA in architecture and enables direct\nsupervised learning. Its causal self-attention boosts computational efficiency\nand learns complex representation by elevating attention map ranks. iLLaMA\nrivals the performance with its encoder-only counterparts, achieving 75.1%\nImageNet top-1 accuracy with only 5.7M parameters. Scaling the model to ~310M\nand pre-training on ImageNet-21K further enhances the accuracy to 86.0%.\nExtensive experiments demonstrate iLLaMA's reliable properties: calibration,\nshape-texture bias, quantization compatibility, ADE20K segmentation and CIFAR\ntransfer learning. We hope our study can kindle fresh views to visual model\ndesign in the wave of LLMs. Pre-trained models and codes are available here.",
      "upvotes": 17
    },
    {
      "title": "Audio Dialogues: Dialogues dataset for audio and music understanding",
      "url": "https://huggingface.co/papers/2404.07616",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2404.07616.pdf",
      "abstract": "Existing datasets for audio understanding primarily focus on single-turn\ninteractions (i.e. audio captioning, audio question answering) for describing\naudio in natural language, thus limiting understanding audio via interactive\ndialogue. To address this gap, we introduce Audio Dialogues: a multi-turn\ndialogue dataset containing 163.8k samples for general audio sounds and music.\nIn addition to dialogues, Audio Dialogues also has question-answer pairs to\nunderstand and compare multiple input audios together. Audio Dialogues\nleverages a prompting-based approach and caption annotations from existing\ndatasets to generate multi-turn dialogues using a Large Language Model (LLM).\nWe evaluate existing audio-augmented large language models on our proposed\ndataset to demonstrate the complexity and applicability of Audio Dialogues. Our\ncode for generating the dataset will be made publicly available. Detailed\nprompts and generated dialogues can be found on the demo website\nhttps://audiodialogues.github.io/.",
      "upvotes": 15
    },
    {
      "title": "Transferable and Principled Efficiency for Open-Vocabulary Segmentation",
      "url": "https://huggingface.co/papers/2404.07448",
      "authors": [
        "Jingxuan Xu",
        "Yao Zhao",
        "Yunchao Wei"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.07448.pdf",
      "abstract": "Recent success of pre-trained foundation vision-language models makes\nOpen-Vocabulary Segmentation (OVS) possible. Despite the promising performance,\nthis approach introduces heavy computational overheads for two challenges: 1)\nlarge model sizes of the backbone; 2) expensive costs during the fine-tuning.\nThese challenges hinder this OVS strategy from being widely applicable and\naffordable in real-world scenarios. Although traditional methods such as model\ncompression and efficient fine-tuning can address these challenges, they often\nrely on heuristics. This means that their solutions cannot be easily\ntransferred and necessitate re-training on different models, which comes at a\ncost. In the context of efficient OVS, we target achieving performance that is\ncomparable to or even better than prior OVS works based on large\nvision-language foundation models, by utilizing smaller models that incur lower\ntraining costs. The core strategy is to make our efficiency principled and thus\nseamlessly transferable from one OVS framework to others without further\ncustomization. Comprehensive experiments on diverse OVS benchmarks demonstrate\nour superior trade-off between segmentation accuracy and computation costs over\nprevious works. Our code is available on https://github.com/Xujxyang/OpenTrans",
      "upvotes": 11
    }
  ]
}