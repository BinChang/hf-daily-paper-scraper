{
  "date": "2024-06-11",
  "papers": [
    {
      "title": "Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation",
      "url": "https://huggingface.co/papers/2406.06525",
      "authors": [
        "Bingyue Peng",
        "Ping Luo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06525.pdf",
      "abstract": "We introduce LlamaGen, a new family of image generation models that apply\noriginal ``next-token prediction'' paradigm of large language models to visual\ngeneration domain. It is an affirmative answer to whether vanilla\nautoregressive models, e.g., Llama, without inductive biases on visual signals\ncan achieve state-of-the-art image generation performance if scaling properly.\nWe reexamine design spaces of image tokenizers, scalability properties of image\ngeneration models, and their training data quality. The outcome of this\nexploration consists of: (1) An image tokenizer with downsample ratio of 16,\nreconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet\nbenchmark. (2) A series of class-conditional image generation models ranging\nfrom 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256\nbenchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A\ntext-conditional image generation model with 775M parameters, from two-stage\ntraining on LAION-COCO and high aesthetics quality images, demonstrating\ncompetitive performance of visual quality and text alignment. (4) We verify the\neffectiveness of LLM serving frameworks in optimizing the inference speed of\nimage generation models and achieve 326% - 414% speedup. We release all models\nand codes to facilitate open-source community of visual generation and\nmultimodal foundation models.",
      "upvotes": 64
    },
    {
      "title": "Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning",
      "url": "https://huggingface.co/papers/2406.06469",
      "authors": [
        "Hannaneh Hajishirzi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06469.pdf",
      "abstract": "Language agents perform complex tasks by using tools to execute each step\nprecisely. However, most existing agents are based on proprietary models or\ndesigned to target specific tasks, such as mathematics or multi-hop question\nanswering. We introduce Husky, a holistic, open-source language agent that\nlearns to reason over a unified action space to address a diverse set of\ncomplex tasks involving numerical, tabular, and knowledge-based reasoning.\nHusky iterates between two stages: 1) generating the next action to take\ntowards solving a given task and 2) executing the action using expert models\nand updating the current solution state. We identify a thorough ontology of\nactions for addressing complex tasks and curate high-quality data to train\nexpert models for executing these actions. Our experiments show that Husky\noutperforms prior language agents across 14 evaluation datasets. Moreover, we\nintroduce HuskyQA, a new evaluation set which stress tests language agents for\nmixed-tool reasoning, with a focus on retrieving missing knowledge and\nperforming numerical reasoning. Despite using 7B models, Husky matches or even\nexceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of\nour holistic approach in addressing complex reasoning problems. Our code and\nmodels are available at https://github.com/agent-husky/Husky-v1.",
      "upvotes": 23
    },
    {
      "title": "Vript: A Video Is Worth Thousands of Words",
      "url": "https://huggingface.co/papers/2406.06040",
      "authors": [
        "Chengqiang Lu",
        "Haoxin Zhang",
        "Yan Gao",
        "Yao Hu",
        "Hai Zhao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06040.pdf",
      "abstract": "Advancements in multimodal learning, particularly in video understanding and\ngeneration, require high-quality video-text datasets for improved model\nperformance. Vript addresses this issue with a meticulously annotated corpus of\n12K high-resolution videos, offering detailed, dense, and script-like captions\nfor over 420K clips. Each clip has a caption of ~145 words, which is over 10x\nlonger than most video-text datasets. Unlike captions only documenting static\ncontent in previous datasets, we enhance video captioning to video scripting by\ndocumenting not just the content, but also the camera operations, which include\nthe shot types (medium shot, close-up, etc) and camera movements (panning,\ntilting, etc). By utilizing the Vript, we explore three training paradigms of\naligning more text with the video modality rather than clip-caption pairs. This\nresults in Vriptor, a top-performing video captioning model among open-source\nmodels, comparable to GPT-4V in performance. Vriptor is also a powerful model\ncapable of end-to-end generation of dense and detailed captions for long\nvideos. Moreover, we introduce Vript-Hard, a benchmark consisting of three\nvideo understanding tasks that are more challenging than existing benchmarks:\nVript-HAL is the first benchmark evaluating action and object hallucinations in\nvideo LLMs, Vript-RR combines reasoning with retrieval resolving question\nambiguity in long-video QAs, and Vript-ERO is a new task to evaluate the\ntemporal understanding of events in long videos rather than actions in short\nvideos in previous works. All code, models, and datasets are available in\nhttps://github.com/mutonix/Vript.",
      "upvotes": 22
    },
    {
      "title": "Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis",
      "url": "https://huggingface.co/papers/2406.06216",
      "authors": [
        "Pengyi Jiao",
        "Zheng-Peng Duan",
        "Xingchao Yang",
        "Bo Ren",
        "Chongyi Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06216.pdf",
      "abstract": "Volumetric rendering based methods, like NeRF, excel in HDR view synthesis\nfrom RAWimages, especially for nighttime scenes. While, they suffer from long\ntraining times and cannot perform real-time rendering due to dense sampling\nrequirements. The advent of 3D Gaussian Splatting (3DGS) enables real-time\nrendering and faster training. However, implementing RAW image-based view\nsynthesis directly using 3DGS is challenging due to its inherent drawbacks: 1)\nin nighttime scenes, extremely low SNR leads to poor structure-from-motion\n(SfM) estimation in distant views; 2) the limited representation capacity of\nspherical harmonics (SH) function is unsuitable for RAW linear color space; and\n3) inaccurate scene structure hampers downstream tasks such as refocusing. To\naddress these issues, we propose LE3D (Lighting Every darkness with 3DGS). Our\nmethod proposes Cone Scatter Initialization to enrich the estimation of SfM,\nand replaces SH with a Color MLP to represent the RAW linear color space.\nAdditionally, we introduce depth distortion and near-far regularizations to\nimprove the accuracy of scene structure for downstream tasks. These designs\nenable LE3D to perform real-time novel view synthesis, HDR rendering,\nrefocusing, and tone-mapping changes. Compared to previous volumetric rendering\nbased methods, LE3D reduces training time to 1% and improves rendering speed by\nup to 4,000 times for 2K resolution images in terms of FPS. Code and viewer can\nbe found in https://github.com/Srameo/LE3D .",
      "upvotes": 18
    },
    {
      "title": "Towards a Personal Health Large Language Model",
      "url": "https://huggingface.co/papers/2406.06474",
      "authors": [
        "Justin Cosentino",
        "Anastasiya Belyaeva",
        "Xin Liu",
        "Nicholas A. Furlotte",
        "Zhun Yang",
        "Chace Lee",
        "Erik Schenck",
        "Yojan Patel",
        "Jian Cui",
        "Logan Douglas Schneider",
        "Robby Bryant",
        "Ryan G. Gomes",
        "Allen Jiang",
        "Roy Lee",
        "Yun Liu",
        "Javier Perez",
        "Jameson K. Rogers",
        "Cathy Speed",
        "Shyam Tailor",
        "Megan Walker",
        "Jeffrey Yu",
        "Tim Althoff"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06474.pdf",
      "abstract": "In health, most large language model (LLM) research has focused on clinical\ntasks. However, mobile and wearable devices, which are rarely integrated into\nsuch tasks, provide rich, longitudinal data for personal health monitoring.\nHere we present Personal Health Large Language Model (PH-LLM), fine-tuned from\nGemini for understanding and reasoning over numerical time-series personal\nhealth data. We created and curated three datasets that test 1) production of\npersonalized insights and recommendations from sleep patterns, physical\nactivity, and physiological responses, 2) expert domain knowledge, and 3)\nprediction of self-reported sleep outcomes. For the first task we designed 857\ncase studies in collaboration with domain experts to assess real-world\nscenarios in sleep and fitness. Through comprehensive evaluation of\ndomain-specific rubrics, we observed that Gemini Ultra 1.0 and PH-LLM are not\nstatistically different from expert performance in fitness and, while experts\nremain superior for sleep, fine-tuning PH-LLM provided significant improvements\nin using relevant domain knowledge and personalizing information for sleep\ninsights. We evaluated PH-LLM domain knowledge using multiple choice sleep\nmedicine and fitness examinations. PH-LLM achieved 79% on sleep and 88% on\nfitness, exceeding average scores from a sample of human experts. Finally, we\ntrained PH-LLM to predict self-reported sleep quality outcomes from textual and\nmultimodal encoding representations of wearable data, and demonstrate that\nmultimodal encoding is required to match performance of specialized\ndiscriminative models. Although further development and evaluation are\nnecessary in the safety-critical personal health domain, these results\ndemonstrate both the broad knowledge and capabilities of Gemini models and the\nbenefit of contextualizing physiological data for personal health applications\nas done with PH-LLM.",
      "upvotes": 17
    },
    {
      "title": "Tx-LLM: A Large Language Model for Therapeutics",
      "url": "https://huggingface.co/papers/2406.06316",
      "authors": [
        "Eric Wang",
        "Eeshit Dhaval Vaishnav",
        "Byron Lee",
        "S. Sara Mahdavi",
        "Christopher Semturs",
        "David Fleet",
        "Shekoofeh Azizi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06316.pdf",
      "abstract": "Developing therapeutics is a lengthy and expensive process that requires the\nsatisfaction of many different criteria, and AI models capable of expediting\nthe process would be invaluable. However, the majority of current AI approaches\naddress only a narrowly defined set of tasks, often circumscribed within a\nparticular domain. To bridge this gap, we introduce Tx-LLM, a generalist large\nlanguage model (LLM) fine-tuned from PaLM-2 which encodes knowledge about\ndiverse therapeutic modalities. Tx-LLM is trained using a collection of 709\ndatasets that target 66 tasks spanning various stages of the drug discovery\npipeline. Using a single set of weights, Tx-LLM simultaneously processes a wide\nvariety of chemical or biological entities(small molecules, proteins, nucleic\nacids, cell lines, diseases) interleaved with free-text, allowing it to predict\na broad range of associated properties, achieving competitive with\nstate-of-the-art (SOTA) performance on 43 out of 66 tasks and exceeding SOTA on\n22. Among these, Tx-LLM is particularly powerful and exceeds best-in-class\nperformance on average for tasks combining molecular SMILES representations\nwith text such as cell line names or disease names, likely due to context\nlearned during pretraining. We observe evidence of positive transfer between\ntasks with diverse drug types (e.g.,tasks involving small molecules and tasks\ninvolving proteins), and we study the impact of model size, domain finetuning,\nand prompting strategies on performance. We believe Tx-LLM represents an\nimportant step towards LLMs encoding biochemical knowledge and could have a\nfuture role as an end-to-end tool across the drug discovery development\npipeline.",
      "upvotes": 14
    },
    {
      "title": "VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers",
      "url": "https://huggingface.co/papers/2406.05370",
      "authors": [
        "Long Zhou",
        "Sheng Zhao",
        "Yao Qian"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05370.pdf",
      "abstract": "This paper introduces VALL-E 2, the latest advancement in neural codec\nlanguage models that marks a milestone in zero-shot text-to-speech synthesis\n(TTS), achieving human parity for the first time. Based on its predecessor,\nVALL-E, the new iteration introduces two significant enhancements: Repetition\nAware Sampling refines the original nucleus sampling process by accounting for\ntoken repetition in the decoding history. It not only stabilizes the decoding\nbut also circumvents the infinite loop issue. Grouped Code Modeling organizes\ncodec codes into groups to effectively shorten the sequence length, which not\nonly boosts inference speed but also addresses the challenges of long sequence\nmodeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E\n2 surpasses previous systems in speech robustness, naturalness, and speaker\nsimilarity. It is the first of its kind to reach human parity on these\nbenchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech,\neven for sentences that are traditionally challenging due to their complexity\nor repetitive phrases. The advantages of this work could contribute to valuable\nendeavors, such as generating speech for individuals with aphasia or people\nwith amyotrophic lateral sclerosis. Demos of VALL-E 2 will be posted to\nhttps://aka.ms/valle2.",
      "upvotes": 14
    },
    {
      "title": "Margin-aware Preference Optimization for Aligning Diffusion Models without Reference",
      "url": "https://huggingface.co/papers/2406.06424",
      "authors": [
        "James Thorne"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06424.pdf",
      "abstract": "Modern alignment techniques based on human preferences, such as RLHF and DPO,\ntypically employ divergence regularization relative to the reference model to\nensure training stability. However, this often limits the flexibility of models\nduring alignment, especially when there is a clear distributional discrepancy\nbetween the preference data and the reference model. In this paper, we focus on\nthe alignment of recent text-to-image diffusion models, such as Stable\nDiffusion XL (SDXL), and find that this \"reference mismatch\" is indeed a\nsignificant problem in aligning these models due to the unstructured nature of\nvisual modalities: e.g., a preference for a particular stylistic aspect can\neasily induce such a discrepancy. Motivated by this observation, we propose a\nnovel and memory-friendly preference alignment method for diffusion models that\ndoes not depend on any reference model, coined margin-aware preference\noptimization (MaPO). MaPO jointly maximizes the likelihood margin between the\npreferred and dispreferred image sets and the likelihood of the preferred sets,\nsimultaneously learning general stylistic features and preferences. For\nevaluation, we introduce two new pairwise preference datasets, which comprise\nself-generated image pairs from SDXL, Pick-Style and Pick-Safety, simulating\ndiverse scenarios of reference mismatch. Our experiments validate that MaPO can\nsignificantly improve alignment on Pick-Style and Pick-Safety and general\npreference alignment when used with Pick-a-Pic v2, surpassing the base SDXL and\nother existing methods. Our code, models, and datasets are publicly available\nvia https://mapo-t2i.github.io",
      "upvotes": 11
    },
    {
      "title": "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization",
      "url": "https://huggingface.co/papers/2406.05981",
      "authors": [
        "Yichao Fu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05981.pdf",
      "abstract": "Large language models (LLMs) have shown impressive performance on language\ntasks but face challenges when deployed on resource-constrained devices due to\ntheir extensive parameters and reliance on dense multiplications, resulting in\nhigh memory demands and latency bottlenecks. Shift-and-add reparameterization\noffers a promising solution by replacing costly multiplications with\nhardware-friendly primitives in both the attention and multi-layer perceptron\n(MLP) layers of an LLM. However, current reparameterization techniques require\ntraining from scratch or full parameter fine-tuning to restore accuracy, which\nis resource-intensive for LLMs. To address this, we propose accelerating\npretrained LLMs through post-training shift-and-add reparameterization,\ncreating efficient multiplication-free models, dubbed ShiftAddLLM.\nSpecifically, we quantize each weight matrix into binary matrices paired with\ngroup-wise scaling factors. The associated multiplications are reparameterized\ninto (1) shifts between activations and scaling factors and (2) queries and\nadds according to the binary matrices. To reduce accuracy loss, we present a\nmulti-objective optimization method to minimize both weight and output\nactivation reparameterization errors. Additionally, based on varying\nsensitivity across layers to reparameterization, we develop an automated bit\nallocation strategy to further reduce memory usage and latency. Experiments on\nfive LLM families and eight tasks consistently validate the effectiveness of\nShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points\nat comparable or lower latency compared to the most competitive quantized LLMs\nat 3 and 2 bits, respectively, and more than 80% memory and energy reductions\nover the original LLMs. Codes and models are available at\nhttps://github.com/GATECH-EIC/ShiftAddLLM.",
      "upvotes": 11
    },
    {
      "title": "Unified Text-to-Image Generation and Retrieval",
      "url": "https://huggingface.co/papers/2406.05814",
      "authors": [
        "Wenjie Wang",
        "Liqiang Nie"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05814.pdf",
      "abstract": "How humans can efficiently and effectively acquire images has always been a\nperennial question. A typical solution is text-to-image retrieval from an\nexisting database given the text query; however, the limited database typically\nlacks creativity. By contrast, recent breakthroughs in text-to-image generation\nhave made it possible to produce fancy and diverse visual content, but it faces\nchallenges in synthesizing knowledge-intensive images. In this work, we rethink\nthe relationship between text-to-image generation and retrieval and propose a\nunified framework in the context of Multimodal Large Language Models (MLLMs).\nSpecifically, we first explore the intrinsic discriminative abilities of MLLMs\nand introduce a generative retrieval method to perform retrieval in a\ntraining-free manner. Subsequently, we unify generation and retrieval in an\nautoregressive generation way and propose an autonomous decision module to\nchoose the best-matched one between generated and retrieved images as the\nresponse to the text query. Additionally, we construct a benchmark called\nTIGeR-Bench, including creative and knowledge-intensive domains, to standardize\nthe evaluation of unified text-to-image generation and retrieval. Extensive\nexperimental results on TIGeR-Bench and two retrieval benchmarks, i.e.,\nFlickr30K and MS-COCO, demonstrate the superiority and effectiveness of our\nproposed method.",
      "upvotes": 10
    },
    {
      "title": "IllumiNeRF: 3D Relighting without Inverse Rendering",
      "url": "https://huggingface.co/papers/2406.06527",
      "authors": [
        "Pratul P. Srinivasan",
        "Keunhong Park",
        "Ricardo Martin Brualla",
        "Philipp Henzler"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06527.pdf",
      "abstract": "Existing methods for relightable view synthesis -- using a set of images of\nan object under unknown lighting to recover a 3D representation that can be\nrendered from novel viewpoints under a target illumination -- are based on\ninverse rendering, and attempt to disentangle the object geometry, materials,\nand lighting that explain the input images. Furthermore, this typically\ninvolves optimization through differentiable Monte Carlo rendering, which is\nbrittle and computationally-expensive. In this work, we propose a simpler\napproach: we first relight each input image using an image diffusion model\nconditioned on lighting and then reconstruct a Neural Radiance Field (NeRF)\nwith these relit images, from which we render novel views under the target\nlighting. We demonstrate that this strategy is surprisingly competitive and\nachieves state-of-the-art results on multiple relighting benchmarks. Please see\nour project page at https://illuminerf.github.io/.",
      "upvotes": 8
    },
    {
      "title": "MLCM: Multistep Consistency Distillation of Latent Diffusion Model",
      "url": "https://huggingface.co/papers/2406.05768",
      "authors": [
        "Zhenyi Liao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05768.pdf",
      "abstract": "Distilling large latent diffusion models (LDMs) into ones that are fast to\nsample from is attracting growing research interest. However, the majority of\nexisting methods face a dilemma where they either (i) depend on multiple\nindividual distilled models for different sampling budgets, or (ii) sacrifice\ngeneration quality with limited (e.g., 2-4) and/or moderate (e.g., 5-8)\nsampling steps. To address these, we extend the recent multistep consistency\ndistillation (MCD) strategy to representative LDMs, establishing the Multistep\nLatent Consistency Models (MLCMs) approach for low-cost high-quality image\nsynthesis. MLCM serves as a unified model for various sampling steps due to the\npromise of MCD. We further augment MCD with a progressive training strategy to\nstrengthen inter-segment consistency to boost the quality of few-step\ngenerations. We take the states from the sampling trajectories of the teacher\nmodel as training data for MLCMs to lift the requirements for high-quality\ntraining datasets and to bridge the gap between the training and inference of\nthe distilled model. MLCM is compatible with preference learning strategies for\nfurther improvement of visual quality and aesthetic appeal. Empirically, MLCM\ncan generate high-quality, delightful images with only 2-8 sampling steps. On\nthe MSCOCO-2017 5K benchmark, MLCM distilled from SDXL gets a CLIP Score of\n33.30, Aesthetic Score of 6.19, and Image Reward of 1.20 with only 4 steps,\nsubstantially surpassing 4-step LCM [23], 8-step SDXL-Lightning [17], and\n8-step HyperSD [33]. We also demonstrate the versatility of MLCMs in\napplications including controllable generation, image style transfer, and\nChinese-to-image generation.",
      "upvotes": 8
    },
    {
      "title": "ExtraNeRF: Visibility-Aware View Extrapolation of Neural Radiance Fields with Diffusion Models",
      "url": "https://huggingface.co/papers/2406.06133",
      "authors": [
        "Brian L. Curless",
        "Janne Kontkanen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06133.pdf",
      "abstract": "We propose ExtraNeRF, a novel method for extrapolating the range of views\nhandled by a Neural Radiance Field (NeRF). Our main idea is to leverage NeRFs\nto model scene-specific, fine-grained details, while capitalizing on diffusion\nmodels to extrapolate beyond our observed data. A key ingredient is to track\nvisibility to determine what portions of the scene have not been observed, and\nfocus on reconstructing those regions consistently with diffusion models. Our\nprimary contributions include a visibility-aware diffusion-based inpainting\nmodule that is fine-tuned on the input imagery, yielding an initial NeRF with\nmoderate quality (often blurry) inpainted regions, followed by a second\ndiffusion model trained on the input imagery to consistently enhance, notably\nsharpen, the inpainted imagery from the first pass. We demonstrate high-quality\nresults, extrapolating beyond a small number of (typically six or fewer) input\nviews, effectively outpainting the NeRF as well as inpainting newly disoccluded\nregions inside the original viewing volume. We compare with related work both\nquantitatively and qualitatively and show significant gains over prior art.",
      "upvotes": 7
    },
    {
      "title": "GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement",
      "url": "https://huggingface.co/papers/2406.05649",
      "authors": [
        "Sergey Korolev",
        "Sergey Tulyakov"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05649.pdf",
      "abstract": "We propose a novel approach for 3D mesh reconstruction from multi-view\nimages. Our method takes inspiration from large reconstruction models like LRM\nthat use a transformer-based triplane generator and a Neural Radiance Field\n(NeRF) model trained on multi-view images. However, in our method, we introduce\nseveral important modifications that allow us to significantly enhance 3D\nreconstruction quality. First of all, we examine the original LRM architecture\nand find several shortcomings. Subsequently, we introduce respective\nmodifications to the LRM architecture, which lead to improved multi-view image\nrepresentation and more computationally efficient training. Second, in order to\nimprove geometry reconstruction and enable supervision at full image\nresolution, we extract meshes from the NeRF field in a differentiable manner\nand fine-tune the NeRF model through mesh rendering. These modifications allow\nus to achieve state-of-the-art performance on both 2D and 3D evaluation\nmetrics, such as a PSNR of 28.67 on Google Scanned Objects (GSO) dataset.\nDespite these superior results, our feed-forward model still struggles to\nreconstruct complex textures, such as text and portraits on assets. To address\nthis, we introduce a lightweight per-instance texture refinement procedure.\nThis procedure fine-tunes the triplane representation and the NeRF color\nestimation model on the mesh surface using the input multi-view images in just\n4 seconds. This refinement improves the PSNR to 29.79 and achieves faithful\nreconstruction of complex textures, such as text. Additionally, our approach\nenables various downstream applications, including text- or image-to-3D\ngeneration.",
      "upvotes": 7
    }
  ]
}