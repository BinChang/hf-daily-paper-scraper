{
  "date": "2024-08-17",
  "papers": [
    {
      "title": "DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search",
      "url": "https://huggingface.co/papers/2408.08152",
      "authors": [
        "Z. Z. Ren",
        "Wanjia Zhao",
        "Haocheng Wang",
        "Liyue Zhang",
        "Qiushi Du",
        "Z. F. Wu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08152.pdf",
      "abstract": "We introduce DeepSeek-Prover-V1.5, an open-source language model designed for\ntheorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both\ntraining and inference processes. Pre-trained on DeepSeekMath-Base with\nspecialization in formal mathematical languages, the model undergoes supervised\nfine-tuning using an enhanced formal theorem proving dataset derived from\nDeepSeek-Prover-V1. Further refinement is achieved through reinforcement\nlearning from proof assistant feedback (RLPAF). Beyond the single-pass\nwhole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a\nvariant of Monte-Carlo tree search that employs an intrinsic-reward-driven\nexploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5\ndemonstrates significant improvements over DeepSeek-Prover-V1, achieving new\nstate-of-the-art results on the test set of the high school level miniF2F\nbenchmark (63.5%) and the undergraduate level ProofNet benchmark (25.3%).",
      "upvotes": 51
    },
    {
      "title": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm",
      "url": "https://huggingface.co/papers/2408.08072",
      "authors": [
        "Xingwei Qu",
        "Jiawei Guo",
        "Zhenzhu Yang",
        "Jiaheng Liu",
        "Chenghua Lin",
        "Lei Ma",
        "Jiajun Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08072.pdf",
      "abstract": "Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce I-SHEEP, an Iterative\nSelf-EnHancEmEnt Paradigm.This\nhuman-like paradigm enables LLMs to continuously self-align from\nscratch with nothing. Compared to the one-time alignment method Dromedary\nsun2023principledriven, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\nhttps://anonymous.4open.science/r/I-SHEEP.",
      "upvotes": 31
    },
    {
      "title": "Towards flexible perception with visual memory",
      "url": "https://huggingface.co/papers/2408.08172",
      "authors": [
        "Robert Geirhos",
        "Priyank Jaini",
        "Austin Stone",
        "Xi Yi",
        "George Toderici",
        "Abhijit Ogale",
        "Jonathon Shlens"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08172.pdf",
      "abstract": "Training a neural network is a monolithic endeavor, akin to carving knowledge\ninto stone: once the process is completed, editing the knowledge in a network\nis nearly impossible, since all information is distributed across the network's\nweights. We here explore a simple, compelling alternative by marrying the\nrepresentational power of deep neural networks with the flexibility of a\ndatabase. Decomposing the task of image classification into image similarity\n(from a pre-trained embedding) and search (via fast nearest neighbor retrieval\nfrom a knowledge database), we build a simple and flexible visual memory that\nhas the following key capabilities: (1.) The ability to flexibly add data\nacross scales: from individual samples all the way to entire classes and\nbillion-scale data; (2.) The ability to remove data through unlearning and\nmemory pruning; (3.) An interpretable decision-mechanism on which we can\nintervene to control its behavior. Taken together, these capabilities\ncomprehensively demonstrate the benefits of an explicit visual memory. We hope\nthat it might contribute to a conversation on how knowledge should be\nrepresented in deep vision models -- beyond carving it in ``stone'' weights.",
      "upvotes": 19
    },
    {
      "title": "Heavy Labels Out! Dataset Distillation with Label Space Lightening",
      "url": "https://huggingface.co/papers/2408.08201",
      "authors": [
        "Zigeng Chen",
        "Jingwen Ye",
        "Xinchao Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08201.pdf",
      "abstract": "Dataset distillation or condensation aims to condense a large-scale training\ndataset into a much smaller synthetic one such that the training performance of\ndistilled and original sets on neural networks are similar. Although the number\nof training samples can be reduced substantially, current state-of-the-art\nmethods heavily rely on enormous soft labels to achieve satisfactory\nperformance. As a result, the required storage can be comparable even to\noriginal datasets, especially for large-scale ones. To solve this problem,\ninstead of storing these heavy labels, we propose a novel label-lightening\nframework termed HeLlO aiming at effective image-to-label projectors, with\nwhich synthetic labels can be directly generated online from synthetic images.\nSpecifically, to construct such projectors, we leverage prior knowledge in\nopen-source foundation models, e.g., CLIP, and introduce a LoRA-like\nfine-tuning strategy to mitigate the gap between pre-trained and target\ndistributions, so that original models for soft-label generation can be\ndistilled into a group of low-rank matrices. Moreover, an effective image\noptimization method is proposed to further mitigate the potential error between\nthe original and distilled label generators. Extensive experiments demonstrate\nthat with only about 0.003% of the original storage required for a complete set\nof soft labels, we achieve comparable performance to current state-of-the-art\ndataset distillation methods on large-scale datasets. Our code will be\navailable.",
      "upvotes": 17
    },
    {
      "title": "Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability",
      "url": "https://huggingface.co/papers/2408.07852",
      "authors": [
        "Jiri Hron",
        "Laura Culp",
        "Gamaleldin Elsayed",
        "Rosanne Liu",
        "Ben Adlam",
        "Maxwell Bileschi",
        "Bernd Bohnet",
        "JD Co-Reyes",
        "Noah Fiedel",
        "C. Daniel Freeman",
        "Izzeddin Gur",
        "Kathleen Kenealy",
        "Jaehoon Lee",
        "Peter J. Liu",
        "Gaurav Mishra",
        "Igor Mordatch",
        "Azade Nova",
        "Roman Novak",
        "Aaron Parisi",
        "Jeffrey Pennington",
        "Alex Rizkowsky",
        "Isabelle Simpson"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07852.pdf",
      "abstract": "While many capabilities of language models (LMs) improve with increased\ntraining budget, the influence of scale on hallucinations is not yet fully\nunderstood. Hallucinations come in many forms, and there is no universally\naccepted definition. We thus focus on studying only those hallucinations where\na correct answer appears verbatim in the training set. To fully control the\ntraining data content, we construct a knowledge graph (KG)-based dataset, and\nuse it to train a set of increasingly large LMs. We find that for a fixed\ndataset, larger and longer-trained LMs hallucinate less. However, hallucinating\non leq5% of the training data requires an order of magnitude larger model,\nand thus an order of magnitude more compute, than Hoffmann et al. (2022)\nreported was optimal. Given this costliness, we study how hallucination\ndetectors depend on scale. While we see detector size improves performance on\nfixed LM's outputs, we find an inverse relationship between the scale of the LM\nand the detectability of its hallucinations.",
      "upvotes": 14
    },
    {
      "title": "FancyVideo: Towards Dynamic and Consistent Video Generation via Cross-frame Textual Guidance",
      "url": "https://huggingface.co/papers/2408.08189",
      "authors": [
        "Jing Wang",
        "Bo Cheng",
        "Xiaodan Liang",
        "Yuhui Yin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08189.pdf",
      "abstract": "Synthesizing motion-rich and temporally consistent videos remains a challenge\nin artificial intelligence, especially when dealing with extended durations.\nExisting text-to-video (T2V) models commonly employ spatial cross-attention for\ntext control, equivalently guiding different frame generations without\nframe-specific textual guidance. Thus, the model's capacity to comprehend the\ntemporal logic conveyed in prompts and generate videos with coherent motion is\nrestricted. To tackle this limitation, we introduce FancyVideo, an innovative\nvideo generator that improves the existing text-control mechanism with the\nwell-designed Cross-frame Textual Guidance Module (CTGM). Specifically, CTGM\nincorporates the Temporal Information Injector (TII), Temporal Affinity Refiner\n(TAR), and Temporal Feature Booster (TFB) at the beginning, middle, and end of\ncross-attention, respectively, to achieve frame-specific textual guidance.\nFirstly, TII injects frame-specific information from latent features into text\nconditions, thereby obtaining cross-frame textual conditions. Then, TAR refines\nthe correlation matrix between cross-frame textual conditions and latent\nfeatures along the time dimension. Lastly, TFB boosts the temporal consistency\nof latent features. Extensive experiments comprising both quantitative and\nqualitative evaluations demonstrate the effectiveness of FancyVideo. Our\napproach achieves state-of-the-art T2V generation results on the EvalCrafter\nbenchmark and facilitates the synthesis of dynamic and consistent videos. The\nvideo show results can be available at https://fancyvideo.github.io/, and we\nwill make our code and model weights publicly available.",
      "upvotes": 14
    },
    {
      "title": "BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts",
      "url": "https://huggingface.co/papers/2408.08274",
      "authors": [
        "Qizhen Zhang",
        "Nikolas Gritsch",
        "Dwaraknath Gnaneshwar",
        "Simon Guo",
        "David Cairuz",
        "Bharat Venkitesh",
        "Jakob Foerster",
        "Phil Blunsom",
        "Sebastian Ruder",
        "Ahmet Ustun",
        "Acyr Locatelli"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08274.pdf",
      "abstract": "The Mixture of Experts (MoE) framework has become a popular architecture for\nlarge language models due to its superior performance over dense models.\nHowever, training MoEs from scratch in a large-scale regime is prohibitively\nexpensive. Existing methods mitigate this by pre-training multiple dense expert\nmodels independently and using them to initialize an MoE. This is done by using\nexperts' feed-forward network (FFN) to initialize the MoE's experts while\nmerging other parameters. However, this method limits the reuse of dense model\nparameters to only the FFN layers, thereby constraining the advantages when\n\"upcycling\" these models into MoEs. We propose BAM (Branch-Attend-Mix), a\nsimple yet effective method that addresses this shortcoming. BAM makes full use\nof specialized dense models by not only using their FFN to initialize the MoE\nlayers but also leveraging experts' attention parameters fully by initializing\nthem into a soft-variant of Mixture of Attention (MoA) layers. We explore two\nmethods for upcycling attention parameters: 1) initializing separate attention\nexperts from dense models including all attention parameters for the best model\nperformance; and 2) sharing key and value parameters across all experts to\nfacilitate for better inference efficiency. To further improve efficiency, we\nadopt a parallel attention transformer architecture to MoEs, which allows the\nattention experts and FFN experts to be computed concurrently. Our experiments\non seed models ranging from 590 million to 2 billion parameters demonstrate\nthat BAM surpasses baselines in both perplexity and downstream task\nperformance, within the same computational and data constraints.",
      "upvotes": 11
    },
    {
      "title": "The ShareLM Collection and Plugin: Contributing Human-Model Chats for the Benefit of the Community",
      "url": "https://huggingface.co/papers/2408.08291",
      "authors": [
        "Shachar Don-Yehiya",
        "Omri Abend"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08291.pdf",
      "abstract": "Human-model conversations provide a window into users' real-world scenarios,\nbehavior, and needs, and thus are a valuable resource for model development and\nresearch. While for-profit companies collect user data through the APIs of\ntheir models, using it internally to improve their own models, the open source\nand research community lags behind.\n  We introduce the ShareLM collection, a unified set of human conversations\nwith large language models, and its accompanying plugin, a Web extension for\nvoluntarily contributing user-model conversations. Where few platforms share\ntheir chats, the ShareLM plugin adds this functionality, thus, allowing users\nto share conversations from most platforms. The plugin allows the user to rate\ntheir conversations, both at the conversation and the response levels, and\ndelete conversations they prefer to keep private before they ever leave the\nuser's local storage. We release the plugin conversations as part of the\nShareLM collection, and call for more community effort in the field of open\nhuman-model data.\n  The code, plugin, and data are available.",
      "upvotes": 9
    },
    {
      "title": "FuseChat: Knowledge Fusion of Chat Models",
      "url": "https://huggingface.co/papers/2408.07990",
      "authors": [
        "Longguang Zhong",
        "Ziyi Yang",
        "Ruijun Chen",
        "Xiaojun Quan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07990.pdf",
      "abstract": "While training large language models (LLMs) from scratch can indeed lead to\nmodels with distinct capabilities and strengths, it incurs substantial costs\nand may lead to redundancy in competencies. Knowledge fusion aims to integrate\nexisting LLMs of diverse architectures and capabilities into a more potent LLM\nthrough lightweight continual training, thereby reducing the need for costly\nLLM development. In this work, we propose a new framework for the knowledge\nfusion of chat LLMs through two main stages, resulting in FuseChat. Firstly, we\nconduct pairwise knowledge fusion on source chat LLMs of varying structures and\nscales to create multiple target LLMs with identical structure and size via\nlightweight fine-tuning. During this process, a statistics-based token\nalignment approach is introduced as the cornerstone for fusing LLMs with\ndifferent structures. Secondly, we merge these target LLMs within the parameter\nspace, where we propose a novel method for determining the merging coefficients\nbased on the magnitude of parameter updates before and after fine-tuning. We\nimplement and validate FuseChat using six prominent chat LLMs with diverse\narchitectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha,\nNH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and\nQwen-1.5-Chat-72B. Experimental results on two instruction-following\nbenchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of\nFuseChat-7B over baselines of various sizes. Our model is even comparable to\nthe larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench.\nOur code, model weights, and data are public at\nhttps://github.com/fanqiwan/FuseAI.",
      "upvotes": 9
    },
    {
      "title": "Accelerating High-Fidelity Waveform Generation via Adversarial Flow Matching Optimization",
      "url": "https://huggingface.co/papers/2408.08019",
      "authors": [
        "Seong-Whan Lee"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08019.pdf",
      "abstract": "This paper introduces PeriodWave-Turbo, a high-fidelity and high-efficient\nwaveform generation model via adversarial flow matching optimization. Recently,\nconditional flow matching (CFM) generative models have been successfully\nadopted for waveform generation tasks, leveraging a single vector field\nestimation objective for training. Although these models can generate\nhigh-fidelity waveform signals, they require significantly more ODE steps\ncompared to GAN-based models, which only need a single generation step.\nAdditionally, the generated samples often lack high-frequency information due\nto noisy vector field estimation, which fails to ensure high-frequency\nreproduction. To address this limitation, we enhance pre-trained CFM-based\ngenerative models by incorporating a fixed-step generator modification. We\nutilized reconstruction losses and adversarial feedback to accelerate\nhigh-fidelity waveform generation. Through adversarial flow matching\noptimization, it only requires 1,000 steps of fine-tuning to achieve\nstate-of-the-art performance across various objective metrics. Moreover, we\nsignificantly reduce inference speed from 16 steps to 2 or 4 steps.\nAdditionally, by scaling up the backbone of PeriodWave from 29M to 70M\nparameters for improved generalization, PeriodWave-Turbo achieves unprecedented\nperformance, with a perceptual evaluation of speech quality (PESQ) score of\n4.454 on the LibriTTS dataset. Audio samples, source code and checkpoints will\nbe available at https://github.com/sh-lee-prml/PeriodWave.",
      "upvotes": 9
    },
    {
      "title": "MVInpainter: Learning Multi-View Consistent Inpainting to Bridge 2D and 3D Editing",
      "url": "https://huggingface.co/papers/2408.08000",
      "authors": [
        "Chenjie Cao",
        "Chaohui Yu",
        "Yanwei Fu",
        "Fan Wang",
        "Xiangyang Xue"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08000.pdf",
      "abstract": "Novel View Synthesis (NVS) and 3D generation have recently achieved prominent\nimprovements. However, these works mainly focus on confined categories or\nsynthetic 3D assets, which are discouraged from generalizing to challenging\nin-the-wild scenes and fail to be employed with 2D synthesis directly.\nMoreover, these methods heavily depended on camera poses, limiting their\nreal-world applications. To overcome these issues, we propose MVInpainter,\nre-formulating the 3D editing as a multi-view 2D inpainting task. Specifically,\nMVInpainter partially inpaints multi-view images with the reference guidance\nrather than intractably generating an entirely novel view from scratch, which\nlargely simplifies the difficulty of in-the-wild NVS and leverages unmasked\nclues instead of explicit pose conditions. To ensure cross-view consistency,\nMVInpainter is enhanced by video priors from motion components and appearance\nguidance from concatenated reference key&value attention. Furthermore,\nMVInpainter incorporates slot attention to aggregate high-level optical flow\nfeatures from unmasked regions to control the camera movement with pose-free\ntraining and inference. Sufficient scene-level experiments on both\nobject-centric and forward-facing datasets verify the effectiveness of\nMVInpainter, including diverse tasks, such as multi-view object removal,\nsynthesis, insertion, and replacement. The project page is\nhttps://ewrfcas.github.io/MVInpainter/.",
      "upvotes": 7
    },
    {
      "title": "Can Large Language Models Understand Symbolic Graphics Programs?",
      "url": "https://huggingface.co/papers/2408.08313",
      "authors": [
        "Zeju Qiu",
        "Weiyang Liu",
        "Haiwen Feng",
        "Zhen Liu",
        "Tim Z. Xiao",
        "Katherine M. Collins",
        "Joshua B. Tenenbaum",
        "Adrian Weller",
        "Michael J. Black",
        "Bernhard Schölkopf"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08313.pdf",
      "abstract": "Assessing the capabilities of large language models (LLMs) is often\nchallenging, in part, because it is hard to find tasks to which they have not\nbeen exposed during training. We take one step to address this challenge by\nturning to a new task: focusing on symbolic graphics programs, which are a\npopular representation for graphics content that procedurally generates visual\ndata. LLMs have shown exciting promise towards program synthesis, but do they\nunderstand symbolic graphics programs? Unlike conventional programs, symbolic\ngraphics programs can be translated to graphics content. Here, we characterize\nan LLM's understanding of symbolic programs in terms of their ability to answer\nquestions related to the graphics content. This task is challenging as the\nquestions are difficult to answer from the symbolic programs alone -- yet, they\nwould be easy to answer from the corresponding graphics content as we verify\nthrough a human experiment. To understand symbolic programs, LLMs may need to\npossess the ability to imagine how the corresponding graphics content would\nlook without directly accessing the rendered visual content. We use this task\nto evaluate LLMs by creating a large benchmark for the semantic understanding\nof symbolic graphics programs. This benchmark is built via program-graphics\ncorrespondence, hence requiring minimal human efforts. We evaluate current LLMs\non our benchmark to elucidate a preliminary assessment of their ability to\nreason about visual scenes from programs. We find that this task distinguishes\nexisting LLMs and models considered good at reasoning perform better. Lastly,\nwe introduce Symbolic Instruction Tuning (SIT) to improve this ability.\nSpecifically, we query GPT4-o with questions and images generated by symbolic\nprograms. Such data are then used to finetune an LLM. We also find that SIT\ndata can improve the general instruction following ability of LLMs.",
      "upvotes": 6
    }
  ]
}