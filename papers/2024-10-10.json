{
  "date": "2024-10-10",
  "papers": [
    {
      "title": "Aria: An Open Multimodal Native Mixture-of-Experts Model",
      "url": "https://huggingface.co/papers/2410.05993",
      "authors": [
        "Yudong Liu",
        "Yue Wang",
        "Zhiqi Shen",
        "Bowen Qu",
        "Bei Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05993.pdf",
      "abstract": "Information comes in diverse modalities. Multimodal native AI models are\nessential to integrate real-world information and deliver comprehensive\nunderstanding. While proprietary multimodal native models exist, their lack of\nopenness imposes obstacles for adoptions, let alone adaptations. To fill this\ngap, we introduce Aria, an open multimodal native model with best-in-class\nperformance across a wide range of multimodal, language, and coding tasks. Aria\nis a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual\ntoken and text token, respectively. It outperforms Pixtral-12B and\nLlama3.2-11B, and is competitive against the best proprietary models on various\nmultimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline,\nwhich progressively equips the model with strong capabilities in language\nunderstanding, multimodal understanding, long context window, and instruction\nfollowing. We open-source the model weights along with a codebase that\nfacilitates easy adoptions and adaptations of Aria in real-world applications.",
      "upvotes": 107
    },
    {
      "title": "GLEE: A Unified Framework and Benchmark for Language-based Economic Environments",
      "url": "https://huggingface.co/papers/2410.05254",
      "authors": [
        "Samuel Joseph Amouyal",
        "Roi Reichart",
        "Moshe Tennenholtz"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05254.pdf",
      "abstract": "Large Language Models (LLMs) show significant potential in economic and\nstrategic interactions, where communication via natural language is often\nprevalent. This raises key questions: Do LLMs behave rationally? Can they mimic\nhuman behavior? Do they tend to reach an efficient and fair outcome? What is\nthe role of natural language in the strategic interaction? How do\ncharacteristics of the economic environment influence these dynamics? These\nquestions become crucial concerning the economic and societal implications of\nintegrating LLM-based agents into real-world data-driven systems, such as\nonline retail platforms and recommender systems. While the ML community has\nbeen exploring the potential of LLMs in such multi-agent setups, varying\nassumptions, design choices and evaluation criteria across studies make it\ndifficult to draw robust and meaningful conclusions. To address this, we\nintroduce a benchmark for standardizing research on two-player, sequential,\nlanguage-based games. Inspired by the economic literature, we define three base\nfamilies of games with consistent parameterization, degrees of freedom and\neconomic measures to evaluate agents' performance (self-gain), as well as the\ngame outcome (efficiency and fairness). We develop an open-source framework for\ninteraction simulation and analysis, and utilize it to collect a dataset of LLM\nvs. LLM interactions across numerous game configurations and an additional\ndataset of human vs. LLM interactions. Through extensive experimentation, we\ndemonstrate how our framework and dataset can be used to: (i) compare the\nbehavior of LLM-based agents to human players in various economic contexts;\n(ii) evaluate agents in both individual and collective performance measures;\nand (iii) quantify the effect of the economic characteristics of the\nenvironments on the behavior of agents.",
      "upvotes": 80
    },
    {
      "title": "Personalized Visual Instruction Tuning",
      "url": "https://huggingface.co/papers/2410.07113",
      "authors": [
        "Jipeng Zhang",
        "Tong Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07113.pdf",
      "abstract": "Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated significant progress; however, these models exhibit a notable\nlimitation, which we refer to as \"face blindness\". Specifically, they can\nengage in general conversations but fail to conduct personalized dialogues\ntargeting at specific individuals. This deficiency hinders the application of\nMLLMs in personalized settings, such as tailored visual assistants on mobile\ndevices, or domestic robots that need to recognize members of the family. In\nthis paper, we introduce Personalized Visual Instruction Tuning (PVIT), a novel\ndata curation and training framework designed to enable MLLMs to identify\ntarget individuals within an image and engage in personalized and coherent\ndialogues. Our approach involves the development of a sophisticated pipeline\nthat autonomously generates training data containing personalized\nconversations. This pipeline leverages the capabilities of various visual\nexperts, image generation models, and (multi-modal) large language models. To\nevaluate the personalized potential of MLLMs, we present a benchmark called\nP-Bench, which encompasses various question types with different levels of\ndifficulty. The experiments demonstrate a substantial personalized performance\nenhancement after fine-tuning with our curated dataset.",
      "upvotes": 69
    },
    {
      "title": "Pixtral 12B",
      "url": "https://huggingface.co/papers/2410.07073",
      "authors": [
        "Jessica Chudnovsky",
        "Amélie Héliou",
        "Paul Jacob",
        "Andy Lo",
        "William Marshall",
        "Louis Martin",
        "Pavankumar Muddireddy"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07073.pdf",
      "abstract": "We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.\nPixtral-12B is trained to understand both natural images and documents,\nachieving leading performance on various multimodal benchmarks, surpassing a\nnumber of larger models. Unlike many open-source models, Pixtral is also a\ncutting-edge text model for its size, and does not compromise on natural\nlanguage performance to excel in multimodal tasks. Pixtral uses a new vision\nencoder trained from scratch, which allows it to ingest images at their natural\nresolution and aspect ratio. This gives users flexibility on the number of\ntokens used to process an image. Pixtral is also able to process any number of\nimages in its long context window of 128K tokens. Pixtral 12B substanially\noutperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B).\nIt also outperforms much larger open models like Llama-3.2 90B while being 7x\nsmaller. We further contribute an open-source benchmark, MM-MT-Bench, for\nevaluating vision-language models in practical scenarios, and provide detailed\nanalysis and code for standardized evaluation protocols for multimodal LLMs.\nPixtral-12B is released under Apache 2.0 license.",
      "upvotes": 59
    },
    {
      "title": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation",
      "url": "https://huggingface.co/papers/2410.05363",
      "authors": [
        "Xinyu Tan",
        "Dianqi Li",
        "Yu Qiao",
        "Ping Luo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05363.pdf",
      "abstract": "Text-to-video (T2V) models like Sora have made significant strides in\nvisualizing complex prompts, which is increasingly viewed as a promising path\ntowards constructing the universal world simulator. Cognitive psychologists\nbelieve that the foundation for achieving this goal is the ability to\nunderstand intuitive physics. However, the capacity of these models to\naccurately represent intuitive physics remains largely unexplored. To bridge\nthis gap, we introduce PhyGenBench, a comprehensive Physics\nGeneration Benchmark designed to evaluate physical\ncommonsense correctness in T2V generation. PhyGenBench comprises 160 carefully\ncrafted prompts across 27 distinct physical laws, spanning four fundamental\ndomains, which could comprehensively assesses models' understanding of physical\ncommonsense. Alongside PhyGenBench, we propose a novel evaluation framework\ncalled PhyGenEval. This framework employs a hierarchical evaluation structure\nutilizing appropriate advanced vision-language models and large language models\nto assess physical commonsense. Through PhyGenBench and PhyGenEval, we can\nconduct large-scale automated assessments of T2V models' understanding of\nphysical commonsense, which align closely with human feedback. Our evaluation\nresults and in-depth analysis demonstrate that current models struggle to\ngenerate videos that comply with physical commonsense. Moreover, simply scaling\nup models or employing prompt engineering techniques is insufficient to fully\naddress the challenges presented by PhyGenBench (e.g., dynamic scenarios). We\nhope this study will inspire the community to prioritize the learning of\nphysical commonsense in these models beyond entertainment applications. We will\nrelease the data and codes at https://github.com/OpenGVLab/PhyGenBench",
      "upvotes": 44
    },
    {
      "title": "IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation",
      "url": "https://huggingface.co/papers/2410.07171",
      "authors": [
        "Ling Yang",
        "Guohao Li",
        "Yong Tang",
        "Mengdi Wang",
        "Bin Cui"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07171.pdf",
      "abstract": "Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made\nnotable strides in compositional text-to-image generation. However, these\nmethods typically exhibit distinct strengths for compositional generation, with\nsome excelling in handling attribute binding and others in spatial\nrelationships. This disparity highlights the need for an approach that can\nleverage the complementary strengths of various models to comprehensively\nimprove the composition capability. To this end, we introduce IterComp, a novel\nframework that aggregates composition-aware model preferences from multiple\nmodels and employs an iterative feedback learning approach to enhance\ncompositional generation. Specifically, we curate a gallery of six powerful\nopen-source diffusion models and evaluate their three key compositional\nmetrics: attribute binding, spatial relationships, and non-spatial\nrelationships. Based on these metrics, we develop a composition-aware model\npreference dataset comprising numerous image-rank pairs to train\ncomposition-aware reward models. Then, we propose an iterative feedback\nlearning method to enhance compositionality in a closed-loop manner, enabling\nthe progressive self-refinement of both the base diffusion model and reward\nmodels over multiple iterations. Theoretical proof demonstrates the\neffectiveness and extensive experiments show our significant superiority over\nprevious SOTA methods (e.g., Omost and FLUX), particularly in multi-category\nobject composition and complex semantic alignment. IterComp opens new research\navenues in reward feedback learning for diffusion models and compositional\ngeneration. Code: https://github.com/YangLing0818/IterComp",
      "upvotes": 41
    },
    {
      "title": "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching",
      "url": "https://huggingface.co/papers/2410.06885",
      "authors": [
        "Zhikang Niu",
        "Ziyang Ma",
        "Jian Zhao",
        "Xie Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06885.pdf",
      "abstract": "This paper introduces F5-TTS, a fully non-autoregressive text-to-speech\nsystem based on flow matching with Diffusion Transformer (DiT). Without\nrequiring complex designs such as duration model, text encoder, and phoneme\nalignment, the text input is simply padded with filler tokens to the same\nlength as input speech, and then the denoising is performed for speech\ngeneration, which was originally proved feasible by E2 TTS. However, the\noriginal design of E2 TTS makes it hard to follow due to its slow convergence\nand low robustness. To address these issues, we first model the input with\nConvNeXt to refine the text representation, making it easy to align with the\nspeech. We further propose an inference-time Sway Sampling strategy, which\nsignificantly improves our model's performance and efficiency. This sampling\nstrategy for flow step can be easily applied to existing flow matching based\nmodels without retraining. Our design allows faster training and achieves an\ninference RTF of 0.15, which is greatly improved compared to state-of-the-art\ndiffusion-based TTS models. Trained on a public 100K hours multilingual\ndataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching\n(F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless\ncode-switching capability, and speed control efficiency. Demo samples can be\nfound at https://SWivid.github.io/F5-TTS. We release all code and checkpoints\nto promote community development.",
      "upvotes": 40
    },
    {
      "title": "Pyramidal Flow Matching for Efficient Video Generative Modeling",
      "url": "https://huggingface.co/papers/2410.05954",
      "authors": [
        "Kun Xu",
        "Kun Xu",
        "Hao Jiang",
        "Yang Song",
        "Yadong Mu",
        "Zhouchen Lin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05954.pdf",
      "abstract": "Video generation requires modeling a vast spatiotemporal space, which demands\nsignificant computational resources and data usage. To reduce the complexity,\nthe prevailing approaches employ a cascaded architecture to avoid direct\ntraining with full resolution. Despite reducing computational demands, the\nseparate optimization of each sub-stage hinders knowledge sharing and\nsacrifices flexibility. This work introduces a unified pyramidal flow matching\nalgorithm. It reinterprets the original denoising trajectory as a series of\npyramid stages, where only the final stage operates at the full resolution,\nthereby enabling more efficient video generative modeling. Through our\nsophisticated design, the flows of different pyramid stages can be interlinked\nto maintain continuity. Moreover, we craft autoregressive video generation with\na temporal pyramid to compress the full-resolution history. The entire\nframework can be optimized in an end-to-end manner and with a single unified\nDiffusion Transformer (DiT). Extensive experiments demonstrate that our method\nsupports generating high-quality 5-second (up to 10-second) videos at 768p\nresolution and 24 FPS within 20.7k A100 GPU training hours. All code and models\nwill be open-sourced at https://pyramid-flow.github.io.",
      "upvotes": 37
    },
    {
      "title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate",
      "url": "https://huggingface.co/papers/2410.07167",
      "authors": [
        "Xiaoyi Dong",
        "Pan Zhang",
        "Yuhang Cao",
        "Jiaqi Wang",
        "Weiming Zhang",
        "Nenghai Yu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07167.pdf",
      "abstract": "We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) Effective to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) Robust toward different training/evaluation\ndata. 3) Generalize across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.",
      "upvotes": 37
    },
    {
      "title": "Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning",
      "url": "https://huggingface.co/papers/2410.06373",
      "authors": [
        "Luyuan Zhang",
        "Yang Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06373.pdf",
      "abstract": "This paper delves into the interplay between vision backbones and optimizers,\nunvealing an inter-dependent phenomenon termed\n\\textbf{backbone-optimizer coupling bias}\n(BOCB). We observe that canonical CNNs, such as VGG and ResNet, exhibit a\nmarked co-dependency with SGD families, while recent architectures like ViTs\nand ConvNeXt share a tight coupling with the adaptive learning rate ones. We\nfurther show that BOCB can be introduced by both optimizers and certain\nbackbone designs and may significantly impact the pre-training and downstream\nfine-tuning of vision models. Through in-depth empirical analysis, we summarize\ntakeaways on recommended optimizers and insights into robust vision backbone\narchitectures. We hope this work can inspire the community to question\nlong-held assumptions on backbones and optimizers, stimulate further\nexplorations, and thereby contribute to more robust vision systems. The source\ncode and models are publicly available at https://bocb-ai.github.io/.",
      "upvotes": 34
    },
    {
      "title": "Falcon Mamba: The First Competitive Attention-free 7B Language Model",
      "url": "https://huggingface.co/papers/2410.05355",
      "authors": [
        "Dhia Eddine Rhaiem",
        "Ilyas Chahed",
        "Hakim Hacid"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05355.pdf",
      "abstract": "In this technical report, we present Falcon Mamba 7B, a new base large\nlanguage model based on the novel Mamba architecture. Falcon Mamba 7B is\ntrained on 5.8 trillion tokens with carefully selected data mixtures. As a pure\nMamba-based model, Falcon Mamba 7B surpasses leading open-weight models based\non Transformers, such as Mistral 7B, Llama3.1 8B, and Falcon2 11B. It is on par\nwith Gemma 7B and outperforms models with different architecture designs, such\nas RecurrentGemma 9B and RWKV-v6 Finch 7B/14B. Currently, Falcon Mamba 7B is\nthe best-performing Mamba model in the literature at this scale, surpassing\nboth existing Mamba and hybrid Mamba-Transformer models, according to the Open\nLLM Leaderboard. Due to its architecture, Falcon Mamba 7B is significantly\nfaster at inference and requires substantially less memory for long sequence\ngeneration. Despite recent studies suggesting that hybrid Mamba-Transformer\nmodels outperform pure architecture designs, we demonstrate that even the pure\nMamba design can achieve similar, or even superior results compared to the\nTransformer and hybrid designs. We make the weights of our implementation of\nFalcon Mamba 7B publicly available on\nhttps://huggingface.co/tiiuae/falcon-mamba-7b, under a permissive license.",
      "upvotes": 28
    },
    {
      "title": "MM-Ego: Towards Building Egocentric Multimodal LLMs",
      "url": "https://huggingface.co/papers/2410.07177",
      "authors": [
        "Lin Chen",
        "Zongyu Lin",
        "Bowen Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07177.pdf",
      "abstract": "This research aims to comprehensively explore building a multimodal\nfoundation model for egocentric video understanding. To achieve this goal, we\nwork on three fronts. First, as there is a lack of QA data for egocentric video\nunderstanding, we develop a data engine that efficiently generates 7M\nhigh-quality QA samples for egocentric videos ranging from 30 seconds to one\nhour long, based on human-annotated data. This is currently the largest\negocentric QA dataset. Second, we contribute a challenging egocentric QA\nbenchmark with 629 videos and 7,026 questions to evaluate the models' ability\nin recognizing and memorizing visual details across videos of varying lengths.\nWe introduce a new de-biasing evaluation method to help mitigate the\nunavoidable language bias present in the models being evaluated. Third, we\npropose a specialized multimodal architecture featuring a novel \"Memory Pointer\nPrompting\" mechanism. This design includes a global glimpse step to gain an\noverarching understanding of the entire video and identify key visual\ninformation, followed by a fallback step that utilizes the key visual\ninformation to generate responses. This enables the model to more effectively\ncomprehend extended video content. With the data, benchmark, and model, we\nsuccessfully build MM-Ego, an egocentric multimodal LLM that shows powerful\nperformance on egocentric video understanding.",
      "upvotes": 20
    },
    {
      "title": "Story-Adapter: A Training-free Iterative Framework for Long Story Visualization",
      "url": "https://huggingface.co/papers/2410.06244",
      "authors": [
        "Yuanqi Chang",
        "Bingjie Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06244.pdf",
      "abstract": "Story visualization, the task of generating coherent images based on a\nnarrative, has seen significant advancements with the emergence of\ntext-to-image models, particularly diffusion models. However, maintaining\nsemantic consistency, generating high-quality fine-grained interactions, and\nensuring computational feasibility remain challenging, especially in long story\nvisualization (i.e., up to 100 frames). In this work, we propose a\ntraining-free and computationally efficient framework, termed Story-Adapter, to\nenhance the generative capability of long stories. Specifically, we propose an\niterative paradigm to refine each generated image, leveraging both the text\nprompt and all generated images from the previous iteration. Central to our\nframework is a training-free global reference cross-attention module, which\naggregates all generated images from the previous iteration to preserve\nsemantic consistency across the entire story, while minimizing computational\ncosts with global embeddings. This iterative process progressively optimizes\nimage generation by repeatedly incorporating text constraints, resulting in\nmore precise and fine-grained interactions. Extensive experiments validate the\nsuperiority of Story-Adapter in improving both semantic consistency and\ngenerative capability for fine-grained interactions, particularly in long story\nscenarios. The project page and associated code can be accessed via\nhttps://jwmao1.github.io/storyadapter .",
      "upvotes": 19
    },
    {
      "title": "One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation",
      "url": "https://huggingface.co/papers/2410.07170",
      "authors": [
        "Benedikt Alkin",
        "Marc Peter Deisenroth",
        "Sepp Hochreiter"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07170.pdf",
      "abstract": "Foundation models (FMs) are pre-trained on large-scale datasets and then\nfine-tuned on a downstream task for a specific application. The most successful\nand most commonly used fine-tuning method is to update the pre-trained weights\nvia a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are\nusually initialized at random with a uniform rank distribution across model\nweights. Recent works focus on weight-driven initialization or learning of\nadaptive ranks during training. Both approaches have only been investigated in\nisolation, resulting in slow convergence or a uniform rank distribution, in\nturn leading to sub-optimal performance. We propose to enhance LoRA by\ninitializing the new weights in a data-driven manner by computing singular\nvalue decomposition on minibatches of activation vectors. Then, we initialize\nthe LoRA matrices with the obtained right-singular vectors and re-distribute\nranks among all weight matrices to explain the maximal amount of variance and\ncontinue the standard LoRA fine-tuning procedure. This results in our new\nmethod Explained Variance Adaptation (EVA). We apply EVA to a variety of\nfine-tuning tasks ranging from language generation and understanding to image\nclassification and reinforcement learning. EVA exhibits faster convergence than\ncompetitors and attains the highest average score across a multitude of tasks\nper domain.",
      "upvotes": 15
    },
    {
      "title": "Self-Boosting Large Language Models with Synthetic Preference Data",
      "url": "https://huggingface.co/papers/2410.06961",
      "authors": [
        "Zhifang Sui"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06961.pdf",
      "abstract": "Through alignment with human preferences, Large Language Models (LLMs) have\nadvanced significantly in generating honest, harmless, and helpful responses.\nHowever, collecting high-quality preference data is a resource-intensive and\ncreativity-demanding process, especially for the continual improvement of LLMs.\nWe introduce SynPO, a self-boosting paradigm that leverages synthetic\npreference data for model alignment. SynPO employs an iterative mechanism\nwherein a self-prompt generator creates diverse prompts, and a response\nimprover refines model responses progressively. This approach trains LLMs to\nautonomously learn the generative rewards for their own outputs and eliminates\nthe need for large-scale annotation of prompts and human preferences. After\nfour SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements\nin instruction-following abilities, achieving over 22.1% win rate improvements\non AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general\nperformance of LLMs on various tasks, validated by a 3.2 to 5.0 average score\nincrease on the well-recognized Open LLM leaderboard.",
      "upvotes": 15
    },
    {
      "title": "T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design",
      "url": "https://huggingface.co/papers/2410.05677",
      "authors": [
        "Qian Long"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05677.pdf",
      "abstract": "In this paper, we focus on enhancing a diffusion-based text-to-video (T2V)\nmodel during the post-training phase by distilling a highly capable consistency\nmodel from a pretrained T2V model. Our proposed method, T2V-Turbo-v2,\nintroduces a significant advancement by integrating various supervision\nsignals, including high-quality training data, reward model feedback, and\nconditional guidance, into the consistency distillation process. Through\ncomprehensive ablation studies, we highlight the crucial importance of\ntailoring datasets to specific learning objectives and the effectiveness of\nlearning from diverse reward models for enhancing both the visual quality and\ntext-video alignment. Additionally, we highlight the vast design space of\nconditional guidance strategies, which centers on designing an effective energy\nfunction to augment the teacher ODE solver. We demonstrate the potential of\nthis approach by extracting motion guidance from the training datasets and\nincorporating it into the ODE solver, showcasing its effectiveness in improving\nthe motion quality of the generated videos with the improved motion-related\nmetrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2\nestablishes a new state-of-the-art result on VBench, with a Total score of\n85.13, surpassing proprietary systems such as Gen-3 and Kling.",
      "upvotes": 14
    },
    {
      "title": "CursorCore: Assist Programming through Aligning Anything",
      "url": "https://huggingface.co/papers/2410.07002",
      "authors": [
        "Qi Liu",
        "Rui Li",
        "Shijin Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07002.pdf",
      "abstract": "Large language models have been successfully applied to programming\nassistance tasks, such as code completion, code insertion, and instructional\ncode editing. However, these applications remain insufficiently automated and\nstruggle to effectively integrate various types of information during the\nprogramming process, including coding history, current code, and user\ninstructions. In this work, we propose a new conversational framework that\ncomprehensively integrates these information sources, collect data to train our\nmodels and evaluate their performance. Firstly, to thoroughly evaluate how well\nmodels align with different types of information and the quality of their\noutputs, we introduce a new benchmark, APEval (Assist Programming Eval), to\ncomprehensively assess the performance of models in programming assistance\ntasks. Then, for data collection, we develop a data generation pipeline,\nProgramming-Instruct, which synthesizes training data from diverse sources,\nsuch as GitHub and online judge platforms. This pipeline can automatically\ngenerate various types of messages throughout the programming process. Finally,\nusing this pipeline, we generate 219K samples, fine-tune multiple models, and\ndevelop the CursorCore series. We show that CursorCore outperforms other models\nof comparable size. This framework unifies applications such as inline chat and\nautomated editing, contributes to the advancement of coding assistants. Code,\nmodels and data are freely available at\nhttps://github.com/TechxGenus/CursorCore.",
      "upvotes": 13
    },
    {
      "title": "TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation",
      "url": "https://huggingface.co/papers/2410.05591",
      "authors": [
        "Jong Chul Ye"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05591.pdf",
      "abstract": "Despite significant advancements in customizing text-to-image and video\ngeneration models, generating images and videos that effectively integrate\nmultiple personalized concepts remains a challenging task. To address this, we\npresent TweedieMix, a novel method for composing customized diffusion models\nduring the inference phase. By analyzing the properties of reverse diffusion\nsampling, our approach divides the sampling process into two stages. During the\ninitial steps, we apply a multiple object-aware sampling technique to ensure\nthe inclusion of the desired target objects. In the later steps, we blend the\nappearances of the custom concepts in the de-noised image space using Tweedie's\nformula. Our results demonstrate that TweedieMix can generate multiple\npersonalized concepts with higher fidelity than existing methods. Moreover, our\nframework can be effortlessly extended to image-to-video diffusion models,\nenabling the generation of videos that feature multiple personalized concepts.\nResults and source code are in our anonymous project page.",
      "upvotes": 13
    },
    {
      "title": "ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler",
      "url": "https://huggingface.co/papers/2410.05651",
      "authors": [
        "Taesung Kwon",
        "Jong Chul Ye"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05651.pdf",
      "abstract": "Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V)\ndiffusion models has greatly enhanced video generation, especially in terms of\nkeyframe interpolation. However, current image-to-video diffusion models, while\npowerful in generating videos from a single conditioning frame, need adaptation\nfor two-frame (start & end) conditioned generation, which is essential for\neffective bounded interpolation. Unfortunately, existing approaches that fuse\ntemporally forward and backward paths in parallel often suffer from\noff-manifold issues, leading to artifacts or requiring multiple iterative\nre-noising steps. In this work, we introduce a novel, bidirectional sampling\nstrategy to address these off-manifold issues without requiring extensive\nre-noising or fine-tuning. Our method employs sequential sampling along both\nforward and backward paths, conditioned on the start and end frames,\nrespectively, ensuring more coherent and on-manifold generation of intermediate\nframes. Additionally, we incorporate advanced guidance techniques, CFG++ and\nDDS, to further enhance the interpolation process. By integrating these, our\nmethod achieves state-of-the-art performance, efficiently generating\nhigh-quality, smooth videos between keyframes. On a single 3090 GPU, our method\ncan interpolate 25 frames at 1024 x 576 resolution in just 195 seconds,\nestablishing it as a leading solution for keyframe interpolation.",
      "upvotes": 13
    },
    {
      "title": "Response Tuning: Aligning Large Language Models without Instruction",
      "url": "https://huggingface.co/papers/2410.02465",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.02465.pdf",
      "abstract": "Instruction tuning-supervised fine-tuning using instruction-response pairs-is\na foundational step in transitioning pre-trained Large Language Models (LLMs)\ninto helpful and safe chat assistants. Our hypothesis is that establishing an\nadequate output space can enable such a transition given the capabilities\ninherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT),\nwhich eliminates the instruction-conditioning step in instruction tuning and\nsolely focuses on response space supervision. Our experiments demonstrate that\nRT models, trained only using responses, can effectively respond to a wide\nrange of instructions and exhibit helpfulness comparable to that of their\ninstruction-tuned counterparts. Furthermore, we observe that controlling the\ntraining response distribution can significantly improve their user preference\nor elicit target behaviors such as refusing assistance for unsafe queries. Our\nfindings illuminate the role of establishing an adequate output space in\nalignment, highlighting the potential of the extensive inherent capabilities of\npre-trained LLMs.",
      "upvotes": 12
    },
    {
      "title": "AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs",
      "url": "https://huggingface.co/papers/2410.05295",
      "authors": [
        "Edward Suh",
        "Yevgeniy Vorobeychik",
        "Zhuoqing Mao",
        "Patrick McDaniel",
        "Bo Li",
        "Chaowei Xiao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05295.pdf",
      "abstract": "In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that\ncan automatically discover as many jailbreak strategies as possible from\nscratch, without any human intervention or predefined scopes (e.g., specified\ncandidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo\ncan significantly outperform baseline methods, achieving a 74.3% higher average\nattack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an\n88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a\nunified framework that can incorporate existing human-designed jailbreak\nstrategies in a plug-and-play manner. By integrating human-designed strategies,\nAutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on\nGPT-4-1106-turbo.",
      "upvotes": 12
    },
    {
      "title": "Temporal Reasoning Transfer from Text to Video",
      "url": "https://huggingface.co/papers/2410.06166",
      "authors": [
        "Xu Sun",
        "Lingpeng Kong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06166.pdf",
      "abstract": "Video Large Language Models (Video LLMs) have shown promising capabilities in\nvideo comprehension, yet they struggle with tracking temporal changes and\nreasoning about temporal relationships. While previous research attributed this\nlimitation to the ineffective temporal encoding of visual inputs, our\ndiagnostic study reveals that video representations contain sufficient\ninformation for even small probing classifiers to achieve perfect accuracy.\nSurprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning\ncapability stems from the underlying LLM's inherent difficulty with temporal\nconcepts, as evidenced by poor performance on textual temporal\nquestion-answering tasks. Building on this discovery, we introduce the Textual\nTemporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning\ntasks in pure text format from existing image-text datasets, addressing the\nscarcity of video samples with complex temporal scenarios. Remarkably, without\nusing any video data, T3 enhances LongVA-7B's temporal understanding, yielding\na 5.3 absolute accuracy improvement on the challenging TempCompass benchmark,\nwhich enables our model to outperform ShareGPT4Video-8B trained on 28,000 video\nsamples. Additionally, the enhanced LongVA-7B model achieves competitive\nperformance on comprehensive video benchmarks. For example, it achieves a 49.7\naccuracy on the Temporal Reasoning task of Video-MME, surpassing powerful\nlarge-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Further\nanalysis reveals a strong correlation between textual and video temporal task\nperformance, validating the efficacy of transferring temporal reasoning\nabilities from text to video domains.",
      "upvotes": 12
    },
    {
      "title": "Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis",
      "url": "https://huggingface.co/papers/2410.07155",
      "authors": [
        "Ling Yang",
        "Siyu Li",
        "Jiaming Liu",
        "Zixiang Zhang",
        "Kaixin Zhu",
        "Yongzhen Guo",
        "Minkai Xu",
        "Stefano Ermon",
        "Wentao Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07155.pdf",
      "abstract": "Recent advances in diffusion models have demonstrated exceptional\ncapabilities in image and video generation, further improving the effectiveness\nof 4D synthesis. Existing 4D generation methods can generate high-quality 4D\nobjects or scenes based on user-friendly conditions, benefiting the gaming and\nvideo industries. However, these methods struggle to synthesize significant\nobject deformation of complex 4D transitions and interactions within scenes. To\naddress this challenge, we propose Trans4D, a novel text-to-4D synthesis\nframework that enables realistic complex scene transitions. Specifically, we\nfirst use multi-modal large language models (MLLMs) to produce a physic-aware\nscene description for 4D scene initialization and effective transition timing\nplanning. Then we propose a geometry-aware 4D transition network to realize a\ncomplex scene-level 4D transition based on the plan, which involves expressive\ngeometrical object deformation. Extensive experiments demonstrate that Trans4D\nconsistently outperforms existing state-of-the-art methods in generating 4D\nscenes with accurate and high-quality transitions, validating its\neffectiveness. Code: https://github.com/YangLing0818/Trans4D",
      "upvotes": 11
    },
    {
      "title": "Diversity-Rewarded CFG Distillation",
      "url": "https://huggingface.co/papers/2410.06084",
      "authors": [
        "Geoffrey Cideron",
        "Sertan Girgin",
        "Romuald Elie"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06084.pdf",
      "abstract": "Generative models are transforming creative domains such as music generation,\nwith inference-time strategies like Classifier-Free Guidance (CFG) playing a\ncrucial role. However, CFG doubles inference cost while limiting originality\nand diversity across generated contents. In this paper, we introduce\ndiversity-rewarded CFG distillation, a novel finetuning procedure that distills\nthe strengths of CFG while addressing its limitations. Our approach optimises\ntwo training objectives: (1) a distillation objective, encouraging the model\nalone (without CFG) to imitate the CFG-augmented predictions, and (2) an RL\nobjective with a diversity reward, promoting the generation of diverse outputs\nfor a given prompt. By finetuning, we learn model weights with the ability to\ngenerate high-quality and diverse outputs, without any inference overhead. This\nalso unlocks the potential of weight-based model merging strategies: by\ninterpolating between the weights of two models (the first focusing on quality,\nthe second on diversity), we can control the quality-diversity trade-off at\ndeployment time, and even further boost performance. We conduct extensive\nexperiments on the MusicLM (Agostinelli et al., 2023) text-to-music generative\nmodel, where our approach surpasses CFG in terms of quality-diversity Pareto\noptimality. According to human evaluators, our finetuned-then-merged model\ngenerates samples with higher quality-diversity than the base model augmented\nwith CFG. Explore our generations at\nhttps://google-research.github.io/seanet/musiclm/diverse_music/.",
      "upvotes": 10
    },
    {
      "title": "BroadWay: Boost Your Text-to-Video Generation Model in a Training-free Way",
      "url": "https://huggingface.co/papers/2410.06241",
      "authors": [
        "Jiazi Bu",
        "Pengyang Ling",
        "Pan Zhang",
        "Tong Wu",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Jiaqi Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06241.pdf",
      "abstract": "The text-to-video (T2V) generation models, offering convenient visual\ncreation, have recently garnered increasing attention. Despite their\nsubstantial potential, the generated videos may present artifacts, including\nstructural implausibility, temporal inconsistency, and a lack of motion, often\nresulting in near-static video. In this work, we have identified a correlation\nbetween the disparity of temporal attention maps across different blocks and\nthe occurrence of temporal inconsistencies. Additionally, we have observed that\nthe energy contained within the temporal attention maps is directly related to\nthe magnitude of motion amplitude in the generated videos. Based on these\nobservations, we present BroadWay, a training-free method to improve the\nquality of text-to-video generation without introducing additional parameters,\naugmenting memory or sampling time. Specifically, BroadWay is composed of two\nprincipal components: 1) Temporal Self-Guidance improves the structural\nplausibility and temporal consistency of generated videos by reducing the\ndisparity between the temporal attention maps across various decoder blocks. 2)\nFourier-based Motion Enhancement enhances the magnitude and richness of motion\nby amplifying the energy of the map. Extensive experiments demonstrate that\nBroadWay significantly improves the quality of text-to-video generation with\nnegligible additional cost.",
      "upvotes": 10
    },
    {
      "title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",
      "url": "https://huggingface.co/papers/2410.06458",
      "authors": [
        "Yu-Hsiang Lin",
        "Haw-Shiuan Chang",
        "Shereen Oraby",
        "Vivek Subramanian",
        "Tagyoung Chung"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06458.pdf",
      "abstract": "Instruction following is a key capability for LLMs. However, recent studies\nhave shown that LLMs often struggle with instructions containing multiple\nconstraints (e.g. a request to create a social media post \"in a funny tone\"\nwith \"no hashtag\"). Despite this, most evaluations focus solely on synthetic\ndata. To address this, we introduce RealInstruct, the first benchmark designed\nto evaluate LLMs' ability to follow real-world multi-constrained instructions\nby leveraging queries real users asked AI assistants. We also investigate\nmodel-based evaluation as a cost-effective alternative to human annotation for\nthis task. Our findings reveal that even the proprietary GPT-4 model fails to\nmeet at least one constraint on over 21% of instructions, highlighting the\nlimitations of state-of-the-art models. To address the performance gap between\nopen-source and proprietary models, we propose the Decompose, Critique and\nRefine (DeCRIM) self-correction pipeline, which enhances LLMs' ability to\nfollow constraints. DeCRIM works by decomposing the original instruction into a\nlist of constraints and using a Critic model to decide when and where the LLM's\nresponse needs refinement. Our results show that DeCRIM improves Mistral's\nperformance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback.\nMoreover, we demonstrate that with strong feedback, open-source LLMs with\nDeCRIM can outperform GPT-4 on both benchmarks.",
      "upvotes": 8
    },
    {
      "title": "Collective Critics for Creative Story Generation",
      "url": "https://huggingface.co/papers/2410.02428",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.02428.pdf",
      "abstract": "Generating a long story of several thousand words with narrative coherence\nusing Large Language Models (LLMs) has been a challenging task. Previous\nresearch has addressed this challenge by proposing different frameworks that\ncreate a story plan and generate a long story based on that plan. However,\nthese frameworks have been mainly focusing on maintaining narrative coherence\nin stories, often overlooking creativity in story planning and the\nexpressiveness of the stories generated from those plans, which are desirable\nproperties to captivate readers' interest. In this paper, we propose Collective\nCritics for Creative Story Generation framework (CritiCS), which is composed of\nplan refining stage (CrPlan) and story generation stage (CrText), to integrate\na collective revision mechanism that promotes those properties into long-form\nstory generation process. Specifically, in each stage, a group of LLM critics\nand one leader collaborate to incrementally refine drafts of plan and story\nthroughout multiple rounds. Extensive human evaluation shows that the CritiCS\ncan significantly enhance story creativity and reader engagement, while also\nmaintaining narrative coherence. Furthermore, the design of the framework\nallows active participation from human writers in any role within the critique\nprocess, enabling interactive human-machine collaboration in story writing.",
      "upvotes": 8
    },
    {
      "title": "Mixed-Session Conversation with Egocentric Memory",
      "url": "https://huggingface.co/papers/2410.02503",
      "authors": [
        "Taeyoung Kim"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.02503.pdf",
      "abstract": "Recently introduced dialogue systems have demonstrated high usability.\nHowever, they still fall short of reflecting real-world conversation scenarios.\nCurrent dialogue systems exhibit an inability to replicate the dynamic,\ncontinuous, long-term interactions involving multiple partners. This shortfall\narises because there have been limited efforts to account for both aspects of\nreal-world dialogues: deeply layered interactions over the long-term dialogue\nand widely expanded conversation networks involving multiple participants. As\nthe effort to incorporate these aspects combined, we introduce Mixed-Session\nConversation, a dialogue system designed to construct conversations with\nvarious partners in a multi-session dialogue setup. We propose a new dataset\ncalled MiSC to implement this system. The dialogue episodes of MiSC consist of\n6 consecutive sessions, with four speakers (one main speaker and three\npartners) appearing in each episode. Also, we propose a new dialogue model with\na novel memory management mechanism, called Egocentric Memory Enhanced\nMixed-Session Conversation Agent (EMMA). EMMA collects and retains memories\nfrom the main speaker's perspective during conversations with partners,\nenabling seamless continuity in subsequent interactions. Extensive human\nevaluations validate that the dialogues in MiSC demonstrate a seamless\nconversational flow, even when conversation partners change in each session.\nEMMA trained with MiSC is also evaluated to maintain high memorability without\ncontradiction throughout the entire conversation.",
      "upvotes": 8
    },
    {
      "title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
      "url": "https://huggingface.co/papers/2410.06555",
      "authors": [
        "Shuyue Guo",
        "Meng Cao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06555.pdf",
      "abstract": "As multimodal large language models (MLLMs) continue to demonstrate\nincreasingly competitive performance across a broad spectrum of tasks, more\nintricate and comprehensive benchmarks have been developed to assess these\ncutting-edge models. These benchmarks introduce new challenges to core\ncapabilities such as perception, reasoning, and planning. However, existing\nmultimodal benchmarks fall short in providing a focused evaluation of\nmulti-step planning based on spatial relationships in images. To bridge this\ngap, we present ING-VP, the first INteractive Game-based Vision Planning\nbenchmark, specifically designed to evaluate the spatial imagination and\nmulti-step reasoning abilities of MLLMs. ING-VP features 6 distinct games,\nencompassing 300 levels, each with 6 unique configurations. A single model\nengages in over 60,000 rounds of interaction. The benchmark framework allows\nfor multiple comparison settings, including image-text vs. text-only inputs,\nsingle-step vs. multi-step reasoning, and with-history vs. without-history\nconditions, offering valuable insights into the model's capabilities. We\nevaluated numerous state-of-the-art MLLMs, with the highest-performing model,\nClaude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the\nanticipated standard. This work aims to provide a specialized evaluation\nframework to drive advancements in MLLMs' capacity for complex spatial\nreasoning and planning. The code is publicly available at\nhttps://github.com/Thisisus7/ING-VP.git.",
      "upvotes": 8
    },
    {
      "title": "TextToon: Real-Time Text Toonify Head Avatar from Single Video",
      "url": "https://huggingface.co/papers/2410.07160",
      "authors": [
        "Lele Chen",
        "Chenliang Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07160.pdf",
      "abstract": "We propose TextToon, a method to generate a drivable toonified avatar. Given\na short monocular video sequence and a written instruction about the avatar\nstyle, our model can generate a high-fidelity toonified avatar that can be\ndriven in real-time by another video with arbitrary identities. Existing\nrelated works heavily rely on multi-view modeling to recover geometry via\ntexture embeddings, presented in a static manner, leading to control\nlimitations. The multi-view video input also makes it difficult to deploy these\nmodels in real-world applications. To address these issues, we adopt a\nconditional embedding Tri-plane to learn realistic and stylized facial\nrepresentations in a Gaussian deformation field. Additionally, we expand the\nstylization capabilities of 3D Gaussian Splatting by introducing an adaptive\npixel-translation neural network and leveraging patch-aware contrastive\nlearning to achieve high-quality images. To push our work into consumer\napplications, we develop a real-time system that can operate at 48 FPS on a GPU\nmachine and 15-18 FPS on a mobile machine. Extensive experiments demonstrate\nthe efficacy of our approach in generating textual avatars over existing\nmethods in terms of quality and real-time animation. Please refer to our\nproject page for more details: https://songluchuan.github.io/TextToon/.",
      "upvotes": 8
    },
    {
      "title": "Multimodal Situational Safety",
      "url": "https://huggingface.co/papers/2410.06172",
      "authors": [
        "Kaiwen Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06172.pdf",
      "abstract": "Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating\nimpressive capabilities as multimodal assistants that interact with both humans\nand their environments. However, this increased sophistication introduces\nsignificant safety concerns. In this paper, we present the first evaluation and\nanalysis of a novel safety challenge termed Multimodal Situational Safety,\nwhich explores how safety considerations vary based on the specific situation\nin which the user or agent is engaged. We argue that for an MLLM to respond\nsafely, whether through language or action, it often needs to assess the safety\nimplications of a language query within its corresponding visual context. To\nevaluate this capability, we develop the Multimodal Situational Safety\nbenchmark (MSSBench) to assess the situational safety performance of current\nMLLMs. The dataset comprises 1,820 language query-image pairs, half of which\nthe image context is safe, and the other half is unsafe. We also develop an\nevaluation framework that analyzes key safety aspects, including explicit\nsafety reasoning, visual understanding, and, crucially, situational safety\nreasoning. Our findings reveal that current MLLMs struggle with this nuanced\nsafety problem in the instruction-following setting and struggle to tackle\nthese situational safety challenges all at once, highlighting a key area for\nfuture research. Furthermore, we develop multi-agent pipelines to coordinately\nsolve safety challenges, which shows consistent improvement in safety over the\noriginal MLLM response. Code and data: mssbench.github.io.",
      "upvotes": 8
    },
    {
      "title": "TRACE: Temporal Grounding Video LLM via Causal Event Modeling",
      "url": "https://huggingface.co/papers/2410.05643",
      "authors": [
        "Jingyu Liu",
        "Mingda Li",
        "Xiaoying Tang",
        "Xi Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05643.pdf",
      "abstract": "Video Temporal Grounding (VTG) is a crucial capability for video\nunderstanding models and plays a vital role in downstream tasks such as video\nbrowsing and editing. To effectively handle various tasks simultaneously and\nenable zero-shot prediction, there is a growing trend in employing video LLMs\nfor VTG tasks. However, current video LLM-based methods rely exclusively on\nnatural language generation, lacking the ability to model the clear structure\ninherent in videos, which restricts their effectiveness in tackling VTG tasks.\nTo address this issue, this paper first formally introduces causal event\nmodeling framework, which represents videos as sequences of events, and predict\nthe current event using previous events, video inputs, and textural\ninstructions. Each event consists of three components: timestamps, salient\nscores, and textual captions. We then propose a novel task-interleaved video\nLLM called TRACE to effectively implement the causal event modeling framework\nin practice. The TRACE processes visual frames, timestamps, salient scores, and\ntext as distinct tasks, employing various encoders and decoding heads for each.\nTask tokens are arranged in an interleaved sequence according to the causal\nevent modeling framework's formulation. Extensive experiments on various VTG\ntasks and datasets demonstrate the superior performance of TRACE compared to\nstate-of-the-art video LLMs. Our model and code are available at\nhttps://github.com/gyxxyg/TRACE.",
      "upvotes": 8
    },
    {
      "title": "Data Selection via Optimal Control for Language Models",
      "url": "https://huggingface.co/papers/2410.07064",
      "authors": [
        "Yaru Hao",
        "Minlie Huang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07064.pdf",
      "abstract": "This work investigates the selection of high-quality pre-training data from\nmassive corpora to enhance LMs' capabilities for downstream usage. We formulate\ndata selection as a generalized Optimal Control problem, which can be solved\ntheoretically by Pontryagin's Maximum Principle (PMP), yielding a set of\nnecessary conditions that characterize the relationship between optimal data\nselection and LM training dynamics. Based on these theoretical results, we\nintroduce PMP-based Data Selection (PDS), a framework that approximates optimal\ndata selection by solving the PMP conditions. In our experiments, we adopt PDS\nto select data from CommmonCrawl and show that the PDS-selected corpus\naccelerates the learning of LMs and constantly boosts their performance on a\nwide range of downstream tasks across various model sizes. Moreover, the\nbenefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by\nthe extrapolation of the test loss curves according to the Scaling Laws. PDS\nalso improves data utilization when the pre-training data is limited, by\nreducing the data demand by 1.8 times, which mitigates the quick exhaustion of\navailable web-crawled corpora. Our code, data, and model checkpoints can be\nfound in https://github.com/microsoft/LMOps/tree/main/data_selection.",
      "upvotes": 8
    },
    {
      "title": "FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance",
      "url": "https://huggingface.co/papers/2410.05791",
      "authors": [
        "Pei Xu",
        "Haochen Shi",
        "Elizabeth Schumann",
        "C. Karen Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05791.pdf",
      "abstract": "Piano playing requires agile, precise, and coordinated hand control that\nstretches the limits of dexterity. Hand motion models with the sophistication\nto accurately recreate piano playing have a wide range of applications in\ncharacter animation, embodied AI, biomechanics, and VR/AR. In this paper, we\nconstruct a first-of-its-kind large-scale dataset that contains approximately\n10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153\npieces of classical music. To capture natural performances, we designed a\nmarkerless setup in which motions are reconstructed from multi-view videos\nusing state-of-the-art pose estimation models. The motion data is further\nrefined via inverse kinematics using the high-resolution MIDI key-pressing data\nobtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the\ncollected dataset, we developed a pipeline that can synthesize\nphysically-plausible hand motions for musical scores outside of the dataset.\nOur approach employs a combination of imitation learning and reinforcement\nlearning to obtain policies for physics-based bimanual control involving the\ninteraction between hands and piano keys. To solve the sampling efficiency\nproblem with the large motion dataset, we use a diffusion model to generate\nnatural reference motions, which provide high-level trajectory and fingering\n(finger order and placement) information. However, the generated reference\nmotion alone does not provide sufficient accuracy for piano performance\nmodeling. We then further augmented the data by using musical similarity to\nretrieve similar motions from the captured dataset to boost the precision of\nthe RL policy. With the proposed method, our model generates natural, dexterous\nmotions that generalize to music from outside the training dataset.",
      "upvotes": 7
    },
    {
      "title": "Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning",
      "url": "https://huggingface.co/papers/2410.05664",
      "authors": [
        "Sangdon Park",
        "Dongwoo Kim"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05664.pdf",
      "abstract": "As text-to-image diffusion models become advanced enough for commercial\napplications, there is also increasing concern about their potential for\nmalicious and harmful use. Model unlearning has been proposed to mitigate the\nconcerns by removing undesired and potentially harmful information from the\npre-trained model. So far, the success of unlearning is mainly measured by\nwhether the unlearned model can generate a target concept while maintaining\nimage quality. However, unlearning is typically tested under limited scenarios,\nand the side effects of unlearning have barely been studied in the current\nliterature. In this work, we thoroughly analyze unlearning under various\nscenarios with five key aspects. Our investigation reveals that every method\nhas side effects or limitations, especially in more complex and realistic\nsituations. By releasing our comprehensive evaluation framework with the source\ncodes and artifacts, we hope to inspire further research in this area, leading\nto more reliable and effective unlearning methods.",
      "upvotes": 7
    },
    {
      "title": "Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning",
      "url": "https://huggingface.co/papers/2410.04223",
      "authors": [
        "Gang Liu",
        "Michael Sun",
        "Wojciech Matusik",
        "Jie Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04223.pdf",
      "abstract": "While large language models (LLMs) have integrated images, adapting them to\ngraphs remains challenging, limiting their applications in materials and drug\ndesign. This difficulty stems from the need for coherent autoregressive\ngeneration across texts and graphs. To address this, we introduce Llamole, the\nfirst multimodal LLM capable of interleaved text and graph generation, enabling\nmolecular inverse design with retrosynthetic planning. Llamole integrates a\nbase LLM with the Graph Diffusion Transformer and Graph Neural Networks for\nmulti-conditional molecular generation and reaction inference within texts,\nwhile the LLM, with enhanced molecular understanding, flexibly controls\nactivation among the different graph modules. Additionally, Llamole integrates\nA* search with LLM-based cost functions for efficient retrosynthetic planning.\nWe create benchmarking datasets and conduct extensive experiments to evaluate\nLlamole against in-context learning and supervised fine-tuning. Llamole\nsignificantly outperforms 14 adapted LLMs across 12 metrics for controllable\nmolecular design and retrosynthetic planning.",
      "upvotes": 7
    },
    {
      "title": "Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders",
      "url": "https://huggingface.co/papers/2410.06462",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.06462.pdf",
      "abstract": "The research builds and evaluates the adversarial potential to introduce\ncopied code or hallucinated AI recommendations for malicious code in popular\ncode repositories. While foundational large language models (LLMs) from OpenAI,\nGoogle, and Anthropic guard against both harmful behaviors and toxic strings,\nprevious work on math solutions that embed harmful prompts demonstrate that the\nguardrails may differ between expert contexts. These loopholes would appear in\nmixture of expert's models when the context of the question changes and may\noffer fewer malicious training examples to filter toxic comments or recommended\noffensive actions. The present work demonstrates that foundational models may\nrefuse to propose destructive actions correctly when prompted overtly but may\nunfortunately drop their guard when presented with a sudden change of context,\nlike solving a computer programming challenge. We show empirical examples with\ntrojan-hosting repositories like GitHub, NPM, NuGet, and popular content\ndelivery networks (CDN) like jsDelivr which amplify the attack surface. In the\nLLM's directives to be helpful, example recommendations propose application\nprogramming interface (API) endpoints which a determined domain-squatter could\nacquire and setup attack mobile infrastructure that triggers from the naively\ncopied code. We compare this attack to previous work on context-shifting and\ncontrast the attack surface as a novel version of \"living off the land\" attacks\nin the malware literature. In the latter case, foundational language models can\nhijack otherwise innocent user prompts to recommend actions that violate their\nowners' safety policies when posed directly without the accompanying coding\nsupport request.",
      "upvotes": 7
    },
    {
      "title": "Retrieval-Augmented Decision Transformer: External Memory for In-context RL",
      "url": "https://huggingface.co/papers/2410.07071",
      "authors": [
        "Markus Hofmarcher",
        "Sepp Hochreiter"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07071.pdf",
      "abstract": "In-context learning (ICL) is the ability of a model to learn a new task by\nobserving a few exemplars in its context. While prevalent in NLP, this\ncapability has recently also been observed in Reinforcement Learning (RL)\nsettings. Prior in-context RL methods, however, require entire episodes in the\nagent's context. Given that complex environments typically lead to long\nepisodes with sparse rewards, these methods are constrained to simple\nenvironments with short episodes. To address these challenges, we introduce\nRetrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external\nmemory mechanism to store past experiences from which it retrieves only\nsub-trajectories relevant for the current situation. The retrieval component in\nRA-DT does not require training and can be entirely domain-agnostic. We\nevaluate the capabilities of RA-DT on grid-world environments, robotics\nsimulations, and procedurally-generated video games. On grid-worlds, RA-DT\noutperforms baselines, while using only a fraction of their context length.\nFurthermore, we illuminate the limitations of current in-context RL methods on\ncomplex environments and discuss future directions. To facilitate future\nresearch, we release datasets for four of the considered environments.",
      "upvotes": 6
    },
    {
      "title": "MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering",
      "url": "https://huggingface.co/papers/2410.07095",
      "authors": [
        "Jun Shern Chan",
        "Oliver Jaffe",
        "James Aung",
        "Dane Sherburn",
        "Kevin Liu",
        "Leon Maksin",
        "Aleksander Mądry"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07095.pdf",
      "abstract": "We introduce MLE-bench, a benchmark for measuring how well AI agents perform\nat machine learning engineering. To this end, we curate 75 ML\nengineering-related competitions from Kaggle, creating a diverse set of\nchallenging tasks that test real-world ML engineering skills such as training\nmodels, preparing datasets, and running experiments. We establish human\nbaselines for each competition using Kaggle's publicly available leaderboards.\nWe use open-source agent scaffolds to evaluate several frontier language models\non our benchmark, finding that the best-performing setup--OpenAI's o1-preview\nwith AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in\n16.9% of competitions. In addition to our main results, we investigate various\nforms of resource scaling for AI agents and the impact of contamination from\npre-training. We open-source our benchmark code (github.com/openai/mle-bench/)\nto facilitate future research in understanding the ML engineering capabilities\nof AI agents.",
      "upvotes": 6
    },
    {
      "title": "Jointly Generating Multi-view Consistent PBR Textures using Collaborative Control",
      "url": "https://huggingface.co/papers/2410.06985",
      "authors": [
        "Dante De Nigris",
        "Simon Donné"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06985.pdf",
      "abstract": "Multi-view consistency remains a challenge for image diffusion models. Even\nwithin the Text-to-Texture problem, where perfect geometric correspondences are\nknown a priori, many methods fail to yield aligned predictions across views,\nnecessitating non-trivial fusion methods to incorporate the results onto the\noriginal mesh. We explore this issue for a Collaborative Control workflow\nspecifically in PBR Text-to-Texture. Collaborative Control directly models PBR\nimage probability distributions, including normal bump maps; to our knowledge,\nthe only diffusion model to directly output full PBR stacks. We discuss the\ndesign decisions involved in making this model multi-view consistent, and\ndemonstrate the effectiveness of our approach in ablation studies, as well as\npractical applications.",
      "upvotes": 5
    },
    {
      "title": "MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders",
      "url": "https://huggingface.co/papers/2410.06845",
      "authors": [
        "May Fung",
        "Qingyun Wang",
        "Chi Han",
        "Manling Li",
        "Heng Ji"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06845.pdf",
      "abstract": "Mental health disorders are one of the most serious diseases in the world.\nMost people with such a disease lack access to adequate care, which highlights\nthe importance of training models for the diagnosis and treatment of mental\nhealth disorders. However, in the mental health domain, privacy concerns limit\nthe accessibility of personalized treatment data, making it challenging to\nbuild powerful models. In this paper, we introduce MentalArena, a self-play\nframework to train language models by generating domain-specific personalized\ndata, where we obtain a better model capable of making a personalized diagnosis\nand treatment (as a therapist) and providing information (as a patient). To\naccurately model human-like mental health patients, we devise Symptom Encoder,\nwhich simulates a real patient from both cognition and behavior perspectives.\nTo address intent bias during patient-therapist interactions, we propose\nSymptom Decoder to compare diagnosed symptoms with encoded symptoms, and\ndynamically manage the dialogue between patient and therapist according to the\nidentified deviations. We evaluated MentalArena against 6 benchmarks, including\nbiomedicalQA and mental health tasks, compared to 6 advanced models. Our\nmodels, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform\ntheir counterparts, including GPT-4o. We hope that our work can inspire future\nresearch on personalized care. Code is available in\nhttps://github.com/Scarelette/MentalArena/tree/main",
      "upvotes": 5
    },
    {
      "title": "Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach",
      "url": "https://huggingface.co/papers/2410.06949",
      "authors": [
        "Yuxuan Chen",
        "Yuan Yuan",
        "Minlie Huang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06949.pdf",
      "abstract": "In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nTypes, and Distorted Handling Solutions. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a multi\nagent framework inspired by expert developer strategies for exception handling.\nSeeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist\nLLMs in detecting, capturing, and resolving exceptions more effectively. Our\nwork is the first systematic study on leveraging LLMs to enhance exception\nhandling practices, providing valuable insights for future improvements in code\nreliability.",
      "upvotes": 5
    },
    {
      "title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks",
      "url": "https://huggingface.co/papers/2410.05160",
      "authors": [
        "Rui Meng",
        "Xinyi Yang",
        "Semih Yavuz",
        "Yingbo Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05160.pdf",
      "abstract": "Embedding models have been crucial in enabling various downstream tasks such\nas semantic similarity, information retrieval, and clustering. Recently, there\nhas been a surge of interest in developing universal text embedding models that\ncan generalize across tasks (e.g., MTEB). However, progress in learning\nuniversal multimodal embedding models has been relatively slow despite their\nimportance. In this work, we aim to explore the potential for building\nuniversal embeddings capable of handling a wide range of downstream tasks. Our\ncontributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark),\nwhich covers 4 meta-tasks (i.e. classification, visual question answering,\nmultimodal retrieval, and visual grounding) and 36 datasets, including 20\ntraining and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model ->\nVector), a contrastive training framework that converts any state-of-the-art\nvision-language model into an embedding model via training on MMEB. Unlike\nprevious models such as CLIP and BLIP, VLM2Vec can process any combination of\nimages and text to generate a fixed-dimensional vector based on task\ninstructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate\nthem on MMEB's evaluation split. Our results show that \\model achieves an\nabsolute average improvement of 10% to 20% over existing multimodal embedding\nmodels on both in-distribution and out-of-distribution datasets in MMEB.",
      "upvotes": 4
    },
    {
      "title": "Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA",
      "url": "https://huggingface.co/papers/2410.06524",
      "authors": [
        "Hal Daumé III",
        "Jordan Boyd-Graber"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06524.pdf",
      "abstract": "Recent advancements of large language models (LLMs) have led to claims of AI\nsurpassing humans in natural language processing (NLP) tasks such as textual\nunderstanding and reasoning. This work investigates these assertions by\nintroducing CAIMIRA, a novel framework rooted in item response theory (IRT)\nthat enables quantitative assessment and comparison of problem-solving\nabilities of question-answering (QA) agents: humans and AI systems. Through\nanalysis of over 300,000 responses from ~70 AI systems and 155 humans across\nthousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in\nknowledge domains and reasoning skills. Humans outperform AI systems in\nknowledge-grounded abductive and conceptual reasoning, while state-of-the-art\nLLMs like GPT-4 and LLaMA show superior performance on targeted information\nretrieval and fact-based reasoning, particularly when information gaps are\nwell-defined and addressable through pattern matching or data retrieval. These\nfindings highlight the need for future QA tasks to focus on questions that\nchallenge not only higher-order reasoning and scientific thinking, but also\ndemand nuanced linguistic interpretation and cross-contextual knowledge\napplication, helping advance AI developments that better emulate or complement\nhuman cognitive abilities in real-world problem-solving.",
      "upvotes": 4
    },
    {
      "title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment",
      "url": "https://huggingface.co/papers/2410.05873",
      "authors": [
        "Nafiseh Nikeghbal",
        "Jana Diesner",
        "François Yvon",
        "Hinrich Schütze"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05873.pdf",
      "abstract": "English-centric large language models (LLMs) often show strong multilingual\ncapabilities. However, the multilingual performance of these models remains\nunclear and is not thoroughly evaluated for many languages. Most benchmarks for\nmultilinguality focus on classic NLP tasks, or cover a minimal number of\nlanguages. We introduce MEXA, a method for assessing the multilingual\ncapabilities of pre-trained English-centric LLMs using parallel sentences,\nwhich are available for more languages than existing downstream tasks. MEXA\nleverages the fact that English-centric LLMs use English as a kind of pivot\nlanguage in their intermediate layers. It computes the alignment between\nEnglish and non-English languages using parallel sentences to evaluate the\ntransfer of language understanding from English to other languages. This\nalignment can be used to estimate model performance in other languages. We\nconduct studies using various parallel datasets (FLORES-200 and Bible), models\n(Llama family, Gemma family, Mistral, and OLMo), and established downstream\ntasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute\nembeddings in decoder-only models. Our results show that MEXA, in its default\nsettings, achieves a statistically significant average Pearson correlation of\n0.90 with three established downstream tasks across nine models and two\nparallel datasets. This suggests that MEXA is a reliable method for estimating\nthe multilingual capabilities of English-centric LLMs, providing a clearer\nunderstanding of their multilingual potential and the inner workings of LLMs.\nLeaderboard: https://huggingface.co/spaces/cis-lmu/Mexa, Code:\nhttps://github.com/cisnlp/Mexa.",
      "upvotes": 3
    },
    {
      "title": "TinyEmo: Scaling down Emotional Reasoning via Metric Projection",
      "url": "https://huggingface.co/papers/2410.07062",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.07062.pdf",
      "abstract": "This paper introduces TinyEmo, a family of small multi-modal language models\nfor emotional reasoning and classification. Our approach features: (1) a\nsynthetic emotional instruct dataset for both pre-training and fine-tuning\nstages, (2) a Metric Projector that delegates classification from the language\nmodel allowing for more efficient training and inference, (3) a multi-modal\nlarge language model (MM-LLM) for emotional reasoning, and (4) a semi-automated\nframework for bias detection. TinyEmo is able to perform emotion classification\nand emotional reasoning, all while using substantially fewer parameters than\ncomparable models. This efficiency allows us to freely incorporate more diverse\nemotional datasets, enabling strong performance on classification tasks, with\nour smallest model (700M parameters) outperforming larger state-of-the-art\nmodels based on general-purpose MM-LLMs with over 7B parameters. Additionally,\nthe Metric Projector allows for interpretability and indirect bias detection in\nlarge models without additional training, offering an approach to understand\nand improve AI systems.\n  We release code, models, and dataset at https://github.com/ggcr/TinyEmo",
      "upvotes": 3
    },
    {
      "title": "VHELM: A Holistic Evaluation of Vision Language Models",
      "url": "https://huggingface.co/papers/2410.07112",
      "authors": [
        "Tony Lee",
        "Haoqin Tu",
        "Chi Heem Wong",
        "Wenhao Zheng",
        "Yiyang Zhou",
        "Yifan Mai",
        "Josselin Somerville Roberts",
        "Michihiro Yasunaga",
        "Huaxiu Yao",
        "Cihang Xie",
        "Percy Liang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07112.pdf",
      "abstract": "Current benchmarks for assessing vision-language models (VLMs) often focus on\ntheir perception or problem-solving capabilities and neglect other critical\naspects such as fairness, multilinguality, or toxicity. Furthermore, they\ndiffer in their evaluation procedures and the scope of the evaluation, making\nit difficult to compare models. To address these issues, we extend the HELM\nframework to VLMs to present the Holistic Evaluation of Vision Language Models\n(VHELM). VHELM aggregates various datasets to cover one or more of the 9\naspects: visual perception, knowledge, reasoning, bias, fairness,\nmultilinguality, robustness, toxicity, and safety. In doing so, we produce a\ncomprehensive, multi-dimensional view of the capabilities of the VLMs across\nthese important factors. In addition, we standardize the standard inference\nparameters, methods of prompting, and evaluation metrics to enable fair\ncomparisons across models. Our framework is designed to be lightweight and\nautomatic so that evaluation runs are cheap and fast. Our initial run evaluates\n22 VLMs on 21 existing datasets to provide a holistic snapshot of the models.\nWe uncover new key findings, such as the fact that efficiency-focused models\n(e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than\ntheir full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark\nbut not when evaluated on the other aspects. For transparency, we release the\nraw model generations and complete results on our website\n(https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living\nbenchmark, and we hope to continue adding new datasets and models over time.",
      "upvotes": 2
    },
    {
      "title": "Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling",
      "url": "https://huggingface.co/papers/2410.07145",
      "authors": [
        "Xinrong Zhang",
        "Shengding Hu",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07145.pdf",
      "abstract": "One essential advantage of recurrent neural networks (RNNs) over\ntransformer-based language models is their linear computational complexity\nconcerning the sequence length, which makes them much faster in handling long\nsequences during inference. However, most publicly available RNNs (e.g., Mamba\nand RWKV) are trained on sequences with less than 10K tokens, and their\neffectiveness in longer contexts remains largely unsatisfying so far. In this\npaper, we study the cause of the inability to process long context for RNNs and\nsuggest critical mitigations. We examine two practical concerns when applying\nstate-of-the-art RNNs to long contexts: (1) the inability to extrapolate to\ninputs longer than the training length and (2) the upper bound of memory\ncapacity. Addressing the first concern, we first investigate *state collapse*\n(SC), a phenomenon that causes severe performance degradation on sequence\nlengths not encountered during training. With controlled experiments, we\nattribute this to overfitting due to the recurrent state being\noverparameterized for the training length. For the second concern, we train a\nseries of Mamba-2 models on long documents to empirically estimate the\nrecurrent state capacity in language modeling and passkey retrieval. Then,\nthree SC mitigation methods are proposed to improve Mamba-2's length\ngeneralizability, allowing the model to process more than 1M tokens without SC.\nWe also find that the recurrent state capacity in passkey retrieval scales\nexponentially to the state size, and we empirically train a Mamba-2 370M with\nnear-perfect passkey retrieval accuracy on 256K context length. This suggests a\npromising future for RNN-based long-context modeling.",
      "upvotes": 2
    },
    {
      "title": "Does Spatial Cognition Emerge in Frontier Models?",
      "url": "https://huggingface.co/papers/2410.06468",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.06468.pdf",
      "abstract": "Not yet. We present SPACE, a benchmark that systematically evaluates spatial\ncognition in frontier models. Our benchmark builds on decades of research in\ncognitive science. It evaluates large-scale mapping abilities that are brought\nto bear when an organism traverses physical environments, smaller-scale\nreasoning about object shapes and layouts, and cognitive infrastructure such as\nspatial attention and memory. For many tasks, we instantiate parallel\npresentations via text and images, allowing us to benchmark both large language\nmodels and large multimodal models. Results suggest that contemporary frontier\nmodels fall short of the spatial intelligence of animals, performing near\nchance level on a number of classic tests of animal cognition.",
      "upvotes": 2
    }
  ]
}