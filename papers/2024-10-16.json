{
  "date": "2024-10-16",
  "papers": [
    {
      "title": "Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free",
      "url": "https://huggingface.co/papers/2410.10814",
      "authors": [
        "Ziyue Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.10814.pdf",
      "abstract": "While large language models (LLMs) excel on generation tasks, their\ndecoder-only architecture often limits their potential as embedding models if\nno further representation finetuning is applied. Does this contradict their\nclaim of generalists? To answer the question, we take a closer look at\nMixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE\nLLMs can serve as an off-the-shelf embedding model with promising performance\non a diverse class of embedding-focused tasks, without requiring any\nfinetuning. Moreover, our extensive analysis shows that the MoE routing weights\n(RW) is complementary to the hidden state (HS) of LLMs, a widely-used\nembedding. Compared to HS, we find that RW is more robust to the choice of\nprompts and focuses on high-level semantics. Motivated by the analysis, we\npropose MoEE combining RW and HS, which achieves better performance than using\neither separately. Our exploration of their combination and prompting strategy\nshed several novel insights, e.g., a weighted sum of RW and HS similarities\noutperforms the similarity on their concatenation. Our experiments are\nconducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding\nBenchmark (MTEB). The results demonstrate the significant improvement brought\nby MoEE to LLM-based embedding without further finetuning.",
      "upvotes": 48
    },
    {
      "title": "LLM$\\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models",
      "url": "https://huggingface.co/papers/2410.09342",
      "authors": [
        "Zihan Zhou",
        "Chong Li",
        "Xinyi Chen",
        "Yu Chao",
        "Zhili Li",
        "Haoyu Wang",
        "Rongqiao An",
        "Qi Shi",
        "Zhixing Tan",
        "Xu Han",
        "Xiaodong Shi",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.09342.pdf",
      "abstract": "Enlarging the context window of large language models (LLMs) has become a\ncrucial research area, particularly for applications involving extremely long\ntexts. In this work, we propose a novel training-free framework for processing\nlong texts, utilizing a divide-and-conquer strategy to achieve comprehensive\ndocument understanding. The proposed LLMtimesMapReduce framework splits the\nentire document into several chunks for LLMs to read and then aggregates the\nintermediate answers to produce the final output. The main challenge for\ndivide-and-conquer long text processing frameworks lies in the risk of losing\nessential long-range information when splitting the document, which can lead\nthe model to produce incomplete or incorrect answers based on the segmented\ntexts. Disrupted long-range information can be classified into two categories:\ninter-chunk dependency and inter-chunk conflict. We design a structured\ninformation protocol to better cope with inter-chunk dependency and an\nin-context confidence calibration mechanism to resolve inter-chunk conflicts.\nExperimental results demonstrate that LLMtimesMapReduce can outperform\nrepresentative open-source and commercial long-context LLMs, and is applicable\nto several different models.",
      "upvotes": 37
    },
    {
      "title": "Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts",
      "url": "https://huggingface.co/papers/2410.10626",
      "authors": [
        "Guorui Zheng",
        "Xidong Wang",
        "Juhao Liang",
        "Nuo Chen",
        "Yuping Zheng",
        "Benyou Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.10626.pdf",
      "abstract": "Adapting medical Large Language Models to local languages can reduce barriers\nto accessing healthcare services, but data scarcity remains a significant\nchallenge, particularly for low-resource languages. To address this, we first\nconstruct a high-quality medical dataset and conduct analysis to ensure its\nquality. In order to leverage the generalization capability of multilingual\nLLMs to efficiently scale to more resource-constrained languages, we explore\nthe internal information flow of LLMs from a multilingual perspective using\nMixture of Experts (MoE) modularity. Technically, we propose a novel MoE\nrouting method that employs language-specific experts and cross-lingual\nrouting. Inspired by circuit theory, our routing analysis revealed a Spread Out\nin the End information flow mechanism: while earlier layers concentrate\ncross-lingual information flow, the later layers exhibit language-specific\ndivergence. This insight directly led to the development of the Post-MoE\narchitecture, which applies sparse routing only in the later layers while\nmaintaining dense others. Experimental results demonstrate that this approach\nenhances the generalization of multilingual models to other languages while\npreserving interpretability. Finally, to efficiently scale the model to 50\nlanguages, we introduce the concept of language family experts, drawing on\nlinguistic priors, which enables scaling the number of languages without adding\nadditional parameters.",
      "upvotes": 37
    },
    {
      "title": "What Matters in Transformers? Not All Attention is Needed",
      "url": "https://huggingface.co/papers/2406.15786",
      "authors": [
        "Shwai He",
        "Guoheng Sun",
        "Zheyu Shen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15786.pdf",
      "abstract": "While scaling Transformer-based large language models (LLMs) has demonstrated\npromising performance across various tasks, it also introduces redundant\narchitectures, posing efficiency challenges for real-world deployment. Despite\nsome recognition of redundancy in LLMs, the variability of redundancy across\ndifferent architectures in transformers, such as MLP and Attention layers, is\nunder-explored. In this work, we investigate redundancy across different\nmodules within Transformers, including Blocks, MLP, and Attention layers, using\na similarity-based metric. Surprisingly, despite the critical role of attention\nlayers in distinguishing transformers from other architectures, we found that a\nlarge portion of these layers exhibit excessively high similarity and can be\npruned without degrading performance. For instance, Llama-2-70B achieved a\n48.4\\% speedup with only a 2.4\\% performance drop by pruning half of the\nattention layers. Furthermore, by tracing model checkpoints throughout the\ntraining process, we observed that attention layer redundancy is inherent and\nconsistent across training stages. Additionally, we further propose a method\nthat jointly drops Attention and MLP layers, allowing us to more aggressively\ndrop additional layers. For instance, when dropping 31 layers (Attention +\nMLP), Llama-2-13B still retains 90\\% of the performance on the MMLU task. Our\nwork provides valuable insights for future network architecture design. The\ncode is released at: https://github.com/Shwai-He/LLM-Drop.",
      "upvotes": 27
    },
    {
      "title": "MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation",
      "url": "https://huggingface.co/papers/2410.11779",
      "authors": [
        "Chenxi Wang",
        "Xiang Chen",
        "Bozhong Tian",
        "Haoming Xu",
        "Shumin Deng",
        "Huajun Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11779.pdf",
      "abstract": "Multimodal Large Language Models (MLLMs) frequently exhibit hallucination\nphenomena, but the underlying reasons remain poorly understood. In this paper,\nwe present an empirical analysis and find that, although MLLMs incorrectly\ngenerate the objects in the final output, they are actually able to recognize\nvisual objects in the preceding layers. We speculate that this may be due to\nthe strong knowledge priors of the language model suppressing the visual\ninformation, leading to hallucinations. Motivated by this, we propose a novel\ndynamic correction decoding method for MLLMs (DeCo), which adaptively selects\nthe appropriate preceding layers and proportionally integrates knowledge into\nthe final layer to adjust the output logits. Note that DeCo is model agnostic\nand can be seamlessly incorporated with various classic decoding strategies and\napplied to different MLLMs. We evaluate DeCo on widely-used benchmarks,\ndemonstrating that it can reduce hallucination rates by a large margin compared\nto baselines, highlighting its potential to mitigate hallucinations. Code is\navailable at https://github.com/zjunlp/DeCo.",
      "upvotes": 24
    },
    {
      "title": "LVD-2M: A Long-take Video Dataset with Temporally Dense Captions",
      "url": "https://huggingface.co/papers/2410.10816",
      "authors": [
        "Daquan Zhou",
        "Zhijie Lin",
        "Jiashi Feng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.10816.pdf",
      "abstract": "The efficacy of video generation models heavily depends on the quality of\ntheir training datasets. Most previous video generation models are trained on\nshort video clips, while recently there has been increasing interest in\ntraining long video generation models directly on longer videos. However, the\nlack of such high-quality long videos impedes the advancement of long video\ngeneration. To promote research in long video generation, we desire a new\ndataset with four key features essential for training long video generation\nmodels: (1) long videos covering at least 10 seconds, (2) long-take videos\nwithout cuts, (3) large motion and diverse contents, and (4) temporally dense\ncaptions. To achieve this, we introduce a new pipeline for selecting\nhigh-quality long-take videos and generating temporally dense captions.\nSpecifically, we define a set of metrics to quantitatively assess video quality\nincluding scene cuts, dynamic degrees, and semantic-level quality, enabling us\nto filter high-quality long-take videos from a large amount of source videos.\nSubsequently, we develop a hierarchical video captioning pipeline to annotate\nlong videos with temporally-dense captions. With this pipeline, we curate the\nfirst long-take video dataset, LVD-2M, comprising 2 million long-take videos,\neach covering more than 10 seconds and annotated with temporally dense\ncaptions. We further validate the effectiveness of LVD-2M by fine-tuning video\ngeneration models to generate long videos with dynamic motions. We believe our\nwork will significantly contribute to future research in long video generation.",
      "upvotes": 19
    },
    {
      "title": "MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models",
      "url": "https://huggingface.co/papers/2410.11710",
      "authors": [
        "Pei Wang",
        "Yanan Wu",
        "Zekun Wang",
        "Jiaheng Liu",
        "Xiaoshuai Song",
        "Zhongyuan Peng",
        "Ken Deng",
        "Chenchen Zhang",
        "Jiakai Wang",
        "Junran Peng",
        "Hangyu Guo",
        "Zhaoxiang Zhang",
        "Wenbo Su",
        "Bo Zheng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11710.pdf",
      "abstract": "Large Language Models (LLMs) have displayed massive improvements in reasoning\nand decision-making skills and can hold natural conversations with users.\nRecently, many tool-use benchmark datasets have been proposed. However,\nexisting datasets have the following limitations: (1). Insufficient evaluation\nscenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation\ncosts (e.g., GPT API costs). To address these limitations, in this work, we\npropose a multi-granularity tool-use benchmark for large language models called\nMTU-Bench. For the \"multi-granularity\" property, our MTU-Bench covers five tool\nusage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool,\nmultiple-turn and single-tool, multiple-turn and multiple-tool, and\nout-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench\nare based on the prediction results and the ground truth without using any GPT\nor human evaluation metrics. Moreover, our MTU-Bench is collected by\ntransforming existing high-quality datasets to simulate real-world tool usage\nscenarios, and we also propose an instruction dataset called MTU-Instruct data\nto enhance the tool-use abilities of existing LLMs. Comprehensive experimental\nresults demonstrate the effectiveness of our MTU-Bench. Code and data will be\nreleased at https: //github.com/MTU-Bench-Team/MTU-Bench.git.",
      "upvotes": 18
    },
    {
      "title": "Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices",
      "url": "https://huggingface.co/papers/2410.11795",
      "authors": [
        "Zhiyuan Ma",
        "Yuzhu Zhang",
        "Guoli Jia",
        "Liangliang Zhao",
        "Yichao Ma",
        "Gaofeng Liu",
        "Jianjun Li",
        "Bowen Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11795.pdf",
      "abstract": "As one of the most popular and sought-after generative models in the recent\nyears, diffusion models have sparked the interests of many researchers and\nsteadily shown excellent advantage in various generative tasks such as image\nsynthesis, video generation, molecule design, 3D scene rendering and multimodal\ngeneration, relying on their dense theoretical principles and reliable\napplication practices. The remarkable success of these recent efforts on\ndiffusion models comes largely from progressive design principles and efficient\narchitecture, training, inference, and deployment methodologies. However, there\nhas not been a comprehensive and in-depth review to summarize these principles\nand practices to help the rapid understanding and application of diffusion\nmodels. In this survey, we provide a new efficiency-oriented perspective on\nthese existing efforts, which mainly focuses on the profound principles and\nefficient practices in architecture designs, model training, fast inference and\nreliable deployment, to guide further theoretical research, algorithm migration\nand model application for new scenarios in a reader-friendly way.\nhttps://github.com/ponyzym/Efficient-DMs-Survey",
      "upvotes": 16
    },
    {
      "title": "SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI",
      "url": "https://huggingface.co/papers/2410.11096",
      "authors": [
        "Yuzhou Nie",
        "Zhun Wang",
        "Yuheng Tang",
        "Wenbo Guo",
        "Bo Li",
        "Dawn Song"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11096.pdf",
      "abstract": "Existing works have established multiple benchmarks to highlight the security\nrisks associated with Code GenAI. These risks are primarily reflected in two\nareas: a model potential to generate insecure code (insecure coding) and its\nutility in cyberattacks (cyberattack helpfulness). While these benchmarks have\nmade significant strides, there remain opportunities for further improvement.\nFor instance, many current benchmarks tend to focus more on a model ability to\nprovide attack suggestions rather than its capacity to generate executable\nattacks. Additionally, most benchmarks rely heavily on static evaluation\nmetrics, which may not be as precise as dynamic metrics such as passing test\ncases. Conversely, expert-verified benchmarks, while offering high-quality\ndata, often operate at a smaller scale. To address these gaps, we develop\nSecCodePLT, a unified and comprehensive evaluation platform for code GenAIs'\nrisks. For insecure code, we introduce a new methodology for data creation that\ncombines experts with automatic generation. Our methodology ensures the data\nquality while enabling large-scale generation. We also associate samples with\ntest cases to conduct code-related dynamic evaluation. For cyberattack\nhelpfulness, we set up a real environment and construct samples to prompt a\nmodel to generate actual attacks, along with dynamic metrics in our\nenvironment. We conduct extensive experiments and show that SecCodePLT\noutperforms the state-of-the-art (SOTA) benchmark CyberSecEval in security\nrelevance. Furthermore, it better identifies the security risks of SOTA models\nin insecure coding and cyberattack helpfulness. Finally, we apply SecCodePLT to\nthe SOTA code agent, Cursor, and, for the first time, identify non-trivial\nsecurity risks in this advanced coding agent.",
      "upvotes": 12
    },
    {
      "title": "EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation",
      "url": "https://huggingface.co/papers/2410.09704",
      "authors": [
        "Xiu Tang",
        "Neal Yuan",
        "Paul Cheng",
        "Debiao Li",
        "Susan Cheng",
        "Bryan He",
        "David Ouyang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.09704.pdf",
      "abstract": "Echocardiography is the most widely used cardiac imaging modality, capturing\nultrasound video data to assess cardiac structure and function. Artificial\nintelligence (AI) in echocardiography has the potential to streamline manual\ntasks and improve reproducibility and precision. However, most echocardiography\nAI models are single-view, single-task systems that do not synthesize\ncomplementary information from multiple views captured during a full exam, and\nthus lead to limited performance and scope of applications. To address this\nproblem, we introduce EchoPrime, a multi-view, view-informed, video-based\nvision-language foundation model trained on over 12 million video-report pairs.\nEchoPrime uses contrastive learning to train a unified embedding model for all\nstandard views in a comprehensive echocardiogram study with representation of\nboth rare and common diseases and diagnoses. EchoPrime then utilizes\nview-classification and a view-informed anatomic attention model to weight\nvideo-specific interpretations that accurately maps the relationship between\nechocardiographic views and anatomical structures. With retrieval-augmented\ninterpretation, EchoPrime integrates information from all echocardiogram videos\nin a comprehensive study and performs holistic comprehensive clinical\nechocardiography interpretation. In datasets from two independent healthcare\nsystems, EchoPrime achieves state-of-the art performance on 23 diverse\nbenchmarks of cardiac form and function, surpassing the performance of both\ntask-specific approaches and prior foundation models. Following rigorous\nclinical evaluation, EchoPrime can assist physicians in the automated\npreliminary assessment of comprehensive echocardiography.",
      "upvotes": 11
    },
    {
      "title": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models",
      "url": "https://huggingface.co/papers/2410.11805",
      "authors": [
        "Han Han",
        "Xiang Zhang",
        "Mengsong Wu",
        "Hao Xiong",
        "Wenliang Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11805.pdf",
      "abstract": "Large language models (LLMs) combined with tool learning have gained\nimpressive results in real-world applications. During tool learning, LLMs may\ncall multiple tools in nested orders, where the latter tool call may take the\nformer response as its input parameters. However, current research on the\nnested tool learning capabilities is still under-explored, since the existing\nbenchmarks lack of relevant data instances. To address this problem, we\nintroduce NesTools to bridge the current gap in comprehensive nested tool\nlearning evaluations. NesTools comprises a novel automatic data generation\nmethod to construct large-scale nested tool calls with different nesting\nstructures. With manual review and refinement, the dataset is in high quality\nand closely aligned with real-world scenarios. Therefore, NesTools can serve as\na new benchmark to evaluate the nested tool learning abilities of LLMs. We\nconduct extensive experiments on 22 LLMs, and provide in-depth analyses with\nNesTools, which shows that current LLMs still suffer from the complex nested\ntool learning task.",
      "upvotes": 11
    },
    {
      "title": "Agent-as-a-Judge: Evaluate Agents with Agents",
      "url": "https://huggingface.co/papers/2410.10934",
      "authors": [
        "Mingchen Zhuge",
        "Changsheng Zhao",
        "Dylan Ashley",
        "Wenyi Wang",
        "Dmitrii Khizbullin",
        "Yunyang Xiong",
        "Zechun Liu",
        "Ernie Chang",
        "Raghuraman Krishnamoorthi",
        "Yuandong Tian",
        "Yangyang Shi",
        "Vikas Chandra",
        "Jürgen Schmidhuber"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.10934.pdf",
      "abstract": "Contemporary evaluation techniques are inadequate for agentic systems. These\napproaches either focus exclusively on final outcomes -- ignoring the\nstep-by-step nature of agentic systems, or require excessive manual labour. To\naddress this, we introduce the Agent-as-a-Judge framework, wherein agentic\nsystems are used to evaluate agentic systems. This is an organic extension of\nthe LLM-as-a-Judge framework, incorporating agentic features that enable\nintermediate feedback for the entire task-solving process. We apply the\nAgent-as-a-Judge to the task of code generation. To overcome issues with\nexisting benchmarks and provide a proof-of-concept testbed for\nAgent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated\nAI development tasks. It includes rich manual annotations, like a total of 365\nhierarchical user requirements. We benchmark three of the popular agentic\nsystems using Agent-as-a-Judge and find it dramatically outperforms\nLLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether,\nwe believe that Agent-as-a-Judge marks a concrete step forward for modern\nagentic systems -- by providing rich and reliable reward signals necessary for\ndynamic and scalable self-improvement.",
      "upvotes": 10
    },
    {
      "title": "GS^3: Efficient Relighting with Triple Gaussian Splatting",
      "url": "https://huggingface.co/papers/2410.11419",
      "authors": [
        "Zoubin Bi",
        "Yixin Zeng",
        "Fan Pei",
        "Xiang Feng",
        "Kun Zhou",
        "Hongzhi Wu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11419.pdf",
      "abstract": "We present a spatial and angular Gaussian based representation and a triple\nsplatting process, for real-time, high-quality novel lighting-and-view\nsynthesis from multi-view point-lit input images. To describe complex\nappearance, we employ a Lambertian plus a mixture of angular Gaussians as an\neffective reflectance function for each spatial Gaussian. To generate\nself-shadow, we splat all spatial Gaussians towards the light source to obtain\nshadow values, which are further refined by a small multi-layer perceptron. To\ncompensate for other effects like global illumination, another network is\ntrained to compute and add a per-spatial-Gaussian RGB tuple. The effectiveness\nof our representation is demonstrated on 30 samples with a wide variation in\ngeometry (from solid to fluffy) and appearance (from translucent to\nanisotropic), as well as using different forms of input data, including\nrendered images of synthetic/reconstructed objects, photographs captured with a\nhandheld camera and a flash, or from a professional lightstage. We achieve a\ntraining time of 40-70 minutes and a rendering speed of 90 fps on a single\ncommodity GPU. Our results compare favorably with state-of-the-art techniques\nin terms of quality/performance. Our code and data are publicly available at\nhttps://GSrelight.github.io/.",
      "upvotes": 10
    },
    {
      "title": "SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning",
      "url": "https://huggingface.co/papers/2410.09754",
      "authors": [
        "Hojoon Lee",
        "Donghu Kim",
        "Hyunseung Kim",
        "Jun Jet Tai",
        "Kaushik Subramanian",
        "Peter R. Wurman",
        "Jaegul Choo",
        "Peter Stone",
        "Takuma Seno"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.09754.pdf",
      "abstract": "Recent advances in CV and NLP have been largely driven by scaling up the\nnumber of network parameters, despite traditional theories suggesting that\nlarger networks are prone to overfitting. These large networks avoid\noverfitting by integrating components that induce a simplicity bias, guiding\nmodels toward simple and generalizable solutions. However, in deep RL,\ndesigning and scaling up networks have been less explored. Motivated by this\nopportunity, we present SimBa, an architecture designed to scale up parameters\nin deep RL by injecting a simplicity bias. SimBa consists of three components:\n(i) an observation normalization layer that standardizes inputs with running\nstatistics, (ii) a residual feedforward block to provide a linear pathway from\nthe input to output, and (iii) a layer normalization to control feature\nmagnitudes. By scaling up parameters with SimBa, the sample efficiency of\nvarious deep RL algorithms-including off-policy, on-policy, and unsupervised\nmethods-is consistently improved. Moreover, solely by integrating SimBa\narchitecture into SAC, it matches or surpasses state-of-the-art deep RL methods\nwith high computational efficiency across DMC, MyoSuite, and HumanoidBench.\nThese results demonstrate SimBa's broad applicability and effectiveness across\ndiverse RL algorithms and environments.",
      "upvotes": 7
    },
    {
      "title": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation",
      "url": "https://huggingface.co/papers/2410.08001",
      "authors": [
        "Hongyang Li",
        "Li Chen",
        "Jisong Cai",
        "Jia Zeng",
        "Heming Cui",
        "Maoqing Yao",
        "Yu Qiao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08001.pdf",
      "abstract": "The increasing demand for versatile robotic systems to operate in diverse and\ndynamic environments has emphasized the importance of a generalist policy,\nwhich leverages a large cross-embodiment data corpus to facilitate broad\nadaptability and high-level reasoning. However, the generalist would struggle\nwith inefficient inference and cost-expensive training. The specialist policy,\ninstead, is curated for specific domain data and excels at task-level precision\nwith efficiency. Yet, it lacks the generalization capacity for a wide range of\napplications. Inspired by these observations, we introduce RoboDual, a\nsynergistic dual-system that supplements the merits of both generalist and\nspecialist policy. A diffusion transformer-based specialist is devised for\nmulti-step action rollouts, exquisitely conditioned on the high-level task\nunderstanding and discretized action output of a vision-language-action (VLA)\nbased generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in\nreal-world setting and 12% gain on CALVIN by introducing a specialist policy\nwith merely 20M trainable parameters. It maintains strong performance with 5%\nof demonstration data only, and enables a 3.8 times higher control frequency in\nreal-world deployment. Code would be made publicly available. Our project page\nis hosted at: https://opendrivelab.com/RoboDual/",
      "upvotes": 3
    },
    {
      "title": "Towards Natural Image Matting in the Wild via Real-Scenario Prior",
      "url": "https://huggingface.co/papers/2410.06593",
      "authors": [
        "Yu Liang",
        "Peng-Tao Jiang",
        "Hao Zhang",
        "Qianru Sun",
        "Yang Tang",
        "Bo Li",
        "Pan Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06593.pdf",
      "abstract": "Recent approaches attempt to adapt powerful interactive segmentation models,\nsuch as SAM, to interactive matting and fine-tune the models based on synthetic\nmatting datasets. However, models trained on synthetic data fail to generalize\nto complex and occlusion scenes. We address this challenge by proposing a new\nmatting dataset based on the COCO dataset, namely COCO-Matting. Specifically,\nthe construction of our COCO-Matting includes accessory fusion and\nmask-to-matte, which selects real-world complex images from COCO and converts\nsemantic segmentation masks to matting labels. The built COCO-Matting comprises\nan extensive collection of 38,251 human instance-level alpha mattes in complex\nnatural scenarios. Furthermore, existing SAM-based matting methods extract\nintermediate features and masks from a frozen SAM and only train a lightweight\nmatting decoder by end-to-end matting losses, which do not fully exploit the\npotential of the pre-trained SAM. Thus, we propose SEMat which revamps the\nnetwork architecture and training objectives. For network architecture, the\nproposed feature-aligned transformer learns to extract fine-grained edge and\ntransparency features. The proposed matte-aligned decoder aims to segment\nmatting-specific objects and convert coarse masks into high-precision mattes.\nFor training objectives, the proposed regularization and trimap loss aim to\nretain the prior from the pre-trained model and push the matting logits\nextracted from the mask decoder to contain trimap-based semantic information.\nExtensive experiments across seven diverse datasets demonstrate the superior\nperformance of our method, proving its efficacy in interactive natural image\nmatting. We open-source our code, models, and dataset at\nhttps://github.com/XiaRho/SEMat.",
      "upvotes": 2
    },
    {
      "title": "Empirical Study of Mutual Reinforcement Effect and Application in Few-shot Text Classification Tasks via Prompt",
      "url": "https://huggingface.co/papers/2410.09745",
      "authors": [
        "Tatsunori Mori"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.09745.pdf",
      "abstract": "The Mutual Reinforcement Effect (MRE) investigates the synergistic\nrelationship between word-level and text-level classifications in text\nclassification tasks. It posits that the performance of both classification\nlevels can be mutually enhanced. However, this mechanism has not been\nadequately demonstrated or explained in prior research. To address this gap, we\nemploy empirical experiment to observe and substantiate the MRE theory. Our\nexperiments on 21 MRE mix datasets revealed the presence of MRE in the model\nand its impact. Specifically, we conducted compare experiments use fine-tune.\nThe results of findings from comparison experiments corroborates the existence\nof MRE. Furthermore, we extended the application of MRE to prompt learning,\nutilizing word-level information as a verbalizer to bolster the model's\nprediction of text-level classification labels. In our final experiment, the\nF1-score significantly surpassed the baseline in 18 out of 21 MRE Mix datasets,\nfurther validating the notion that word-level information enhances the language\nmodel's comprehension of the text as a whole.",
      "upvotes": 2
    },
    {
      "title": "MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval",
      "url": "https://huggingface.co/papers/2410.11619",
      "authors": [
        "Reno Kriz",
        "Kate Sanders",
        "David Etter",
        "Kenton Murray",
        "Cameron Carpenter",
        "Kelly Van Ochten",
        "Hannah Recknor",
        "Jimena Guallar-Blasco",
        "Alexander Martin",
        "Ronald Colaianni",
        "Nolan King",
        "Eugene Yang",
        "Benjamin Van Durme"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11619.pdf",
      "abstract": "Efficiently retrieving and synthesizing information from large-scale\nmultimodal collections has become a critical challenge. However, existing video\nretrieval datasets suffer from scope limitations, primarily focusing on\nmatching descriptive but vague queries with small collections of professionally\nedited, English-centric videos. To address this gap, we introduce\nMultiVENT 2.0, a large-scale, multilingual event-centric video\nretrieval benchmark featuring a collection of more than 218,000 news videos and\n3,906 queries targeting specific world events. These queries specifically\ntarget information found in the visual content, audio, embedded text, and text\nmetadata of the videos, requiring systems leverage all these sources to succeed\nat the task. Preliminary results show that state-of-the-art vision-language\nmodels struggle significantly with this task, and while alternative approaches\nshow promise, they are still insufficient to adequately address this problem.\nThese findings underscore the need for more robust multimodal retrieval\nsystems, as effective video retrieval is a crucial step towards multimodal\ncontent understanding and generation tasks.",
      "upvotes": 0
    }
  ]
}