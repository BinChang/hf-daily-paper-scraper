{
  "date": "2024-08-05",
  "papers": [
    {
      "title": "Medical SAM 2: Segment medical images as video via Segment Anything Model 2",
      "url": "https://huggingface.co/papers/2408.00874",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2408.00874.pdf",
      "abstract": "In this paper, we introduce Medical SAM 2 (MedSAM-2), an advanced\nsegmentation model that utilizes the SAM 2 framework to address both 2D and 3D\nmedical image segmentation tasks. By adopting the philosophy of taking medical\nimages as videos, MedSAM-2 not only applies to 3D medical images but also\nunlocks new One-prompt Segmentation capability. That allows users to provide a\nprompt for just one or a specific image targeting an object, after which the\nmodel can autonomously segment the same type of object in all subsequent\nimages, regardless of temporal relationships between the images. We evaluated\nMedSAM-2 across a variety of medical imaging modalities, including abdominal\norgans, optic discs, brain tumors, thyroid nodules, and skin lesions, comparing\nit against state-of-the-art models in both traditional and interactive\nsegmentation settings. Our findings show that MedSAM-2 not only surpasses\nexisting models in performance but also exhibits superior generalization across\na range of medical image segmentation tasks. Our code will be released at:\nhttps://github.com/MedicineToken/Medical-SAM2",
      "upvotes": 42
    },
    {
      "title": "POA: Pre-training Once for Models of All Sizes",
      "url": "https://huggingface.co/papers/2408.01031",
      "authors": [
        "Lei Yu",
        "Jian Wang",
        "Guo Ye",
        "Huimei He",
        "Ming Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.01031.pdf",
      "abstract": "Large-scale self-supervised pre-training has paved the way for one foundation\nmodel to handle many different vision tasks. Most pre-training methodologies\ntrain a single model of a certain size at one time. Nevertheless, various\ncomputation or storage constraints in real-world scenarios require substantial\nefforts to develop a series of models with different sizes to deploy. Thus, in\nthis study, we propose a novel tri-branch self-supervised training framework,\ntermed as POA (Pre-training Once for All), to tackle this aforementioned issue.\nOur approach introduces an innovative elastic student branch into a modern\nself-distillation paradigm. At each pre-training step, we randomly sample a\nsub-network from the original student to form the elastic student and train all\nbranches in a self-distilling fashion. Once pre-trained, POA allows the\nextraction of pre-trained models of diverse sizes for downstream tasks.\nRemarkably, the elastic student facilitates the simultaneous pre-training of\nmultiple models with different sizes, which also acts as an additional ensemble\nof models of various sizes to enhance representation learning. Extensive\nexperiments, including k-nearest neighbors, linear probing evaluation and\nassessments on multiple downstream tasks demonstrate the effectiveness and\nadvantages of our POA. It achieves state-of-the-art performance using ViT, Swin\nTransformer and ResNet backbones, producing around a hundred models with\ndifferent sizes through a single pre-training session. The code is available\nat: https://github.com/Qichuzyy/POA.",
      "upvotes": 26
    },
    {
      "title": "ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget",
      "url": "https://huggingface.co/papers/2408.00103",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2408.00103.pdf",
      "abstract": "Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in\nNatural Language Processing, serving as critical components in a wide range of\napplications. In this paper, we propose ReLiK, a Retriever-Reader architecture\nfor both EL and RE, where, given an input text, the Retriever module undertakes\nthe identification of candidate entities or relations that could potentially\nappear within the text. Subsequently, the Reader module is tasked to discern\nthe pertinent retrieved entities or relations and establish their alignment\nwith the corresponding textual spans. Notably, we put forward an innovative\ninput representation that incorporates the candidate entities or relations\nalongside the text, making it possible to link entities or extract relations in\na single forward pass and to fully leverage pre-trained language models\ncontextualization capabilities, in contrast with previous\nRetriever-Reader-based methods, which require a forward pass for each\ncandidate. Our formulation of EL and RE achieves state-of-the-art performance\nin both in-domain and out-of-domain benchmarks while using academic budget\ntraining and with up to 40x inference speed compared to competitors. Finally,\nwe show how our architecture can be used seamlessly for Information Extraction\n(cIE), i.e. EL + RE, and setting a new state of the art by employing a shared\nReader that simultaneously extracts entities and relations.",
      "upvotes": 16
    },
    {
      "title": "TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and Resampling",
      "url": "https://huggingface.co/papers/2408.01291",
      "authors": [
        "Zixin Guo",
        "Xinxin Zuo",
        "Juwei Lu",
        "Peng Dai",
        "Songcen Xu",
        "Li Cheng",
        "Yee-Hong Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.01291.pdf",
      "abstract": "Given a 3D mesh, we aim to synthesize 3D textures that correspond to\narbitrary textual descriptions. Current methods for generating and assembling\ntextures from sampled views often result in prominent seams or excessive\nsmoothing. To tackle these issues, we present TexGen, a novel multi-view\nsampling and resampling framework for texture generation leveraging a\npre-trained text-to-image diffusion model. For view consistent sampling, first\nof all we maintain a texture map in RGB space that is parameterized by the\ndenoising step and updated after each sampling step of the diffusion model to\nprogressively reduce the view discrepancy. An attention-guided multi-view\nsampling strategy is exploited to broadcast the appearance information across\nviews. To preserve texture details, we develop a noise resampling technique\nthat aids in the estimation of noise, generating inputs for subsequent\ndenoising steps, as directed by the text prompt and current texture map.\nThrough an extensive amount of qualitative and quantitative evaluations, we\ndemonstrate that our proposed method produces significantly better texture\nquality for diverse 3D objects with a high degree of view consistency and rich\nappearance details, outperforming current state-of-the-art methods.\nFurthermore, our proposed texture generation technique can also be applied to\ntexture editing while preserving the original identity. More experimental\nresults are available at https://dong-huo.github.io/TexGen/",
      "upvotes": 11
    },
    {
      "title": "In-Context Example Selection via Similarity Search Improves Low-Resource Machine Translation",
      "url": "https://huggingface.co/papers/2408.00397",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2408.00397.pdf",
      "abstract": "The ability of generative large language models (LLMs) to perform in-context\nlearning has given rise to a large body of research into how best to prompt\nmodels for various natural language processing tasks. In this paper, we focus\non machine translation (MT), a task that has been shown to benefit from\nin-context translation examples. However no systematic studies have been\npublished on how best to select examples, and mixed results have been reported\non the usefulness of similarity-based selection over random selection. We\nprovide a study covering multiple LLMs and multiple in-context example\nretrieval strategies, comparing multilingual sentence embeddings. We cover\nseveral language directions, representing different levels of language\nresourcedness (English into French, German, Swahili and Wolof). Contrarily to\npreviously published results, we find that sentence embedding similarity can\nimprove MT, especially for low-resource language directions, and discuss the\nbalance between selection pool diversity and quality. We also highlight\npotential problems with the evaluation of LLM-based MT and suggest a more\nappropriate evaluation protocol, adapting the COMET metric to the evaluation of\nLLMs. Code and outputs are freely available at\nhttps://github.com/ArmelRandy/ICL-MT.",
      "upvotes": 10
    },
    {
      "title": "MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models",
      "url": "https://huggingface.co/papers/2408.01337",
      "authors": [
        "George Fazekas"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.01337.pdf",
      "abstract": "Multimodal models that jointly process audio and language hold great promise\nin audio understanding and are increasingly being adopted in the music domain.\nBy allowing users to query via text and obtain information about a given audio\ninput, these models have the potential to enable a variety of music\nunderstanding tasks via language-based interfaces. However, their evaluation\nposes considerable challenges, and it remains unclear how to effectively assess\ntheir ability to correctly interpret music-related inputs with current methods.\nMotivated by this, we introduce MuChoMusic, a benchmark for evaluating music\nunderstanding in multimodal language models focused on audio. MuChoMusic\ncomprises 1,187 multiple-choice questions, all validated by human annotators,\non 644 music tracks sourced from two publicly available music datasets, and\ncovering a wide variety of genres. Questions in the benchmark are crafted to\nassess knowledge and reasoning abilities across several dimensions that cover\nfundamental musical concepts and their relation to cultural and functional\ncontexts. Through the holistic analysis afforded by the benchmark, we evaluate\nfive open-source models and identify several pitfalls, including an\nover-reliance on the language modality, pointing to a need for better\nmultimodal integration. Data and code are open-sourced.",
      "upvotes": 10
    },
    {
      "title": "RelBench: A Benchmark for Deep Learning on Relational Databases",
      "url": "https://huggingface.co/papers/2407.20060",
      "authors": [
        "Joshua Robinson",
        "Rishabh Ranjan",
        "Weihua Hu",
        "Matthias Fey",
        "Jan E. Lenssen",
        "Xinwei He",
        "Jure Leskovec"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.20060.pdf",
      "abstract": "We present RelBench, a public benchmark for solving predictive tasks over\nrelational databases with graph neural networks. RelBench provides databases\nand tasks spanning diverse domains and scales, and is intended to be a\nfoundational infrastructure for future research. We use RelBench to conduct the\nfirst comprehensive study of Relational Deep Learning (RDL) (Fey et al., 2024),\nwhich combines graph neural network predictive models with (deep) tabular\nmodels that extract initial entity-level representations from raw tables.\nEnd-to-end learned RDL models fully exploit the predictive signal encoded in\nprimary-foreign key links, marking a significant shift away from the dominant\nparadigm of manual feature engineering combined with tabular models. To\nthoroughly evaluate RDL against this prior gold-standard, we conduct an\nin-depth user study where an experienced data scientist manually engineers\nfeatures for each task. In this study, RDL learns better models whilst reducing\nhuman work needed by more than an order of magnitude. This demonstrates the\npower of deep learning for solving predictive tasks over relational databases,\nopening up many new research opportunities enabled by RelBench.",
      "upvotes": 7
    },
    {
      "title": "Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models",
      "url": "https://huggingface.co/papers/2408.00113",
      "authors": [
        "Benjamin Wright",
        "Rico Angell",
        "Logan Smith",
        "Claudio Mayrink Verdun",
        "Samuel Marks"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.00113.pdf",
      "abstract": "What latent features are encoded in language model (LM) representations?\nRecent work on training sparse autoencoders (SAEs) to disentangle interpretable\nfeatures in LM representations has shown significant promise. However,\nevaluating the quality of these SAEs is difficult because we lack a\nground-truth collection of interpretable features that we expect good SAEs to\nrecover. We thus propose to measure progress in interpretable dictionary\nlearning by working in the setting of LMs trained on chess and Othello\ntranscripts. These settings carry natural collections of interpretable features\n-- for example, \"there is a knight on F3\" -- which we leverage into\nsupervised metrics for SAE quality. To guide progress in\ninterpretable dictionary learning, we introduce a new SAE training technique,\np-annealing, which improves performance on prior unsupervised\nmetrics as well as our new metrics.",
      "upvotes": 6
    }
  ]
}