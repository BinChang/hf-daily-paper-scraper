{
  "date": "2024-10-24",
  "papers": [
    {
      "title": "MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models",
      "url": "https://huggingface.co/papers/2410.17637",
      "authors": [
        "Xiaoyi Dong",
        "Pan Zhang",
        "Jiaqi Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17637.pdf",
      "abstract": "Visual preference alignment involves training Large Vision-Language Models\n(LVLMs) to predict human preferences between visual inputs. This is typically\nachieved by using labeled datasets of chosen/rejected pairs and employing\noptimization algorithms like direct preference optimization (DPO). Existing\nvisual alignment methods, primarily designed for single-image scenarios,\nstruggle to effectively handle the complexity of multi-image tasks due to the\nscarcity of diverse training data and the high cost of annotating\nchosen/rejected pairs. We present Multi-Image Augmented Direct Preference\nOptimization (MIA-DPO), a visual preference alignment approach that effectively\nhandles multi-image inputs. MIA-DPO mitigates the scarcity of diverse\nmulti-image training data by extending single-image data with unrelated images\narranged in grid collages or pic-in-pic formats, significantly reducing the\ncosts associated with multi-image data annotations. Our observation reveals\nthat attention values of LVLMs vary considerably across different images. We\nuse attention values to identify and filter out rejected responses the model\nmay have mistakenly focused on. Our attention-aware selection for constructing\nthe chosen/rejected pairs without relying on (i) human annotation, (ii) extra\ndata, and (iii) external models or APIs. MIA-DPO is compatible with various\narchitectures and outperforms existing methods on five multi-image benchmarks,\nachieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on the\nrecent InternLM-XC2.5. Moreover, MIA-DPO has a minimal effect on the model's\nability to understand single images.",
      "upvotes": 34
    },
    {
      "title": "LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding",
      "url": "https://huggingface.co/papers/2410.17434",
      "authors": [
        "Changsheng Zhao",
        "Jun Chen",
        "Chenchen Zhu",
        "Zechun Liu",
        "Fanyi Xiao",
        "Balakrishnan Varadarajan",
        "Florian Bordes",
        "Zhuang Liu",
        "Hu Xu",
        "Hyunwoo J. Kim",
        "Bilge Soran",
        "Raghuraman Krishnamoorthi",
        "Mohamed Elhoseiny",
        "Vikas Chandra"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17434.pdf",
      "abstract": "Multimodal Large Language Models (MLLMs) have shown promising progress in\nunderstanding and analyzing video content. However, processing long videos\nremains a significant challenge constrained by LLM's context size. To address\nthis limitation, we propose LongVU, a spatiotemporal adaptive compression\nmechanism thats reduces the number of video tokens while preserving visual\ndetails of long videos. Our idea is based on leveraging cross-modal query and\ninter-frame dependencies to adaptively reduce temporal and spatial redundancy\nin videos. Specifically, we leverage DINOv2 features to remove redundant frames\nthat exhibit high similarity. Then we utilize text-guided cross-modal query for\nselective frame feature reduction. Further, we perform spatial token reduction\nacross frames based on their temporal dependencies. Our adaptive compression\nstrategy effectively processes a large number of frames with little visual\ninformation loss within given context length. Our LongVU consistently surpass\nexisting methods across a variety of video understanding benchmarks, especially\non hour-long video understanding tasks such as VideoMME and MLVU. Given a\nlight-weight LLM, our LongVU also scales effectively into a smaller size with\nstate-of-the-art video understanding performance.",
      "upvotes": 24
    },
    {
      "title": "WorldSimBench: Towards Video Generation Models as World Simulators",
      "url": "https://huggingface.co/papers/2410.18072",
      "authors": [
        "Yiran Qin",
        "Jiwen Yu",
        "Xijun Wang",
        "Lu Sheng",
        "Jing Shao",
        "Lei Bai",
        "Wanli Ouyang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.18072.pdf",
      "abstract": "Recent advancements in predictive models have demonstrated exceptional\ncapabilities in predicting the future state of objects and scenes. However, the\nlack of categorization based on inherent characteristics continues to hinder\nthe progress of predictive model development. Additionally, existing benchmarks\nare unable to effectively evaluate higher-capability, highly embodied\npredictive models from an embodied perspective. In this work, we classify the\nfunctionalities of predictive models into a hierarchy and take the first step\nin evaluating World Simulators by proposing a dual evaluation framework called\nWorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and\nImplicit Manipulative Evaluation, encompassing human preference assessments\nfrom the visual perspective and action-level evaluations in embodied tasks,\ncovering three representative embodied scenarios: Open-Ended Embodied\nEnvironment, Autonomous, Driving, and Robot Manipulation. In the Explicit\nPerceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment\ndataset based on fine-grained human feedback, which we use to train a Human\nPreference Evaluator that aligns with human perception and explicitly assesses\nthe visual fidelity of World Simulators. In the Implicit Manipulative\nEvaluation, we assess the video-action consistency of World Simulators by\nevaluating whether the generated situation-aware video can be accurately\ntranslated into the correct control signals in dynamic environments. Our\ncomprehensive evaluation offers key insights that can drive further innovation\nin video generation models, positioning World Simulators as a pivotal\nadvancement toward embodied artificial intelligence.",
      "upvotes": 16
    },
    {
      "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
      "url": "https://huggingface.co/papers/2410.17891",
      "authors": [
        "Yizhe Zhang",
        "Jiacheng Ye",
        "Lin Zheng",
        "Chenxin An",
        "Peilin Zhao",
        "Wei Bi",
        "Jiawei Han",
        "Hao Peng",
        "Lingpeng Kong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17891.pdf",
      "abstract": "Diffusion Language Models (DLMs) have emerged as a promising new paradigm for\ntext generative modeling, potentially addressing limitations of autoregressive\n(AR) models. However, current DLMs have been studied at a smaller scale\ncompared to their AR counterparts and lack fair comparison on language modeling\nbenchmarks. Additionally, training diffusion models from scratch at scale\nremains challenging. Given the prevalence of open-source AR language models, we\npropose adapting these models to build text diffusion models. We demonstrate\nconnections between AR and diffusion modeling objectives and introduce a simple\ncontinual pre-training approach for training diffusion models. Through\nsystematic evaluation on language modeling, reasoning, and commonsense\nbenchmarks, we show that we can convert AR models ranging from 127M to 7B\nparameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA,\nusing less than 200B tokens for training. Our experimental results reveal that\nthese models outperform earlier DLMs and are competitive with their AR\ncounterparts. We release a suite of DLMs (with 127M, 355M, and 7B parameters)\ncapable of generating fluent text, performing in-context learning, filling in\nthe middle without prompt re-ordering, and following instructions\nhttps://github.com/HKUNLP/DiffuLLaMA.",
      "upvotes": 15
    },
    {
      "title": "Scalable Ranked Preference Optimization for Text-to-Image Generation",
      "url": "https://huggingface.co/papers/2410.18013",
      "authors": [
        "Zeynep Akata",
        "Sergey Tulyakov",
        "Jian Ren"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.18013.pdf",
      "abstract": "Direct Preference Optimization (DPO) has emerged as a powerful approach to\nalign text-to-image (T2I) models with human feedback. Unfortunately, successful\napplication of DPO to T2I models requires a huge amount of resources to collect\nand label large-scale datasets, e.g., millions of generated paired images\nannotated with human preferences. In addition, these human preference datasets\ncan get outdated quickly as the rapid improvements of T2I models lead to higher\nquality images. In this work, we investigate a scalable approach for collecting\nlarge-scale and fully synthetic datasets for DPO training. Specifically, the\npreferences for paired images are generated using a pre-trained reward\nfunction, eliminating the need for involving humans in the annotation process,\ngreatly improving the dataset collection efficiency. Moreover, we demonstrate\nthat such datasets allow averaging predictions across multiple models and\ncollecting ranked preferences as opposed to pairwise preferences. Furthermore,\nwe introduce RankDPO to enhance DPO-based methods using the ranking feedback.\nApplying RankDPO on SDXL and SD3-Medium models with our synthetically generated\npreference dataset ``Syn-Pic'' improves both prompt-following (on benchmarks\nlike T2I-Compbench, GenEval, and DPG-Bench) and visual quality (through user\nstudies). This pipeline presents a practical and scalable solution to develop\nbetter preference datasets to enhance the performance of text-to-image models.",
      "upvotes": 14
    },
    {
      "title": "DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes",
      "url": "https://huggingface.co/papers/2410.18084",
      "authors": [
        "Yu Qiao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.18084.pdf",
      "abstract": "LiDAR scene generation has been developing rapidly recently. However,\nexisting methods primarily focus on generating static and single-frame scenes,\noverlooking the inherently dynamic nature of real-world driving environments.\nIn this work, we introduce DynamicCity, a novel 4D LiDAR generation framework\ncapable of generating large-scale, high-quality LiDAR scenes that capture the\ntemporal evolution of dynamic environments. DynamicCity mainly consists of two\nkey models. 1) A VAE model for learning HexPlane as the compact 4D\nrepresentation. Instead of using naive averaging operations, DynamicCity\nemploys a novel Projection Module to effectively compress 4D LiDAR features\ninto six 2D feature maps for HexPlane construction, which significantly\nenhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we\nutilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in\nparallel, which improves both network training efficiency and reconstruction\naccuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x\ntraining speedup, and 70.84% memory reduction). 2) A DiT-based diffusion model\nfor HexPlane generation. To make HexPlane feasible for DiT generation, a Padded\nRollout Operation is proposed to reorganize all six feature planes of the\nHexPlane as a squared 2D feature map. In particular, various conditions could\nbe introduced in the diffusion or sampling process, supporting versatile 4D\ngeneration applications, such as trajectory- and command-driven generation,\ninpainting, and layout-conditioned generation. Extensive experiments on the\nCarlaSC and Waymo datasets demonstrate that DynamicCity significantly\noutperforms existing state-of-the-art 4D LiDAR generation methods across\nmultiple metrics. The code will be released to facilitate future research.",
      "upvotes": 12
    },
    {
      "title": "M-RewardBench: Evaluating Reward Models in Multilingual Settings",
      "url": "https://huggingface.co/papers/2410.15522",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.15522.pdf",
      "abstract": "Reward models (RMs) have driven the state-of-the-art performance of LLMs\ntoday by enabling the integration of human feedback into the language modeling\nprocess. However, RMs are primarily trained and evaluated in English, and their\ncapabilities in multilingual settings remain largely understudied. In this\nwork, we conduct a systematic evaluation of several reward models in\nmultilingual settings. We first construct the first-of-its-kind multilingual RM\nevaluation benchmark, M-RewardBench, consisting of 2.87k preference instances\nfor 23 typologically diverse languages, that tests the chat, safety, reasoning,\nand translation capabilities of RMs. We then rigorously evaluate a wide range\nof reward models on M-RewardBench, offering fresh insights into their\nperformance across diverse languages. We identify a significant gap in RMs'\nperformances between English and non-English languages and show that RM\npreferences can change substantially from one language to another. We also\npresent several findings on how different multilingual aspects impact RM\nperformance. Specifically, we show that the performance of RMs is improved with\nimproved translation quality. Similarly, we demonstrate that the models exhibit\nbetter performance for high-resource languages. We release M-RewardBench\ndataset and the codebase in this study to facilitate a better understanding of\nRM evaluation in multilingual settings.",
      "upvotes": 10
    },
    {
      "title": "Lightweight Neural App Control",
      "url": "https://huggingface.co/papers/2410.17883",
      "authors": [
        "Jianye Hao",
        "Jun Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17883.pdf",
      "abstract": "This paper introduces a novel mobile phone control architecture, termed ``app\nagents\", for efficient interactions and controls across various Android apps.\nThe proposed Lightweight Multi-modal App Control (LiMAC) takes as input a\ntextual goal and a sequence of past mobile observations, such as screenshots\nand corresponding UI trees, to generate precise actions. To address the\ncomputational constraints inherent to smartphones, within LiMAC, we introduce a\nsmall Action Transformer (AcT) integrated with a fine-tuned vision-language\nmodel (VLM) for real-time decision-making and task execution. We evaluate LiMAC\non two open-source mobile control datasets, demonstrating the superior\nperformance of our small-form-factor approach against fine-tuned versions of\nopen-source VLMs, such as Florence2 and Qwen2-VL. It also significantly\noutperforms prompt engineering baselines utilising closed-source foundation\nmodels like GPT-4o. More specifically, LiMAC increases the overall action\naccuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to\nprompt-engineering baselines.",
      "upvotes": 8
    },
    {
      "title": "MedINST: Meta Dataset of Biomedical Instructions",
      "url": "https://huggingface.co/papers/2410.13458",
      "authors": [
        "Wenhan Han",
        "Meng Fang",
        "Zihan Zhang",
        "Yu Yin",
        "Ling Chen",
        "Mykola Pechenizkiy"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.13458.pdf",
      "abstract": "The integration of large language model (LLM) techniques in the field of\nmedical analysis has brought about significant advancements, yet the scarcity\nof large, diverse, and well-annotated datasets remains a major challenge.\nMedical data and tasks, which vary in format, size, and other parameters,\nrequire extensive preprocessing and standardization for effective use in\ntraining LLMs. To address these challenges, we introduce MedINST, the Meta\nDataset of Biomedical Instructions, a novel multi-domain, multi-task\ninstructional meta-dataset. MedINST comprises 133 biomedical NLP tasks and over\n7 million training samples, making it the most comprehensive biomedical\ninstruction dataset to date. Using MedINST as the meta dataset, we curate\nMedINST32, a challenging benchmark with different task difficulties aiming to\nevaluate LLMs' generalization ability. We fine-tune several LLMs on MedINST and\nevaluate on MedINST32, showcasing enhanced cross-task generalization.",
      "upvotes": 6
    },
    {
      "title": "ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding",
      "url": "https://huggingface.co/papers/2410.13924",
      "authors": [
        "Silvan Weder",
        "Marc Pollefeys"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.13924.pdf",
      "abstract": "The performance of neural networks scales with both their size and the amount\nof data they have been trained on. This is shown in both language and image\ngeneration. However, this requires scaling-friendly network architectures as\nwell as large-scale datasets. Even though scaling-friendly architectures like\ntransformers have emerged for 3D vision tasks, the GPT-moment of 3D vision\nremains distant due to the lack of training data. In this paper, we introduce\nARKit LabelMaker, the first large-scale, real-world 3D dataset with dense\nsemantic annotations. Specifically, we complement ARKitScenes dataset with\ndense semantic annotations that are automatically generated at scale. To this\nend, we extend LabelMaker, a recent automatic annotation pipeline, to serve the\nneeds of large-scale pre-training. This involves extending the pipeline with\ncutting-edge segmentation models as well as making it robust to the challenges\nof large-scale processing. Further, we push forward the state-of-the-art\nperformance on ScanNet and ScanNet200 dataset with prevalent 3D semantic\nsegmentation models, demonstrating the efficacy of our generated dataset.",
      "upvotes": 6
    },
    {
      "title": "TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts",
      "url": "https://huggingface.co/papers/2410.18071",
      "authors": [
        "Yuxuan Xie",
        "Tianhua Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.18071.pdf",
      "abstract": "Recently, multimodal large language models (MLLMs) have received much\nattention for their impressive capabilities. The evaluation of MLLMs is\nbecoming critical to analyzing attributes of MLLMs and providing valuable\ninsights. However, current benchmarks overlook the problem of prompt\nsensitivity - minor prompt variations may lead to significant performance\nfluctuations. Thus, inappropriate prompts may obscure the models' capabilities,\nunderestimating the models' performance. Moreover, different models have\ndifferent preferences for different prompts, and thus, using the same prompt\nfor all models will cause evaluation bias. This paper analyzes this deficiency\nin existing benchmarks and further introduces a new evaluation framework named\nTP-Eval, which introduces a prompt customization method to reduce evaluation\nbiases and tap models' potential. TP-Eval will rewrite the original prompts to\ndifferent customized prompts for different models. In particular, we propose\nsome well-designed modules for prompt customization tailored to the scenario of\nMLLM evaluation. Extensive experiments demonstrate the effectiveness of our\napproach to uncovering models' capabilities, and TP-Eval should benefit the\ncommunity in developing more comprehensive and convincing MLLM evaluation\nbenchmarks.",
      "upvotes": 6
    },
    {
      "title": "LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias",
      "url": "https://huggingface.co/papers/2410.17242",
      "authors": [
        "Haian Jin",
        "Hanwen Jiang",
        "Hao Tan",
        "Kai Zhang",
        "Sai Bi",
        "Tianyuan Zhang",
        "Fujun Luan",
        "Noah Snavely",
        "Zexiang Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17242.pdf",
      "abstract": "We propose the Large View Synthesis Model (LVSM), a novel transformer-based\napproach for scalable and generalizable novel view synthesis from sparse-view\ninputs. We introduce two architectures: (1) an encoder-decoder LVSM, which\nencodes input image tokens into a fixed number of 1D latent tokens, functioning\nas a fully learned scene representation, and decodes novel-view images from\nthem; and (2) a decoder-only LVSM, which directly maps input images to\nnovel-view outputs, completely eliminating intermediate scene representations.\nBoth models bypass the 3D inductive biases used in previous methods -- from 3D\nrepresentations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar\nprojections, plane sweeps) -- addressing novel view synthesis with a fully\ndata-driven approach. While the encoder-decoder model offers faster inference\ndue to its independent latent representation, the decoder-only LVSM achieves\nsuperior quality, scalability, and zero-shot generalization, outperforming\nprevious state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive\nevaluations across multiple datasets demonstrate that both LVSM variants\nachieve state-of-the-art novel view synthesis quality. Notably, our models\nsurpass all previous methods even with reduced computational resources (1-2\nGPUs). Please see our website for more details:\nhttps://haian-jin.github.io/projects/LVSM/ .",
      "upvotes": 3
    },
    {
      "title": "Steering Your Generalists: Improving Robotic Foundation Models via Value Guidance",
      "url": "https://huggingface.co/papers/2410.13816",
      "authors": [
        "Mitsuhiko Nakamoto",
        "Oier Mees",
        "Aviral Kumar",
        "Sergey Levine"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.13816.pdf",
      "abstract": "Large, general-purpose robotic policies trained on diverse demonstration\ndatasets have been shown to be remarkably effective both for controlling a\nvariety of robots in a range of different scenes, and for acquiring broad\nrepertoires of manipulation skills. However, the data that such policies are\ntrained on is generally of mixed quality -- not only are human-collected\ndemonstrations unlikely to perform the task perfectly, but the larger the\ndataset is, the harder it is to curate only the highest quality examples. It\nalso remains unclear how optimal data from one embodiment is for training on\nanother embodiment. In this paper, we present a general and broadly applicable\napproach that enhances the performance of such generalist robot policies at\ndeployment time by re-ranking their actions according to a value function\nlearned via offline RL. This approach, which we call Value-Guided Policy\nSteering (V-GPS), is compatible with a wide range of different generalist\npolicies, without needing to fine-tune or even access the weights of the\npolicy. We show that the same value function can improve the performance of\nfive different state-of-the-art policies with different architectures, even\nthough they were trained on distinct datasets, attaining consistent performance\nimprovement on multiple robotic platforms across a total of 12 tasks. Code and\nvideos can be found at: https://nakamotoo.github.io/V-GPS",
      "upvotes": 1
    }
  ]
}