{
  "date": "2024-08-09",
  "papers": [
    {
      "title": "Transformer Explainer: Interactive Learning of Text-Generative Models",
      "url": "https://huggingface.co/papers/2408.04619",
      "authors": [
        "Zijie J. Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04619.pdf",
      "abstract": "Transformers have revolutionized machine learning, yet their inner workings\nremain opaque to many. We present Transformer Explainer, an interactive\nvisualization tool designed for non-experts to learn about Transformers through\nthe GPT-2 model. Our tool helps users understand complex Transformer concepts\nby integrating a model overview and enabling smooth transitions across\nabstraction levels of mathematical operations and model structures. It runs a\nlive GPT-2 instance locally in the user's browser, empowering users to\nexperiment with their own input and observe in real-time how the internal\ncomponents and parameters of the Transformer work together to predict the next\ntokens. Our tool requires no installation or special hardware, broadening the\npublic's education access to modern generative AI techniques. Our open-sourced\ntool is available at https://poloclub.github.io/transformer-explainer/. A video\ndemo is available at https://youtu.be/ECR4oAwocjs.",
      "upvotes": 154
    },
    {
      "title": "GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI",
      "url": "https://huggingface.co/papers/2408.03361",
      "authors": [
        "Pengcheng Chen",
        "Jin Ye",
        "Yanjun Li",
        "Zhongying Deng",
        "Wei Li",
        "Yanzhou Su",
        "Shaoting Zhang",
        "Bin Fu",
        "Eric J Seibel",
        "Yu Qiao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.03361.pdf",
      "abstract": "Large Vision-Language Models (LVLMs) are capable of handling diverse data\ntypes such as imaging, text, and physiological signals, and can be applied in\nvarious fields. In the medical field, LVLMs have a high potential to offer\nsubstantial assistance for diagnosis and treatment. Before that, it is crucial\nto develop benchmarks to evaluate LVLMs' effectiveness in various medical\napplications. Current benchmarks are often built upon specific academic\nliterature, mainly focusing on a single domain, and lacking varying perceptual\ngranularities. Thus, they face specific challenges, including limited clinical\nrelevance, incomplete evaluations, and insufficient guidance for interactive\nLVLMs. To address these limitations, we developed the GMAI-MMBench, the most\ncomprehensive general medical AI benchmark with well-categorized data structure\nand multi-perceptual granularity to date. It is constructed from 285 datasets\nacross 39 medical image modalities, 18 clinical-related tasks, 18 departments,\nand 4 perceptual granularities in a Visual Question Answering (VQA) format.\nAdditionally, we implemented a lexical tree structure that allows users to\ncustomize evaluation tasks, accommodating various assessment needs and\nsubstantially supporting medical AI research and applications. We evaluated 50\nLVLMs, and the results show that even the advanced GPT-4o only achieves an\naccuracy of 52%, indicating significant room for improvement. Moreover, we\nidentified five key insufficiencies in current cutting-edge LVLMs that need to\nbe addressed to advance the development of better medical applications. We\nbelieve that GMAI-MMBench will stimulate the community to build the next\ngeneration of LVLMs toward GMAI.\n  Project Page: https://uni-medical.github.io/GMAI-MMBench.github.io/",
      "upvotes": 85
    },
    {
      "title": "Sketch2Scene: Automatic Generation of Interactive 3D Game Scenes from User's Casual Sketches",
      "url": "https://huggingface.co/papers/2408.04567",
      "authors": [
        "Yongzhi Xu",
        "Yang Li",
        "Hongdong Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04567.pdf",
      "abstract": "3D Content Generation is at the heart of many computer graphics applications,\nincluding video gaming, film-making, virtual and augmented reality, etc. This\npaper proposes a novel deep-learning based approach for automatically\ngenerating interactive and playable 3D game scenes, all from the user's casual\nprompts such as a hand-drawn sketch. Sketch-based input offers a natural, and\nconvenient way to convey the user's design intention in the content creation\nprocess. To circumvent the data-deficient challenge in learning (i.e. the lack\nof large training data of 3D scenes), our method leverages a pre-trained 2D\ndenoising diffusion model to generate a 2D image of the scene as the conceptual\nguidance. In this process, we adopt the isometric projection mode to factor out\nunknown camera poses while obtaining the scene layout. From the generated\nisometric image, we use a pre-trained image understanding method to segment the\nimage into meaningful parts, such as off-ground objects, trees, and buildings,\nand extract the 2D scene layout. These segments and layouts are subsequently\nfed into a procedural content generation (PCG) engine, such as a 3D video game\nengine like Unity or Unreal, to create the 3D scene. The resulting 3D scene can\nbe seamlessly integrated into a game development environment and is readily\nplayable. Extensive tests demonstrate that our method can efficiently generate\nhigh-quality and interactive 3D game scenes with layouts that closely follow\nthe user's intention.",
      "upvotes": 23
    },
    {
      "title": "LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection",
      "url": "https://huggingface.co/papers/2408.04284",
      "authors": [
        "Alexander Aziz",
        "Yuxia Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04284.pdf",
      "abstract": "The widespread accessibility of large language models (LLMs) to the general\npublic has significantly amplified the dissemination of machine-generated texts\n(MGTs). Advancements in prompt manipulation have exacerbated the difficulty in\ndiscerning the origin of a text (human-authored vs machinegenerated). This\nraises concerns regarding the potential misuse of MGTs, particularly within\neducational and academic domains. In this paper, we present\nLLM-DetectAIve -- a system designed for fine-grained MGT detection.\nIt is able to classify texts into four categories: human-written,\nmachine-generated, machine-written machine-humanized, and human-written\nmachine-polished. Contrary to previous MGT detectors that perform binary\nclassification, introducing two additional categories in LLM-DetectiAIve offers\ninsights into the varying degrees of LLM intervention during the text creation.\nThis might be useful in some domains like education, where any LLM intervention\nis usually prohibited. Experiments show that LLM-DetectAIve can effectively\nidentify the authorship of textual content, proving its usefulness in enhancing\nintegrity in education, academia, and other domains. LLM-DetectAIve is publicly\naccessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video\ndescribing our system is available at https://youtu.be/E8eT_bE7k8c.",
      "upvotes": 22
    },
    {
      "title": "Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models",
      "url": "https://huggingface.co/papers/2408.04594",
      "authors": [
        "Yilun Huang",
        "Yaliang Li",
        "Ying Shen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04594.pdf",
      "abstract": "High-performance Multimodal Large Language Models (MLLMs) rely heavily on\ndata quality. This study introduces a novel dataset named Img-Diff, designed to\nenhance fine-grained image recognition in MLLMs by leveraging insights from\ncontrastive learning and image difference captioning. By analyzing object\ndifferences between similar images, we challenge models to identify both\nmatching and distinct components. We utilize the Stable-Diffusion-XL model and\nadvanced image editing techniques to create pairs of similar images that\nhighlight object replacements. Our methodology includes a Difference Area\nGenerator for object differences identifying, followed by a Difference Captions\nGenerator for detailed difference descriptions. The result is a relatively\nsmall but high-quality dataset of \"object replacement\" samples. We use the the\nproposed dataset to fine-tune state-of-the-art (SOTA) MLLMs such as MGM-7B,\nyielding comprehensive improvements of performance scores over SOTA models that\ntrained with larger-scale datasets, in numerous image difference and Visual\nQuestion Answering tasks. For instance, our trained models notably surpass the\nSOTA models GPT-4V and Gemini on the MMVP benchmark. Besides, we investigate\nalternative methods for generating image difference data through \"object\nremoval\" and conduct thorough evaluation to confirm the dataset's diversity,\nquality, and robustness, presenting several insights on synthesis of such\ncontrastive dataset. To encourage further research and advance the field of\nmultimodal data synthesis and enhancement of MLLMs' fundamental capabilities\nfor image understanding, we release our codes and dataset at\nhttps://github.com/modelscope/data-juicer/tree/ImgDiff.",
      "upvotes": 14
    },
    {
      "title": "Better Alignment with Instruction Back-and-Forth Translation",
      "url": "https://huggingface.co/papers/2408.04614",
      "authors": [
        "Sewoong Oh",
        "Luke Zettlemoyer",
        "Xian Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04614.pdf",
      "abstract": "We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment.",
      "upvotes": 14
    },
    {
      "title": "Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation of LLMs for Low-Resource NLP",
      "url": "https://huggingface.co/papers/2408.04303",
      "authors": [
        "Hayastan Avetisyan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04303.pdf",
      "abstract": "The development of monolingual language models for low and mid-resource\nlanguages continues to be hindered by the difficulty in sourcing high-quality\ntraining data. In this study, we present a novel cross-lingual vocabulary\ntransfer strategy, trans-tokenization, designed to tackle this challenge and\nenable more efficient language adaptation. Our approach focuses on adapting a\nhigh-resource monolingual LLM to an unseen target language by initializing the\ntoken embeddings of the target language using a weighted average of\nsemantically similar token embeddings from the source language. For this, we\nleverage a translation resource covering both the source and target languages.\nWe validate our method with the Tweeties, a series of trans-tokenized LLMs, and\ndemonstrate their competitive performance on various downstream tasks across a\nsmall but diverse set of languages. Additionally, we introduce Hydra LLMs,\nmodels with multiple swappable language modeling heads and embedding tables,\nwhich further extend the capabilities of our trans-tokenization strategy. By\ndesigning a Hydra LLM based on the multilingual model TowerInstruct, we\ndeveloped a state-of-the-art machine translation model for Tatar, in a\nzero-shot manner, completely bypassing the need for high-quality parallel data.\nThis breakthrough is particularly significant for low-resource languages like\nTatar, where high-quality parallel data is hard to come by. By lowering the\ndata and time requirements for training high-quality models, our\ntrans-tokenization strategy allows for the development of LLMs for a wider\nrange of languages, especially those with limited resources. We hope that our\nwork will inspire further research and collaboration in the field of\ncross-lingual vocabulary transfer and contribute to the empowerment of\nlanguages on a global scale.",
      "upvotes": 9
    },
    {
      "title": "Task-oriented Sequential Grounding in 3D Scenes",
      "url": "https://huggingface.co/papers/2408.04034",
      "authors": [
        "Ziyu Zhu",
        "Yixin Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04034.pdf",
      "abstract": "Grounding natural language in physical 3D environments is essential for the\nadvancement of embodied artificial intelligence. Current datasets and models\nfor 3D visual grounding predominantly focus on identifying and localizing\nobjects from static, object-centric descriptions. These approaches do not\nadequately address the dynamic and sequential nature of task-oriented grounding\nnecessary for practical applications. In this work, we propose a new task:\nTask-oriented Sequential Grounding in 3D scenes, wherein an agent must follow\ndetailed step-by-step instructions to complete daily activities by locating a\nsequence of target objects in indoor scenes. To facilitate this task, we\nintroduce SG3D, a large-scale dataset containing 22,346 tasks with 112,236\nsteps across 4,895 real-world 3D scenes. The dataset is constructed using a\ncombination of RGB-D scans from various 3D scene datasets and an automated task\ngeneration pipeline, followed by human verification for quality assurance. We\nadapted three state-of-the-art 3D visual grounding models to the sequential\ngrounding task and evaluated their performance on SG3D. Our results reveal that\nwhile these models perform well on traditional benchmarks, they face\nsignificant challenges with task-oriented sequential grounding, underscoring\nthe need for further research in this area.",
      "upvotes": 8
    },
    {
      "title": "Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics",
      "url": "https://huggingface.co/papers/2408.04631",
      "authors": [
        "Chuanxia Zheng",
        "Christian Rupprecht",
        "Andrea Vedaldi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04631.pdf",
      "abstract": "We present Puppet-Master, an interactive video generative model that can\nserve as a motion prior for part-level dynamics. At test time, given a single\nimage and a sparse set of motion trajectories (i.e., drags), Puppet-Master can\nsynthesize a video depicting realistic part-level motion faithful to the given\ndrag interactions. This is achieved by fine-tuning a large-scale pre-trained\nvideo diffusion model, for which we propose a new conditioning architecture to\ninject the dragging control effectively. More importantly, we introduce the\nall-to-first attention mechanism, a drop-in replacement for the widely adopted\nspatial attention modules, which significantly improves generation quality by\naddressing the appearance and background issues in existing models. Unlike\nother motion-conditioned video generators that are trained on in-the-wild\nvideos and mostly move an entire object, Puppet-Master is learned from\nObjaverse-Animation-HQ, a new dataset of curated part-level motion clips. We\npropose a strategy to automatically filter out sub-optimal animations and\naugment the synthetic renderings with meaningful motion trajectories.\nPuppet-Master generalizes well to real images across various categories and\noutperforms existing methods in a zero-shot manner on a real-world benchmark.\nSee our project page for more results: vgg-puppetmaster.github.io.",
      "upvotes": 8
    },
    {
      "title": "VGGHeads: A Large-Scale Synthetic Dataset for 3D Human Heads",
      "url": "https://huggingface.co/papers/2407.18245",
      "authors": [
        "Orest Kupyn",
        "Eugene Khvedchenia",
        "Christian Rupprecht"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.18245.pdf",
      "abstract": "Human head detection, keypoint estimation, and 3D head model fitting are\nimportant tasks with many applications. However, traditional real-world\ndatasets often suffer from bias, privacy, and ethical concerns, and they have\nbeen recorded in laboratory environments, which makes it difficult for trained\nmodels to generalize. Here, we introduce VGGHeads -- a large scale synthetic\ndataset generated with diffusion models for human head detection and 3D mesh\nestimation. Our dataset comprises over 1 million high-resolution images, each\nannotated with detailed 3D head meshes, facial landmarks, and bounding boxes.\nUsing this dataset we introduce a new model architecture capable of\nsimultaneous heads detection and head meshes reconstruction from a single image\nin a single step. Through extensive experimental evaluations, we demonstrate\nthat models trained on our synthetic data achieve strong performance on real\nimages. Furthermore, the versatility of our dataset makes it applicable across\na broad spectrum of tasks, offering a general and comprehensive representation\nof human heads. Additionally, we provide detailed information about the\nsynthetic data generation pipeline, enabling it to be re-used for other tasks\nand domains.",
      "upvotes": 7
    },
    {
      "title": "Advancing Molecular Machine (Learned) Representations with Stereoelectronics-Infused Molecular Graphs",
      "url": "https://huggingface.co/papers/2408.04520",
      "authors": [
        "Benjamin Sanchez-Lengeling",
        "Samuel M. Blau"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04520.pdf",
      "abstract": "Molecular representation is a foundational element in our understanding of\nthe physical world. Its importance ranges from the fundamentals of chemical\nreactions to the design of new therapies and materials. Previous molecular\nmachine learning models have employed strings, fingerprints, global features,\nand simple molecular graphs that are inherently information-sparse\nrepresentations. However, as the complexity of prediction tasks increases, the\nmolecular representation needs to encode higher fidelity information. This work\nintroduces a novel approach to infusing quantum-chemical-rich information into\nmolecular graphs via stereoelectronic effects. We show that the explicit\naddition of stereoelectronic interactions significantly improves the\nperformance of molecular machine learning models. Furthermore,\nstereoelectronics-infused representations can be learned and deployed with a\ntailored double graph neural network workflow, enabling its application to any\ndownstream molecular machine learning task. Finally, we show that the learned\nrepresentations allow for facile stereoelectronic evaluation of previously\nintractable systems, such as entire proteins, opening new avenues of molecular\ndesign.",
      "upvotes": 5
    },
    {
      "title": "Learning Task Decomposition to Assist Humans in Competitive Programming",
      "url": "https://huggingface.co/papers/2406.04604",
      "authors": [
        "Minlie Huang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04604.pdf",
      "abstract": "When using language models (LMs) to solve complex problems, humans might\nstruggle to understand the LM-generated solutions and repair the flawed ones.\nTo assist humans in repairing them, we propose to automatically decompose\ncomplex solutions into multiple simpler pieces that correspond to specific\nsubtasks. We introduce a novel objective for learning task decomposition,\ntermed assistive value (AssistV), which measures the feasibility and speed for\nhumans to repair the decomposed solution. We collect a dataset of human repair\nexperiences on different decomposed solutions. Utilizing the collected data as\nin-context examples, we then learn to critique, refine, and rank decomposed\nsolutions to improve AssistV. We validate our method under competitive\nprogramming problems: under 177 hours of human study, our method enables\nnon-experts to solve 33.3\\% more problems, speeds them up by 3.3x, and empowers\nthem to match unassisted experts.",
      "upvotes": 4
    },
    {
      "title": "Learning to Predict Program Execution by Modeling Dynamic Dependency on Code Graphs",
      "url": "https://huggingface.co/papers/2408.02816",
      "authors": [
        "Cuong Chi Le",
        "Hoang Nhat Phan",
        "Huy Nhat Phan",
        "Tien N. Nguyen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.02816.pdf",
      "abstract": "Predicting program behavior without execution is an essential and challenging\ntask in software engineering. Traditional models often struggle to capture\ndynamic dependencies and interactions within code. This paper introduces a\nnovel machine learning-based framework called CodeFlowrepresents, which\npredicts code coverage and detects runtime errors through Dynamic Dependencies\nLearning. Utilizing control flow graphs (CFGs), CodeFlowrepresents all possible\nexecution paths and the relationships between different statements, offering a\ncomprehensive understanding of program behavior. It constructs CFGs to depict\nexecution paths and learns vector representations for CFG nodes, capturing\nstatic control-flow dependencies. Additionally, it learns dynamic dependencies\nthrough execution traces, which reflect the impacts among statements during\nexecution. This approach enables accurate prediction of code coverage and\nidentification of runtime errors. Empirical evaluations show significant\nimprovements in code coverage prediction accuracy and effective localization of\nruntime errors, surpassing current models.",
      "upvotes": 4
    }
  ]
}