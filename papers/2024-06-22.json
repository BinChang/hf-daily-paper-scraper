{
  "date": "2024-06-22",
  "papers": [
    {
      "title": "$\\nabla^2$DFT: A Universal Quantum Chemistry Dataset of Drug-Like Molecules and a Benchmark for Neural Network Potentials",
      "url": "https://huggingface.co/papers/2406.14347",
      "authors": [
        "Ilya Shenbin",
        "Anton Alekseev",
        "Mikhail Shirokikh"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14347.pdf",
      "abstract": "Methods of computational quantum chemistry provide accurate approximations of\nmolecular properties crucial for computer-aided drug discovery and other areas\nof chemical science. However, high computational complexity limits the\nscalability of their applications. Neural network potentials (NNPs) are a\npromising alternative to quantum chemistry methods, but they require large and\ndiverse datasets for training. This work presents a new dataset and benchmark\ncalled nabla^2DFT that is based on the nablaDFT. It contains twice as much\nmolecular structures, three times more conformations, new data types and tasks,\nand state-of-the-art models. The dataset includes energies, forces, 17\nmolecular properties, Hamiltonian and overlap matrices, and a wavefunction\nobject. All calculations were performed at the DFT level\n(omegaB97X-D/def2-SVP) for each conformation. Moreover, nabla^2DFT is the\nfirst dataset that contains relaxation trajectories for a substantial number of\ndrug-like molecules. We also introduce a novel benchmark for evaluating NNPs in\nmolecular property prediction, Hamiltonian prediction, and conformational\noptimization tasks. Finally, we propose an extendable framework for training\nNNPs and implement 10 models within it.",
      "upvotes": 98
    },
    {
      "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners",
      "url": "https://huggingface.co/papers/2406.14491",
      "authors": [
        "Junyu Bi",
        "Minlie Huang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14491.pdf",
      "abstract": "Unsupervised multitask pre-training has been the critical method behind the\nrecent success of language models (LMs). However, supervised multitask learning\nstill holds significant promise, as scaling it in the post-training stage\ntrends towards better generalization. In this paper, we explore supervised\nmultitask pre-training by proposing Instruction Pre-Training, a framework that\nscalably augments massive raw corpora with instruction-response pairs to\npre-train LMs. The instruction-response pairs are generated by an efficient\ninstruction synthesizer built on open-source models. In our experiments, we\nsynthesize 200M instruction-response pairs covering 40+ task categories to\nverify the effectiveness of Instruction Pre-Training. In pre-training from\nscratch, Instruction Pre-Training not only consistently enhances pre-trained\nbase models but also benefits more from further instruction tuning. In\ncontinual pre-training, Instruction Pre-Training enables Llama3-8B to be\ncomparable to or even outperform Llama3-70B. Our model, code, and data are\navailable at https://github.com/microsoft/LMOps.",
      "upvotes": 85
    },
    {
      "title": "The Devil is in the Details: StyleFeatureEditor for Detail-Rich StyleGAN Inversion and High Quality Image Editing",
      "url": "https://huggingface.co/papers/2406.10601",
      "authors": [
        "Denis Bobkov",
        "Dmitry Vetrov"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10601.pdf",
      "abstract": "The task of manipulating real image attributes through StyleGAN inversion has\nbeen extensively researched. This process involves searching latent variables\nfrom a well-trained StyleGAN generator that can synthesize a real image,\nmodifying these latent variables, and then synthesizing an image with the\ndesired edits. A balance must be struck between the quality of the\nreconstruction and the ability to edit. Earlier studies utilized the\nlow-dimensional W-space for latent search, which facilitated effective editing\nbut struggled with reconstructing intricate details. More recent research has\nturned to the high-dimensional feature space F, which successfully inverses the\ninput image but loses much of the detail during editing. In this paper, we\nintroduce StyleFeatureEditor -- a novel method that enables editing in both\nw-latents and F-latents. This technique not only allows for the reconstruction\nof finer image details but also ensures their preservation during editing. We\nalso present a new training pipeline specifically designed to train our model\nto accurately edit F-latents. Our method is compared with state-of-the-art\nencoding approaches, demonstrating that our model excels in terms of\nreconstruction quality and is capable of editing even challenging out-of-domain\nexamples. Code is available at\nhttps://github.com/AIRI-Institute/StyleFeatureEditor.",
      "upvotes": 65
    },
    {
      "title": "HARE: HumAn pRiors, a key to small language model Efficiency",
      "url": "https://huggingface.co/papers/2406.11410",
      "authors": [
        "Bin jin",
        "Xuewen Shen",
        "Houqian Zhang",
        "Yongneng Jiang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11410.pdf",
      "abstract": "Human priors play a crucial role in efficiently utilizing data in deep\nlearning. However, with the development of large language models (LLMs), there\nis an increasing emphasis on scaling both model size and data volume, which\noften diminishes the importance of human priors in data construction.\nInfluenced by these trends, existing Small Language Models (SLMs) mainly rely\non web-scraped large-scale training data, neglecting the proper incorporation\nof human priors. This oversight limits the training efficiency of language\nmodels in resource-constrained settings. In this paper, we propose a principle\nto leverage human priors for data construction. This principle emphasizes\nachieving high-performance SLMs by training on a concise dataset that\naccommodates both semantic diversity and data quality consistency, while\navoiding benchmark data leakage. Following this principle, we train an SLM\nnamed HARE-1.1B. Extensive experiments on large-scale benchmark datasets\ndemonstrate that HARE-1.1B performs favorably against state-of-the-art SLMs,\nvalidating the effectiveness of the proposed principle. Additionally, this\nprovides new insights into efficient language model training in\nresource-constrained environments from the view of human priors.",
      "upvotes": 38
    },
    {
      "title": "Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs",
      "url": "https://huggingface.co/papers/2406.14544",
      "authors": [
        "Kai Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14544.pdf",
      "abstract": "Vision Language Models (VLMs) demonstrate remarkable proficiency in\naddressing a wide array of visual questions, which requires strong perception\nand reasoning faculties. Assessing these two competencies independently is\ncrucial for model refinement, despite the inherent difficulty due to the\nintertwined nature of seeing and reasoning in existing VLMs. To tackle this\nissue, we present Prism, an innovative framework designed to disentangle the\nperception and reasoning processes involved in visual question solving. Prism\ncomprises two distinct stages: a perception stage that utilizes a VLM to\nextract and articulate visual information in textual form, and a reasoning\nstage that formulates responses based on the extracted visual information using\na Large Language Model (LLM). This modular design enables the systematic\ncomparison and assessment of both proprietary and open-source VLM for their\nperception and reasoning strengths. Our analytical framework provides several\nvaluable insights, underscoring Prism's potential as a cost-effective solution\nfor vision-language tasks. By combining a streamlined VLM focused on perception\nwith a powerful LLM tailored for reasoning, Prism achieves superior results in\ngeneral vision-language tasks while substantially cutting down on training and\noperational expenses. Quantitative evaluations show that Prism, when configured\nwith a vanilla 2B LLaVA and freely accessible GPT-3.5, delivers performance on\npar with VLMs 10 times larger on the rigorous multimodal benchmark MMStar.\nThe project is released at: https://github.com/SparksJoe/Prism.",
      "upvotes": 34
    },
    {
      "title": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding",
      "url": "https://huggingface.co/papers/2406.14515",
      "authors": [
        "Kai Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14515.pdf",
      "abstract": "The advent of large vision-language models (LVLMs) has spurred research into\ntheir applications in multi-modal contexts, particularly in video\nunderstanding. Traditional VideoQA benchmarks, despite providing quantitative\nmetrics, often fail to encompass the full spectrum of video content and\ninadequately assess models' temporal comprehension. To address these\nlimitations, we introduce MMBench-Video, a quantitative benchmark designed to\nrigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video\nincorporates lengthy videos from YouTube and employs free-form questions,\nmirroring practical use cases. The benchmark is meticulously crafted to probe\nthe models' temporal reasoning skills, with all questions human-annotated\naccording to a carefully constructed ability taxonomy. We employ GPT-4 for\nautomated assessment, demonstrating superior accuracy and robustness over\nearlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted\ncomprehensive evaluations that include both proprietary and open-source LVLMs\nfor images and videos. MMBench-Video stands as a valuable resource for the\nresearch community, facilitating improved evaluation of LVLMs and catalyzing\nprogress in the field of video understanding. The evalutation code of\nMMBench-Video will be integrated into VLMEvalKit:\nhttps://github.com/open-compass/VLMEvalKit.",
      "upvotes": 32
    },
    {
      "title": "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch",
      "url": "https://huggingface.co/papers/2406.14563",
      "authors": [
        "Umberto Michieli",
        "Fabio Pizzati",
        "Philip Torr",
        "Adel Bibi",
        "Bernard Ghanem",
        "Mete Ozay"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14563.pdf",
      "abstract": "Merging Large Language Models (LLMs) is a cost-effective technique for\ncombining multiple expert LLMs into a single versatile model, retaining the\nexpertise of the original ones. However, current approaches often overlook the\nimportance of safety alignment during merging, leading to highly misaligned\nmodels. This work investigates the effects of model merging on alignment. We\nevaluate several popular model merging techniques, demonstrating that existing\nmethods do not only transfer domain expertise but also propagate misalignment.\nWe propose a simple two-step approach to address this problem: (i) generating\nsynthetic safety and domain-specific data, and (ii) incorporating these\ngenerated data into the optimization process of existing data-aware model\nmerging techniques. This allows us to treat alignment as a skill that can be\nmaximized in the resulting merged LLM. Our experiments illustrate the\neffectiveness of integrating alignment-related data during merging, resulting\nin models that excel in both domain expertise and alignment.",
      "upvotes": 29
    },
    {
      "title": "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities",
      "url": "https://huggingface.co/papers/2406.14562",
      "authors": [
        "Richard Zemel"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14562.pdf",
      "abstract": "When presented with questions involving visual thinking, humans naturally\nswitch reasoning modalities, often forming mental images or drawing visual\naids. Large language models have shown promising results in arithmetic and\nsymbolic reasoning by expressing intermediate reasoning in text as a chain of\nthought, yet struggle to extend this capability to answer text queries that are\neasily solved by visual reasoning, even with extensive multimodal pretraining.\nWe introduce a simple method, whiteboard-of-thought prompting, to unlock the\nvisual reasoning capabilities of multimodal large language models across\nmodalities. Whiteboard-of-thought prompting provides multimodal large language\nmodels with a metaphorical `whiteboard' to draw out reasoning steps as images,\nthen returns these images back to the model for further processing. We find\nthis can be accomplished with no demonstrations or specialized modules, instead\nleveraging models' existing ability to write code with libraries such as\nMatplotlib and Turtle. This simple approach shows state-of-the-art results on\nfour difficult natural language tasks that involve visual and spatial\nreasoning. We identify multiple settings where GPT-4o using chain-of-thought\nfails dramatically, including more than one where it achieves 0% accuracy,\nwhile whiteboard-of-thought enables up to 92% accuracy in these same\nsettings. We present a detailed exploration of where the technique succeeds as\nwell as its sources of error.",
      "upvotes": 27
    },
    {
      "title": "Invertible Consistency Distillation for Text-Guided Image Editing in Around 7 Steps",
      "url": "https://huggingface.co/papers/2406.14539",
      "authors": [
        "Nikita Starodubcev",
        "Artem Babenko"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14539.pdf",
      "abstract": "Diffusion distillation represents a highly promising direction for achieving\nfaithful text-to-image generation in a few sampling steps. However, despite\nrecent successes, existing distilled models still do not provide the full\nspectrum of diffusion abilities, such as real image inversion, which enables\nmany precise image manipulation methods. This work aims to enrich distilled\ntext-to-image diffusion models with the ability to effectively encode real\nimages into their latent space. To this end, we introduce invertible\nConsistency Distillation (iCD), a generalized consistency distillation\nframework that facilitates both high-quality image synthesis and accurate image\nencoding in only 3-4 inference steps. Though the inversion problem for\ntext-to-image diffusion models gets exacerbated by high classifier-free\nguidance scales, we notice that dynamic guidance significantly reduces\nreconstruction errors without noticeable degradation in generation performance.\nAs a result, we demonstrate that iCD equipped with dynamic guidance may serve\nas a highly effective tool for zero-shot text-guided image editing, competing\nwith more expensive state-of-the-art alternatives.",
      "upvotes": 26
    },
    {
      "title": "GLiNER multi-task: Generalist Lightweight Model for Various Information Extraction Tasks",
      "url": "https://huggingface.co/papers/2406.12925",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.12925.pdf",
      "abstract": "Information extraction tasks require both accurate, efficient, and\ngeneralisable models. Classical supervised deep learning approaches can achieve\nthe required performance, but they need large datasets and are limited in their\nability to adapt to different tasks. On the other hand, large language models\n(LLMs) demonstrate good generalization, meaning that they can adapt to many\ndifferent tasks based on user requests. However, LLMs are computationally\nexpensive and tend to fail to generate structured outputs. In this article, we\nwill introduce a new kind of GLiNER model that can be used for various\ninformation extraction tasks while being a small encoder model. Our model\nachieved SoTA performance on zero-shot NER benchmarks and leading performance\non question-answering, summarization and relation extraction tasks.\nAdditionally, in this article, we will cover experimental results on\nself-learning approaches for named entity recognition using GLiNER models.",
      "upvotes": 22
    },
    {
      "title": "PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents",
      "url": "https://huggingface.co/papers/2406.13923",
      "authors": [
        "Chunyang Jiang",
        "Yubo Wang",
        "Bei Chen",
        "Qunshu Lin",
        "Wenhu Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13923.pdf",
      "abstract": "Recent advancements in Large Multimodal Models (LMMs) have leveraged\nextensive multimodal datasets to enhance capabilities in complex\nknowledge-driven tasks. However, persistent challenges in perceptual and\nreasoning errors limit their efficacy, particularly in interpreting intricate\nvisual data and deducing multimodal relationships. Addressing these issues, we\nintroduce a novel dataset format, PIN (Paired and INterleaved multimodal\ndocuments), designed to significantly improve both the depth and breadth of\nmultimodal training. The PIN format is built on three foundational principles:\nknowledge intensity, scalability, and support for diverse training modalities.\nThis innovative format combines markdown files and comprehensive images to\nenrich training data with a dense knowledge structure and versatile training\nstrategies. We present PIN-14M, an open-source dataset comprising 14 million\nsamples derived from a diverse range of Chinese and English sources, tailored\nto include complex web and scientific content. This dataset is constructed\nmeticulously to ensure data quality and ethical integrity, aiming to facilitate\nadvanced training strategies and improve model robustness against common\nmultimodal training pitfalls. Our initial results, forming the basis of this\ntechnical report, suggest significant potential for the PIN format in refining\nLMM performance, with plans for future expansions and detailed evaluations of\nits impact on model capabilities.",
      "upvotes": 21
    },
    {
      "title": "DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning",
      "url": "https://huggingface.co/papers/2406.11896",
      "authors": [
        "Hao Bai",
        "Yifei Zhou",
        "Mert Cemri",
        "Alane Suhr"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11896.pdf",
      "abstract": "Training corpuses for vision language models (VLMs) typically lack sufficient\namounts of decision-centric data. This renders off-the-shelf VLMs sub-optimal\nfor decision-making tasks such as in-the-wild device control through graphical\nuser interfaces (GUIs). While training with static demonstrations has shown\nsome promise, we show that such methods fall short for controlling real GUIs\ndue to their failure to deal with real-world stochasticity and non-stationarity\nnot captured in static observational data. This paper introduces a novel\nautonomous RL approach, called DigiRL, for training in-the-wild device control\nagents through fine-tuning a pre-trained VLM in two stages: offline RL to\ninitialize the model, followed by offline-to-online RL. To do this, we build a\nscalable and parallelizable Android learning environment equipped with a\nVLM-based evaluator and develop a simple yet effective RL approach for learning\nin this domain. Our approach runs advantage-weighted RL with advantage\nestimators enhanced to account for stochasticity along with an automatic\ncurriculum for deriving maximal learning signal. We demonstrate the\neffectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our\n1.3B VLM trained with RL achieves a 49.5% absolute improvement -- from 17.7 to\n67.2% success rate -- over supervised fine-tuning with static human\ndemonstration data. These results significantly surpass not only the prior best\nagents, including AppAgent with GPT-4V (8.3% success rate) and the 17B CogAgent\ntrained with AitW data (38.5%), but also the prior best autonomous RL approach\nbased on filtered behavior cloning (57.8%), thereby establishing a new\nstate-of-the-art for digital agents for in-the-wild device control.",
      "upvotes": 18
    },
    {
      "title": "Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models",
      "url": "https://huggingface.co/papers/2406.13542",
      "authors": [
        "Chang Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13542.pdf",
      "abstract": "One core capability of large language models (LLMs) is to follow natural\nlanguage instructions. However, the issue of automatically constructing\nhigh-quality training data to enhance the complex instruction-following\nabilities of LLMs without manual annotation remains unresolved. In this paper,\nwe introduce AutoIF, the first scalable and reliable method for automatically\ngenerating instruction-following training data. AutoIF transforms the\nvalidation of instruction-following data quality into code verification,\nrequiring LLMs to generate instructions, the corresponding code to check the\ncorrectness of the instruction responses, and unit test samples to verify the\ncode's correctness. Then, execution feedback-based rejection sampling can\ngenerate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from\nHuman Feedback (RLHF) training. AutoIF achieves significant improvements across\nthree training algorithms, SFT, Offline DPO, and Online DPO, when applied to\nthe top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and\nstrong-to-weak distillation settings. Our code is publicly available at\nhttps://github.com/QwenLM/AutoIF.",
      "upvotes": 16
    },
    {
      "title": "LiveMind: Low-latency Large Language Models with Simultaneous Inference",
      "url": "https://huggingface.co/papers/2406.14319",
      "authors": [
        "Grace Li Zhang",
        "Xunzhao Yin",
        "Cheng Zhuo",
        "Ulf Schlichtmann",
        "Bing Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14319.pdf",
      "abstract": "In this paper, we introduce a novel low-latency inference framework for large\nlanguage models (LLMs) inference which enables LLMs to perform inferences with\nincomplete prompts. By reallocating computational processes to prompt input\nphase, we achieve a substantial reduction in latency, thereby significantly\nenhancing the interactive experience for users of LLMs. The framework adeptly\nmanages the visibility of the streaming prompt to the model, allowing it to\ninfer from incomplete prompts or await additional prompts. Compared with\ntraditional inference methods that utilize complete prompts, our approach\ndemonstrates an average reduction of 59% in response latency on the MMLU-Pro\ndataset, while maintaining comparable accuracy. Additionally, our framework\nfacilitates collaborative inference and output across different models. By\nemploying an LLM for inference and a small language model (SLM) for output, we\nachieve an average 68% reduction in response latency, alongside a 5.5%\nimprovement in accuracy on the MMLU-Pro dataset compared with the SLM baseline.\nFor long prompts exceeding 20 sentences, the response latency can be reduced by\nup to 93%.",
      "upvotes": 14
    },
    {
      "title": "Improving Visual Commonsense in Language Models via Multiple Image Generation",
      "url": "https://huggingface.co/papers/2406.13621",
      "authors": [
        "Idan Schwartz"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13621.pdf",
      "abstract": "Commonsense reasoning is fundamentally based on multimodal knowledge.\nHowever, existing large language models (LLMs) are primarily trained using\ntextual data only, limiting their ability to incorporate essential visual\ninformation. In contrast, Visual Language Models, which excel at\nvisually-oriented tasks, often fail at non-visual tasks such as basic\ncommonsense reasoning. This divergence highlights a critical challenge - the\nintegration of robust visual understanding with foundational text-based\nlanguage reasoning. To this end, we introduce a method aimed at enhancing LLMs'\nvisual commonsense. Specifically, our method generates multiple images based on\nthe input text prompt and integrates these into the model's decision-making\nprocess by mixing their prediction probabilities. To facilitate multimodal\ngrounded language modeling, we employ a late-fusion layer that combines the\nprojected visual features with the output of a pre-trained LLM conditioned on\ntext only. This late-fusion layer enables predictions based on comprehensive\nimage-text knowledge as well as text only when this is required. We evaluate\nour approach using several visual commonsense reasoning tasks together with\ntraditional NLP tasks, including common sense reasoning and reading\ncomprehension. Our experimental results demonstrate significant superiority\nover existing baselines. When applied to recent state-of-the-art LLMs (e.g.,\nLlama3), we observe improvements not only in visual common sense but also in\ntraditional NLP benchmarks. Code and models are available under\nhttps://github.com/guyyariv/vLMIG.",
      "upvotes": 13
    },
    {
      "title": "Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level",
      "url": "https://huggingface.co/papers/2406.11817",
      "authors": [
        "Chao Yang",
        "Wanli Ouyang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11817.pdf",
      "abstract": "Direct Preference Optimization (DPO), a standard method for aligning language\nmodels with human preferences, is traditionally applied to offline preferences.\nRecent studies show that DPO benefits from iterative training with online\npreferences labeled by a trained reward model. In this work, we identify a\npitfall of vanilla iterative DPO - improved response quality can lead to\nincreased verbosity. To address this, we introduce iterative length-regularized\nDPO (iLR-DPO) to penalize response length. Our empirical results show that\niLR-DPO can enhance a 7B model to perform on par with GPT-4 without increasing\nverbosity. Specifically, our 7B model achieves a 50.5% length-controlled win\nrate against GPT-4 Preview on AlpacaEval 2.0, and excels across\nstandard benchmarks including MT-Bench, Arena-Hard and OpenLLM Leaderboard.\nThese results demonstrate the effectiveness of iterative DPO in aligning\nlanguage models with human feedback.",
      "upvotes": 13
    },
    {
      "title": "REPOEXEC: Evaluate Code Generation with a Repository-Level Executable Benchmark",
      "url": "https://huggingface.co/papers/2406.11927",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.11927.pdf",
      "abstract": "The ability of CodeLLMs to generate executable and functionally correct code\nat the repository-level scale remains largely unexplored. We introduce\nRepoExec, a novel benchmark for evaluating code generation at the\nrepository-level scale. RepoExec focuses on three main aspects: executability,\nfunctional correctness through automated test case generation with high\ncoverage rate, and carefully crafted cross-file contexts to accurately generate\ncode. Our work explores a controlled scenario where developers specify\nnecessary code dependencies, challenging the model to integrate these\naccurately. Experiments show that while pretrained LLMs outperform\ninstruction-tuned models in correctness, the latter excel in utilizing provided\ndependencies and demonstrating debugging capabilities. We also introduce a new\ninstruction-tuned dataset that focuses on code dependencies and demonstrate\nthat CodeLLMs fine-tuned on our dataset have a better capability to leverage\nthese dependencies effectively. RepoExec aims to provide a comprehensive\nevaluation of code functionality and alignment with developer intent, paving\nthe way for more reliable and applicable CodeLLMs in real-world scenarios. The\ndataset and source code can be found\nat~https://github.com/FSoft-AI4Code/RepoExec.",
      "upvotes": 11
    },
    {
      "title": "ExVideo: Extending Video Diffusion Models via Parameter-Efficient Post-Tuning",
      "url": "https://huggingface.co/papers/2406.14130",
      "authors": [
        "Yaliang Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14130.pdf",
      "abstract": "Recently, advancements in video synthesis have attracted significant\nattention. Video synthesis models such as AnimateDiff and Stable Video\nDiffusion have demonstrated the practical applicability of diffusion models in\ncreating dynamic visual content. The emergence of SORA has further spotlighted\nthe potential of video generation technologies. Nonetheless, the extension of\nvideo lengths has been constrained by the limitations in computational\nresources. Most existing video synthesis models can only generate short video\nclips. In this paper, we propose a novel post-tuning methodology for video\nsynthesis models, called ExVideo. This approach is designed to enhance the\ncapability of current video synthesis models, allowing them to produce content\nover extended temporal durations while incurring lower training expenditures.\nIn particular, we design extension strategies across common temporal model\narchitectures respectively, including 3D convolution, temporal attention, and\npositional embedding. To evaluate the efficacy of our proposed post-tuning\napproach, we conduct extension training on the Stable Video Diffusion model.\nOur approach augments the model's capacity to generate up to 5times its\noriginal number of frames, requiring only 1.5k GPU hours of training on a\ndataset comprising 40k videos. Importantly, the substantial increase in video\nlength doesn't compromise the model's innate generalization capabilities, and\nthe model showcases its advantages in generating videos of diverse styles and\nresolutions. We will release the source code and the enhanced model publicly.",
      "upvotes": 10
    },
    {
      "title": "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation",
      "url": "https://huggingface.co/papers/2406.13663",
      "authors": [
        "Arianna Bisazza"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13663.pdf",
      "abstract": "Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.",
      "upvotes": 7
    },
    {
      "title": "$τ$-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains",
      "url": "https://huggingface.co/papers/2406.12045",
      "authors": [
        "Shunyu Yao",
        "Pedram Razavi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12045.pdf",
      "abstract": "Existing benchmarks do not test language agents on their interaction with\nhuman users or ability to follow domain-specific rules, both of which are vital\nfor deploying them in real world applications. We propose tau-bench, a\nbenchmark emulating dynamic conversations between a user (simulated by language\nmodels) and a language agent provided with domain-specific API tools and policy\nguidelines. We employ an efficient and faithful evaluation process that\ncompares the database state at the end of a conversation with the annotated\ngoal state. We also propose a new metric (pass^k) to evaluate the reliability\nof agent behavior over multiple trials. Our experiments show that even\nstate-of-the-art function calling agents (like gpt-4o) succeed on <50% of the\ntasks, and are quite inconsistent (pass^8 <25% in retail). Our findings point\nto the need for methods that can improve the ability of agents to act\nconsistently and follow rules reliably.",
      "upvotes": 6
    },
    {
      "title": "A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models",
      "url": "https://huggingface.co/papers/2406.11289",
      "authors": [
        "Philip S. Yu",
        "Jiawei Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11289.pdf",
      "abstract": "Text summarization research has undergone several significant transformations\nwith the advent of deep neural networks, pre-trained language models (PLMs),\nand recent large language models (LLMs). This survey thus provides a\ncomprehensive review of the research progress and evolution in text\nsummarization through the lens of these paradigm shifts. It is organized into\ntwo main parts: (1) a detailed overview of datasets, evaluation metrics, and\nsummarization methods before the LLM era, encompassing traditional statistical\nmethods, deep learning approaches, and PLM fine-tuning techniques, and (2) the\nfirst detailed examination of recent advancements in benchmarking, modeling,\nand evaluating summarization in the LLM era. By synthesizing existing\nliterature and presenting a cohesive overview, this survey also discusses\nresearch trends, open challenges, and proposes promising research directions in\nsummarization, aiming to guide researchers through the evolving landscape of\nsummarization research.",
      "upvotes": 5
    },
    {
      "title": "StableSemantics: A Synthetic Language-Vision Dataset of Semantic Representations in Naturalistic Images",
      "url": "https://huggingface.co/papers/2406.13735",
      "authors": [
        "Rushikesh Zawar",
        "Shaurya Dewan",
        "Margaret M. Henderson",
        "Leila Wehbe"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13735.pdf",
      "abstract": "Understanding the semantics of visual scenes is a fundamental challenge in\nComputer Vision. A key aspect of this challenge is that objects sharing similar\nsemantic meanings or functions can exhibit striking visual differences, making\naccurate identification and categorization difficult. Recent advancements in\ntext-to-image frameworks have led to models that implicitly capture natural\nscene statistics. These frameworks account for the visual variability of\nobjects, as well as complex object co-occurrences and sources of noise such as\ndiverse lighting conditions. By leveraging large-scale datasets and\ncross-attention conditioning, these models generate detailed and contextually\nrich scene representations. This capability opens new avenues for improving\nobject recognition and scene understanding in varied and challenging\nenvironments. Our work presents StableSemantics, a dataset comprising 224\nthousand human-curated prompts, processed natural language captions, over 2\nmillion synthetic images, and 10 million attention maps corresponding to\nindividual noun chunks. We explicitly leverage human-generated prompts that\ncorrespond to visually interesting stable diffusion generations, provide 10\ngenerations per phrase, and extract cross-attention maps for each image. We\nexplore the semantic distribution of generated images, examine the distribution\nof objects within images, and benchmark captioning and open vocabulary\nsegmentation methods on our data. To the best of our knowledge, we are the\nfirst to release a diffusion dataset with semantic attributions. We expect our\nproposed dataset to catalyze advances in visual semantic understanding and\nprovide a foundation for developing more sophisticated and effective visual\nmodels. Website: https://stablesemantics.github.io/StableSemantics",
      "upvotes": 5
    },
    {
      "title": "From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP",
      "url": "https://huggingface.co/papers/2406.12618",
      "authors": [
        "Tomás Vergara-Browne",
        "Dietrich Klakow",
        "Mor Geva"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12618.pdf",
      "abstract": "Interpretability and analysis (IA) research is a growing subfield within NLP\nwith the goal of developing a deeper understanding of the behavior or inner\nworkings of NLP systems and methods. Despite growing interest in the subfield,\na commonly voiced criticism is that it lacks actionable insights and therefore\nhas little impact on NLP. In this paper, we seek to quantify the impact of IA\nresearch on the broader field of NLP. We approach this with a mixed-methods\nanalysis of: (1) a citation graph of 185K+ papers built from all papers\npublished at ACL and EMNLP conferences from 2018 to 2023, and (2) a survey of\n138 members of the NLP community. Our quantitative results show that IA work is\nwell-cited outside of IA, and central in the NLP citation graph. Through\nqualitative analysis of survey responses and manual annotation of 556 papers,\nwe find that NLP researchers build on findings from IA work and perceive it is\nimportant for progress in NLP, multiple subfields, and rely on its findings and\nterminology for their own work. Many novel methods are proposed based on IA\nfindings and highly influenced by them, but highly influential non-IA work\ncites IA findings without being driven by them. We end by summarizing what is\nmissing in IA work today and provide a call to action, to pave the way for a\nmore impactful future of IA research.",
      "upvotes": 5
    },
    {
      "title": "Sampling 3D Gaussian Scenes in Seconds with Latent Diffusion Models",
      "url": "https://huggingface.co/papers/2406.13099",
      "authors": [
        "Melonie de Almeida",
        "Daniela Ivanova",
        "Titas Anciukevičius"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13099.pdf",
      "abstract": "We present a latent diffusion model over 3D scenes, that can be trained using\nonly 2D image data. To achieve this, we first design an autoencoder that maps\nmulti-view images to 3D Gaussian splats, and simultaneously builds a compressed\nlatent representation of these splats. Then, we train a multi-view diffusion\nmodel over the latent space to learn an efficient generative model. This\npipeline does not require object masks nor depths, and is suitable for complex\nscenes with arbitrary camera positions. We conduct careful experiments on two\nlarge-scale datasets of complex real-world scenes -- MVImgNet and\nRealEstate10K. We show that our approach enables generating 3D scenes in as\nlittle as 0.2 seconds, either from scratch, from a single input view, or from\nsparse input views. It produces diverse and high-quality results while running\nan order of magnitude faster than non-latent diffusion models and earlier\nNeRF-based generative models",
      "upvotes": 4
    }
  ]
}