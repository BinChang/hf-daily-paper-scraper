{
  "date": "2024-03-19",
  "papers": [
    {
      "title": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation",
      "url": "https://huggingface.co/papers/2403.12015",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2403.12015.pdf",
      "abstract": "Diffusion models are the main driver of progress in image and video\nsynthesis, but suffer from slow inference speed. Distillation methods, like the\nrecently introduced adversarial diffusion distillation (ADD) aim to shift the\nmodel from many-shot to single-step inference, albeit at the cost of expensive\nand difficult optimization due to its reliance on a fixed pretrained DINOv2\ndiscriminator. We introduce Latent Adversarial Diffusion Distillation (LADD), a\nnovel distillation approach overcoming the limitations of ADD. In contrast to\npixel-based ADD, LADD utilizes generative features from pretrained latent\ndiffusion models. This approach simplifies training and enhances performance,\nenabling high-resolution multi-aspect ratio image synthesis. We apply LADD to\nStable Diffusion 3 (8B) to obtain SD3-Turbo, a fast model that matches the\nperformance of state-of-the-art text-to-image generators using only four\nunguided sampling steps. Moreover, we systematically investigate its scaling\nbehavior and demonstrate LADD's effectiveness in various applications such as\nimage editing and inpainting.",
      "upvotes": 64
    },
    {
      "title": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback",
      "url": "https://huggingface.co/papers/2403.10704",
      "authors": [
        "Hakim Sidahmed",
        "Zhang Chen",
        "Jarvis Jin",
        "Roman Komarytsia",
        "Christiane Ahlheim",
        "Yonghao Zhu",
        "Bill Byrne",
        "Wei Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.10704.pdf",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong\nmethod to align Pretrained Large Language Models (LLMs) with human preferences.\nBut training models with RLHF is computationally expensive, and an overall\ncomplex process. In this work, we study RLHF where the underlying models are\ntrained using the parameter efficient method of Low-Rank Adaptation (LoRA)\nintroduced by Hu et al. [2021]. We investigate the setup of \"Parameter\nEfficient Reinforcement Learning\" (PERL), in which we perform reward model\ntraining and reinforcement learning using LoRA. We compare PERL to conventional\nfine-tuning (full-tuning) across various configurations for 7 benchmarks,\nincluding 2 novel datasets, of reward modeling and reinforcement learning. We\nfind that PERL performs on par with the conventional RLHF setting, while\ntraining faster, and with less memory. This enables the high performance of\nRLHF, while reducing the computational burden that limits its adoption as an\nalignment technique for Large Language Models. We also release 2 novel thumbs\nup/down preference datasets: \"Taskmaster Coffee\", and \"Taskmaster Ticketing\" to\npromote research around RLHF.",
      "upvotes": 57
    },
    {
      "title": "Larimar: Large Language Models with Episodic Memory Control",
      "url": "https://huggingface.co/papers/2403.11901",
      "authors": [
        "Igor Melnyk",
        "Sihui Dai",
        "Aurélie Lozano",
        "Georgios Kollias",
        "Jiří",
        "Navrátil"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.11901.pdf",
      "abstract": "Efficient and accurate updating of knowledge stored in Large Language Models\n(LLMs) is one of the most pressing research challenges today. This paper\npresents Larimar - a novel, brain-inspired architecture for enhancing LLMs with\na distributed episodic memory. Larimar's memory allows for dynamic, one-shot\nupdates of knowledge without the need for computationally expensive re-training\nor fine-tuning. Experimental results on multiple fact editing benchmarks\ndemonstrate that Larimar attains accuracy comparable to most competitive\nbaselines, even in the challenging sequential editing setup, but also excels in\nspeed - yielding speed-ups of 4-10x depending on the base LLM - as well as\nflexibility due to the proposed architecture being simple, LLM-agnostic, and\nhence general. We further provide mechanisms for selective fact forgetting and\ninput context length generalization with Larimar and show their effectiveness.",
      "upvotes": 32
    },
    {
      "title": "SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion",
      "url": "https://huggingface.co/papers/2403.12008",
      "authors": [
        "David Pankratz",
        "Dmitry Tochilkin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.12008.pdf",
      "abstract": "We present Stable Video 3D (SV3D) -- a latent video diffusion model for\nhigh-resolution, image-to-multi-view generation of orbital videos around a 3D\nobject. Recent work on 3D generation propose techniques to adapt 2D generative\nmodels for novel view synthesis (NVS) and 3D optimization. However, these\nmethods have several disadvantages due to either limited views or inconsistent\nNVS, thereby affecting the performance of 3D object generation. In this work,\nwe propose SV3D that adapts image-to-video diffusion model for novel multi-view\nsynthesis and 3D generation, thereby leveraging the generalization and\nmulti-view consistency of the video models, while further adding explicit\ncamera control for NVS. We also propose improved 3D optimization techniques to\nuse SV3D and its NVS outputs for image-to-3D generation. Extensive experimental\nresults on multiple datasets with 2D and 3D metrics as well as user study\ndemonstrate SV3D's state-of-the-art performance on NVS as well as 3D\nreconstruction compared to prior works.",
      "upvotes": 19
    },
    {
      "title": "Infinite-ID: Identity-preserved Personalization via ID-semantics Decoupling Paradigm",
      "url": "https://huggingface.co/papers/2403.11781",
      "authors": [
        "Yi Wu",
        "Ziqiang Li",
        "Chaoyue Wang",
        "Bin Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.11781.pdf",
      "abstract": "Drawing on recent advancements in diffusion models for text-to-image\ngeneration, identity-preserved personalization has made significant progress in\naccurately capturing specific identities with just a single reference image.\nHowever, existing methods primarily integrate reference images within the text\nembedding space, leading to a complex entanglement of image and text\ninformation, which poses challenges for preserving both identity fidelity and\nsemantic consistency. To tackle this challenge, we propose Infinite-ID, an\nID-semantics decoupling paradigm for identity-preserved personalization.\nSpecifically, we introduce identity-enhanced training, incorporating an\nadditional image cross-attention module to capture sufficient ID information\nwhile deactivating the original text cross-attention module of the diffusion\nmodel. This ensures that the image stream faithfully represents the identity\nprovided by the reference image while mitigating interference from textual\ninput. Additionally, we introduce a feature interaction mechanism that combines\na mixed attention module with an AdaIN-mean operation to seamlessly merge the\ntwo streams. This mechanism not only enhances the fidelity of identity and\nsemantic consistency but also enables convenient control over the styles of the\ngenerated images. Extensive experimental results on both raw photo generation\nand style image generation demonstrate the superior performance of our proposed\nmethod.",
      "upvotes": 17
    },
    {
      "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
      "url": "https://huggingface.co/papers/2403.11703",
      "authors": [
        "Yuan Yao",
        "Junbo Cui",
        "Maosong Sun",
        "Gao Huang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.11703.pdf",
      "abstract": "Visual encoding constitutes the basis of large multimodal models (LMMs) in\nunderstanding the visual world. Conventional LMMs process images in fixed sizes\nand limited resolutions, while recent explorations in this direction are\nlimited in adaptivity, efficiency, and even correctness. In this work, we first\ntake GPT-4V and LLaVA-1.5 as representative examples and expose systematic\nflaws rooted in their visual encoding strategy. To address the challenges, we\npresent LLaVA-UHD, a large multimodal model that can efficiently perceive\nimages in any aspect ratio and high resolution. LLaVA-UHD includes three key\ncomponents: (1) An image modularization strategy that divides native-resolution\nimages into smaller variable-sized slices for efficient and extensible\nencoding, (2) a compression module that further condenses image tokens from\nvisual encoders, and (3) a spatial schema to organize slice tokens for LLMs.\nComprehensive experiments show that LLaVA-UHD outperforms established LMMs\ntrained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our\nmodel built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)\nresolution images using only 94% inference computation, and achieves 6.4\naccuracy improvement on TextVQA. Moreover, the model can be efficiently trained\nin academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of\nLLaVA-1.5). We make the data and code publicly available at\nhttps://github.com/thunlp/LLaVA-UHD.",
      "upvotes": 16
    },
    {
      "title": "LightIt: Illumination Modeling and Control for Diffusion Models",
      "url": "https://huggingface.co/papers/2403.10615",
      "authors": [
        "Kalyan Sunkavalli",
        "Matthias Nießner",
        "Yannick Hold-Geoffroy"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.10615.pdf",
      "abstract": "We introduce LightIt, a method for explicit illumination control for image\ngeneration. Recent generative methods lack lighting control, which is crucial\nto numerous artistic aspects of image generation such as setting the overall\nmood or cinematic appearance. To overcome these limitations, we propose to\ncondition the generation on shading and normal maps. We model the lighting with\nsingle bounce shading, which includes cast shadows. We first train a shading\nestimation module to generate a dataset of real-world images and shading pairs.\nThen, we train a control network using the estimated shading and normals as\ninput. Our method demonstrates high-quality image generation and lighting\ncontrol in numerous scenes. Additionally, we use our generated dataset to train\nan identity-preserving relighting model, conditioned on an image and a target\nshading. Our method is the first that enables the generation of images with\ncontrollable, consistent lighting and performs on par with specialized\nrelighting state-of-the-art methods.",
      "upvotes": 16
    },
    {
      "title": "MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data",
      "url": "https://huggingface.co/papers/2403.11207",
      "authors": [
        "Tong Chen",
        "Charan Santhirasegaran",
        "Jonathan Xu",
        "Thomas Naselaris",
        "Kenneth A. Norman"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.11207.pdf",
      "abstract": "Reconstructions of visual perception from brain activity have improved\ntremendously, but the practical utility of such methods has been limited. This\nis because such models are trained independently per subject where each subject\nrequires dozens of hours of expensive fMRI training data to attain high-quality\nresults. The present work showcases high-quality reconstructions using only 1\nhour of fMRI training data. We pretrain our model across 7 subjects and then\nfine-tune on minimal data from a new subject. Our novel functional alignment\nprocedure linearly maps all brain data to a shared-subject latent space,\nfollowed by a shared non-linear mapping to CLIP image space. We then map from\nCLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP\nlatents as inputs instead of text. This approach improves out-of-subject\ngeneralization with limited training data and also attains state-of-the-art\nimage retrieval and reconstruction metrics compared to single-subject\napproaches. MindEye2 demonstrates how accurate reconstructions of perception\nare possible from a single visit to the MRI facility. All code is available on\nGitHub.",
      "upvotes": 14
    },
    {
      "title": "Generic 3D Diffusion Adapter Using Controlled Multi-View Editing",
      "url": "https://huggingface.co/papers/2403.12032",
      "authors": [
        "Bokui Shen",
        "Gordon Wetzstein",
        "Hao Su",
        "Leonidas Guibas"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.12032.pdf",
      "abstract": "Open-domain 3D object synthesis has been lagging behind image synthesis due\nto limited data and higher computational complexity. To bridge this gap, recent\nworks have investigated multi-view diffusion but often fall short in either 3D\nconsistency, visual quality, or efficiency. This paper proposes MVEdit, which\nfunctions as a 3D counterpart of SDEdit, employing ancestral sampling to\njointly denoise multi-view images and output high-quality textured meshes.\nBuilt on off-the-shelf 2D diffusion models, MVEdit achieves 3D consistency\nthrough a training-free 3D Adapter, which lifts the 2D views of the last\ntimestep into a coherent 3D representation, then conditions the 2D views of the\nnext timestep using rendered views, without uncompromising visual quality. With\nan inference time of only 2-5 minutes, this framework achieves better trade-off\nbetween quality and speed than score distillation. MVEdit is highly versatile\nand extendable, with a wide range of applications including text/image-to-3D\ngeneration, 3D-to-3D editing, and high-quality texture synthesis. In\nparticular, evaluations demonstrate state-of-the-art performance in both\nimage-to-3D and text-guided texture generation tasks. Additionally, we\nintroduce a method for fine-tuning 2D latent diffusion models on small 3D\ndatasets with limited resources, enabling fast low-resolution text-to-3D\ninitialization.",
      "upvotes": 14
    },
    {
      "title": "DiPaCo: Distributed Path Composition",
      "url": "https://huggingface.co/papers/2403.10616",
      "authors": [
        "Qixuan Feng",
        "Andrei A. Rusu",
        "Adhiguna Kuncoro",
        "Yani Donchev",
        "Rachita Chhaparia",
        "Ionel Gog",
        "Marc'Aurelio Ranzato",
        "Arthur Szlam"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.10616.pdf",
      "abstract": "Progress in machine learning (ML) has been fueled by scaling neural network\nmodels. This scaling has been enabled by ever more heroic feats of engineering,\nnecessary for accommodating ML approaches that require high bandwidth\ncommunication between devices working in parallel. In this work, we propose a\nco-designed modular architecture and training approach for ML models, dubbed\nDIstributed PAth COmposition (DiPaCo). During training, DiPaCo distributes\ncomputation by paths through a set of shared modules. Together with a Local-SGD\ninspired optimization (DiLoCo) that keeps modules in sync with drastically\nreduced communication, Our approach facilitates training across poorly\nconnected and heterogeneous workers, with a design that ensures robustness to\nworker failures and preemptions. At inference time, only a single path needs to\nbe executed for each input, without the need for any model compression. We\nconsider this approach as a first prototype towards a new paradigm of\nlarge-scale learning, one that is less synchronous and more modular. Our\nexperiments on the widely used C4 benchmark show that, for the same amount of\ntraining steps but less wall-clock time, DiPaCo exceeds the performance of a 1\nbillion-parameter dense transformer language model by choosing one of 256\npossible paths, each with a size of 150 million parameters.",
      "upvotes": 12
    },
    {
      "title": "VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding",
      "url": "https://huggingface.co/papers/2403.11481",
      "authors": [
        "Yue Fan",
        "Rujie Wu",
        "Jiaqi Li",
        "Zhi Gao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.11481.pdf",
      "abstract": "We explore how reconciling several foundation models (large language models\nand vision-language models) with a novel unified memory mechanism could tackle\nthe challenging video understanding problem, especially capturing the long-term\ntemporal relations in lengthy videos. In particular, the proposed multimodal\nagent VideoAgent: 1) constructs a structured memory to store both the generic\ntemporal event descriptions and object-centric tracking states of the video; 2)\ngiven an input task query, it employs tools including video segment\nlocalization and object memory querying along with other visual foundation\nmodels to interactively solve the task, utilizing the zero-shot tool-use\nability of LLMs. VideoAgent demonstrates impressive performances on several\nlong-horizon video understanding benchmarks, an average increase of 6.6% on\nNExT-QA and 26.0% on EgoSchema over baselines, closing the gap between\nopen-sourced models and private counterparts including Gemini 1.5 Pro.",
      "upvotes": 12
    },
    {
      "title": "LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation",
      "url": "https://huggingface.co/papers/2403.12019",
      "authors": [
        "Xuyi Meng",
        "Bo Dai",
        "Chen Change Loy"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.12019.pdf",
      "abstract": "The field of neural rendering has witnessed significant progress with\nadvancements in generative models and differentiable rendering techniques.\nThough 2D diffusion has achieved success, a unified 3D diffusion pipeline\nremains unsettled. This paper introduces a novel framework called LN3Diff to\naddress this gap and enable fast, high-quality, and generic conditional 3D\ngeneration. Our approach harnesses a 3D-aware architecture and variational\nautoencoder (VAE) to encode the input image into a structured, compact, and 3D\nlatent space. The latent is decoded by a transformer-based decoder into a\nhigh-capacity 3D neural field. Through training a diffusion model on this\n3D-aware latent space, our method achieves state-of-the-art performance on\nShapeNet for 3D generation and demonstrates superior performance in monocular\n3D reconstruction and conditional 3D generation across various datasets.\nMoreover, it surpasses existing 3D diffusion methods in terms of inference\nspeed, requiring no per-instance optimization. Our proposed LN3Diff presents a\nsignificant advancement in 3D generative modeling and holds promise for various\napplications in 3D vision and graphics tasks.",
      "upvotes": 8
    },
    {
      "title": "VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models",
      "url": "https://huggingface.co/papers/2403.12034",
      "authors": [
        "Philip Torr"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.12034.pdf",
      "abstract": "This paper presents a novel paradigm for building scalable 3D generative\nmodels utilizing pre-trained video diffusion models. The primary obstacle in\ndeveloping foundation 3D generative models is the limited availability of 3D\ndata. Unlike images, texts, or videos, 3D data are not readily accessible and\nare difficult to acquire. This results in a significant disparity in scale\ncompared to the vast quantities of other types of data. To address this issue,\nwe propose using a video diffusion model, trained with extensive volumes of\ntext, images, and videos, as a knowledge source for 3D data. By unlocking its\nmulti-view generative capabilities through fine-tuning, we generate a\nlarge-scale synthetic multi-view dataset to train a feed-forward 3D generative\nmodel. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view\ndata, can generate a 3D asset from a single image in seconds and achieves\nsuperior performance when compared to current SOTA feed-forward 3D generative\nmodels, with users preferring our results over 70% of the time.",
      "upvotes": 5
    }
  ]
}