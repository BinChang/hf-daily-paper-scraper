{
  "date": "2024-02-13",
  "papers": [
    {
      "title": "Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model",
      "url": "https://huggingface.co/papers/2402.07827",
      "authors": [
        "Gbemileke Onilude",
        "Hui-Lee Ooi",
        "Phil Blunsom"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07827.pdf",
      "abstract": "Recent breakthroughs in large language models (LLMs) have centered around a\nhandful of data-rich languages. What does it take to broaden access to\nbreakthroughs beyond first-class citizen languages? Our work introduces Aya, a\nmassively multilingual generative language model that follows instructions in\n101 languages of which over 50% are considered as lower-resourced. Aya\noutperforms mT0 and BLOOMZ on the majority of tasks while covering double the\nnumber of languages. We introduce extensive new evaluation suites that broaden\nthe state-of-art for multilingual eval across 99 languages -- including\ndiscriminative and generative tasks, human evaluation, and simulated win rates\nthat cover both held-out tasks and in-distribution performance. Furthermore, we\nconduct detailed investigations on the optimal finetuning mixture composition,\ndata pruning, as well as the toxicity, bias, and safety of our models. We\nopen-source our instruction datasets and our model at\nhttps://hf.co/CohereForAI/aya-101",
      "upvotes": 45
    },
    {
      "title": "OS-Copilot: Towards Generalist Computer Agents with Self-Improvement",
      "url": "https://huggingface.co/papers/2402.07456",
      "authors": [
        "Zhoumianze Liu",
        "Tao Yu",
        "Lingpeng Kong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07456.pdf",
      "abstract": "Autonomous interaction with the computer has been a longstanding challenge\nwith great potential, and the recent proliferation of large language models\n(LLMs) has markedly accelerated progress in building digital agents. However,\nmost of these agents are designed to interact with a narrow domain, such as a\nspecific software or website. This narrow focus constrains their applicability\nfor general computer tasks. To this end, we introduce OS-Copilot, a framework\nto build generalist agents capable of interfacing with comprehensive elements\nin an operating system (OS), including the web, code terminals, files,\nmultimedia, and various third-party applications. We use OS-Copilot to create\nFRIDAY, a self-improving embodied agent for automating general computer tasks.\nOn GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods\nby 35%, showcasing strong generalization to unseen applications via accumulated\nskills from previous tasks. We also present numerical and quantitative evidence\nthat FRIDAY learns to control and self-improve on Excel and Powerpoint with\nminimal supervision. Our OS-Copilot framework and empirical findings provide\ninfrastructure and insights for future research toward more capable and\ngeneral-purpose computer agents.",
      "upvotes": 41
    },
    {
      "title": "ChemLLM: A Chemical Large Language Model",
      "url": "https://huggingface.co/papers/2402.06852",
      "authors": [
        "Wei Liu",
        "Hang Yan",
        "Xiangyu Yue",
        "Shufei Zhang",
        "Mao Su",
        "Hansen Zhong",
        "Wanli Ouyang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.06852.pdf",
      "abstract": "Large language models (LLMs) have made impressive progress in chemistry\napplications, including molecular property prediction, molecular generation,\nexperimental protocol design, etc. However, the community lacks a\ndialogue-based model specifically designed for chemistry. The challenge arises\nfrom the fact that most chemical data and scientific knowledge are primarily\nstored in structured databases, and the direct use of these structured data\ncompromises the model's ability to maintain coherent dialogue. To tackle this\nissue, we develop a novel template-based instruction construction method that\ntransforms structured knowledge into plain dialogue, making it suitable for\nlanguage model training. By leveraging this approach, we develop ChemLLM, the\nfirst large language model dedicated to chemistry, capable of performing\nvarious tasks across chemical disciplines with smooth dialogue interaction.\nChemLLM beats GPT-3.5 on all three principal tasks in chemistry, i.e., name\nconversion, molecular caption, and reaction prediction, and surpasses GPT-4 on\ntwo of them. Remarkably, ChemLLM also shows exceptional adaptability to related\nmathematical and physical tasks despite being trained mainly on\nchemical-centric corpora. Furthermore, ChemLLM demonstrates proficiency in\nspecialized NLP tasks within chemistry, such as literature translation and\ncheminformatic programming. ChemLLM opens up a new avenue for exploration\nwithin chemical studies, while our method of integrating structured chemical\nknowledge into dialogue systems sets a new frontier for developing LLMs across\nvarious scientific fields. Codes, Datasets, and Model weights are publicly\naccessible at hf.co/AI4Chem/ChemLLM-7B-Chat.",
      "upvotes": 26
    },
    {
      "title": "Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models",
      "url": "https://huggingface.co/papers/2402.07033",
      "authors": [
        "Kan Zhu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07033.pdf",
      "abstract": "Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture\nare showing promising performance on various tasks. However, running them on\nresource-constrained settings, where GPU memory resources are not abundant, is\nchallenging due to huge model sizes. Existing systems that offload model\nweights to CPU memory suffer from the significant overhead of frequently moving\ndata between CPU and GPU. In this paper, we propose Fiddler, a\nresource-efficient inference engine with CPU-GPU orchestration for MoE models.\nThe key idea of Fiddler is to use the computation ability of the CPU to\nminimize the data movement between the CPU and GPU. Our evaluation shows that\nFiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in\nparameters, to generate over 3 tokens per second on a single GPU with 24GB\nmemory, showing an order of magnitude improvement over existing methods. The\ncode of Fiddler is publicly available at\nhttps://github.com/efeslab/fiddler",
      "upvotes": 16
    },
    {
      "title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs",
      "url": "https://huggingface.co/papers/2402.07872",
      "authors": [
        "Soroush Nasiriany",
        "Wenhao Yu",
        "Ted Xiao",
        "Danny Driess",
        "Zhuo Xu",
        "Tingnan Zhang",
        "Tsang-Wei Edward Lee",
        "Kuang-Huei Lee",
        "Peng Xu",
        "Yuke Zhu",
        "Nicolas Heess",
        "Sergey Levine"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07872.pdf",
      "abstract": "Vision language models (VLMs) have shown impressive capabilities across a\nvariety of tasks, from logical reasoning to visual understanding. This opens\nthe door to richer interaction with the world, for example robotic control.\nHowever, VLMs produce only textual outputs, while robotic control and other\nspatial tasks require outputting continuous coordinates, actions, or\ntrajectories. How can we enable VLMs to handle such settings without\nfine-tuning on task-specific data?\n  In this paper, we propose a novel visual prompting approach for VLMs that we\ncall Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as\niterative visual question answering. In each iteration, the image is annotated\nwith a visual representation of proposals that the VLM can refer to (e.g.,\ncandidate robot actions, localizations, or trajectories). The VLM then selects\nthe best ones for the task. These proposals are iteratively refined, allowing\nthe VLM to eventually zero in on the best available answer. We investigate\nPIVOT on real-world robotic navigation, real-world manipulation from images,\ninstruction following in simulation, and additional spatial inference tasks\nsuch as localization. We find, perhaps surprisingly, that our approach enables\nzero-shot control of robotic systems without any robot training data,\nnavigation in a variety of environments, and other capabilities. Although\ncurrent performance is far from perfect, our work highlights potentials and\nlimitations of this new regime and shows a promising approach for\nInternet-Scale VLMs in robotic and spatial reasoning domains. Website:\npivot-prompt.github.io and HuggingFace:\nhttps://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.",
      "upvotes": 15
    },
    {
      "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF",
      "url": "https://huggingface.co/papers/2402.07319",
      "authors": [
        "Chen Zhu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07319.pdf",
      "abstract": "In this work, we study the issue of reward hacking on the response length, a\nchallenge emerging in Reinforcement Learning from Human Feedback (RLHF) on\nLLMs. A well-formatted, verbose but less helpful response from the LLMs can\noften deceive LLMs or even human evaluators to achieve high scores. The same\nissue also holds for some reward models in RL. To address the challenges in\nboth training and evaluation, we establish a more reliable evaluation protocol\nfor comparing different training configurations, which inspects the trade-off\nbetween LLM evaluation score and response length obtained by varying training\nhyperparameters. Based on this evaluation, we conduct large-scale studies,\nwhere the results shed insights into the efficacy of hyperparameters and tricks\nused in RL on mitigating length bias. We further propose to improve the reward\nmodel by jointly training two linear heads on shared feature representations to\npredict the rewards, one trained to correlate with length, and the other\ntrained to decorrelate with length and therefore focus more on the actual\ncontent. We then discard the length head in RL to prevent reward hacking on\nlength. Experiments demonstrate that our approach almost eliminates the reward\ncorrelation with length, and improves the obtained policy by a significant\nmargin.",
      "upvotes": 13
    },
    {
      "title": "Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like",
      "url": "https://huggingface.co/papers/2402.07383",
      "authors": [
        "Hemin Yang",
        "Zirun Zhu",
        "Canrun Li",
        "Steven Tsai",
        "Zhen Xiao",
        "Yanqing Liu",
        "Sheng Zhao",
        "Michael Zeng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07383.pdf",
      "abstract": "Laughter is one of the most expressive and natural aspects of human speech,\nconveying emotions, social cues, and humor. However, most text-to-speech (TTS)\nsystems lack the ability to produce realistic and appropriate laughter sounds,\nlimiting their applications and user experience. While there have been prior\nworks to generate natural laughter, they fell short in terms of controlling the\ntiming and variety of the laughter to be generated. In this work, we propose\nELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker\nbased on a short audio prompt with precise control of laughter timing and\nexpression. Specifically, ELaTE works on the audio prompt to mimic the voice\ncharacteristic, the text prompt to indicate the contents of the generated\nspeech, and the input to control the laughter expression, which can be either\nthe start and end times of laughter, or the additional audio prompt that\ncontains laughter to be mimicked. We develop our model based on the foundation\nof conditional flow-matching-based zero-shot TTS, and fine-tune it with\nframe-level representation from a laughter detector as additional conditioning.\nWith a simple scheme to mix small-scale laughter-conditioned data with\nlarge-scale pre-training data, we demonstrate that a pre-trained zero-shot TTS\nmodel can be readily fine-tuned to generate natural laughter with precise\ncontrollability, without losing any quality of the pre-trained zero-shot TTS\nmodel. Through the evaluations, we show that ELaTE can generate laughing speech\nwith significantly higher quality and controllability compared to conventional\nmodels. See https://aka.ms/elate/ for demo samples.",
      "upvotes": 13
    },
    {
      "title": "A Tale of Tails: Model Collapse as a Change of Scaling Laws",
      "url": "https://huggingface.co/papers/2402.07043",
      "authors": [
        "Elvis Dohmatob",
        "Francois Charton",
        "Julia Kempe"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07043.pdf",
      "abstract": "As AI model size grows, neural scaling laws have become a crucial tool to\npredict the improvements of large models when increasing capacity and the size\nof original (human or natural) training data. Yet, the widespread use of\npopular models means that the ecosystem of online data and text will co-evolve\nto progressively contain increased amounts of synthesized data. In this paper\nwe ask: How will the scaling laws change in the inevitable regime where\nsynthetic data makes its way into the training corpus? Will future models,\nstill improve, or be doomed to degenerate up to total (model) collapse? We\ndevelop a theoretical framework of model collapse through the lens of scaling\nlaws. We discover a wide range of decay phenomena, analyzing loss of scaling,\nshifted scaling with number of generations, the ''un-learning\" of skills, and\ngrokking when mixing human and synthesized data. Our theory is validated by\nlarge-scale experiments with a transformer on an arithmetic task and text\ngeneration using the large language model Llama2.",
      "upvotes": 13
    },
    {
      "title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models",
      "url": "https://huggingface.co/papers/2402.07865",
      "authors": [
        "Dorsa Sadigh"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07865.pdf",
      "abstract": "Visually-conditioned language models (VLMs) have seen growing adoption in\napplications such as visual dialogue, scene understanding, and robotic task\nplanning; adoption that has fueled a wealth of new models such as LLaVa,\nInstructBLIP, and PaLI-3. Despite the volume of new releases, key design\ndecisions around image preprocessing, architecture, and optimization are\nunder-explored, making it challenging to understand what factors account for\nmodel performance - a challenge further complicated by the lack of objective,\nconsistent evaluations. To address these gaps, we first compile a suite of\nstandardized evaluations spanning visual question answering, object\nlocalization from language, and targeted challenge sets that probe properties\nsuch as hallucination; evaluations that provide calibrated, fine-grained\ninsight into a VLM's capabilities. Second, we rigorously investigate VLMs along\nkey design axes, including pretrained visual representations and quantifying\nthe tradeoffs of using base vs. instruct-tuned language models, amongst others.\nWe couple our analysis with three resource contributions: (1) a unified\nframework for evaluating VLMs, (2) optimized, flexible code for VLM training,\nand (3) checkpoints for all models, including a family of VLMs at the 7-13B\nscale that strictly outperform InstructBLIP and LLaVa v1.5, the\nstate-of-the-art in open-source VLMs.",
      "upvotes": 12
    },
    {
      "title": "Scaling Laws for Fine-Grained Mixture of Experts",
      "url": "https://huggingface.co/papers/2402.07871",
      "authors": [
        "Jakub Krajewski",
        "Jan Ludziejewski",
        "Kamil Adamczewski",
        "Kamil Ciebiera",
        "Krystian Król",
        "Tomasz Odrzygóźdź",
        "Piotr Sankowski",
        "Marek Cygan",
        "Sebastian Jaszczur"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07871.pdf",
      "abstract": "Mixture of Experts (MoE) models have emerged as a primary solution for\nreducing the computational cost of Large Language Models. In this work, we\nanalyze their scaling properties, incorporating an expanded range of variables.\nSpecifically, we introduce a new hyperparameter, granularity, whose adjustment\nenables precise control over the size of the experts. Building on this, we\nestablish scaling laws for fine-grained MoE, taking into account the number of\ntraining tokens, model size, and granularity. Leveraging these laws, we derive\nthe optimal training configuration for a given computational budget. Our\nfindings not only show that MoE models consistently outperform dense\nTransformers but also highlight that the efficiency gap between dense and MoE\nmodels widens as we scale up the model size and training budget. Furthermore,\nwe demonstrate that the common practice of setting the size of experts in MoE\nto mirror the feed-forward layer is not optimal at almost any computational\nbudget.",
      "upvotes": 11
    },
    {
      "title": "AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts",
      "url": "https://huggingface.co/papers/2402.07625",
      "authors": [
        "Yang Yuan",
        "Andrew Chi-Chih Yao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07625.pdf",
      "abstract": "To improve language models' proficiency in mathematical reasoning via\ncontinual pretraining, we introduce a novel strategy that leverages base\nlanguage models for autonomous data selection. Departing from conventional\nsupervised fine-tuning or trained classifiers with human-annotated data, our\napproach utilizes meta-prompted language models as zero-shot verifiers to\nautonomously evaluate and select high-quality mathematical content, and we\nrelease the curated open-source AutoMathText dataset encompassing over 200GB of\ndata. To demonstrate the efficacy of our method, we continuously pretrained a\n7B-parameter Mistral language model on the AutoMathText dataset, achieving\nsubstantial improvements in downstream performance on the MATH dataset with a\ntoken amount reduced by orders of magnitude compared to previous continuous\npretraining works. Our method showcases a 2 times increase in pretraining token\nefficiency compared to baselines, underscoring the potential of our approach in\nenhancing models' mathematical reasoning capabilities. The AutoMathText dataset\nis available at https://huggingface.co/datasets/math-ai/AutoMathText. The code\nis available at https://github.com/yifanzhang-pro/AutoMathText.",
      "upvotes": 11
    },
    {
      "title": "Suppressing Pink Elephants with Direct Principle Feedback",
      "url": "https://huggingface.co/papers/2402.07896",
      "authors": [
        "Siddharth Verma"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07896.pdf",
      "abstract": "Existing methods for controlling language models, such as RLHF and\nConstitutional AI, involve determining which LLM behaviors are desirable and\ntraining them into a language model. However, in many cases, it is desirable\nfor LLMs to be controllable at inference time, so that they can be\nused in multiple contexts with diverse needs. We illustrate this with the\nPink Elephant Problem: instructing an LLM to avoid discussing a\ncertain entity (a ``Pink Elephant''), and instead discuss a preferred entity\n(``Grey Elephant''). We apply a novel simplification of Constitutional AI,\nDirect Principle Feedback, which skips the ranking of responses and\nuses DPO directly on critiques and revisions. Our results show that after DPF\nfine-tuning on our synthetic Pink Elephants dataset, our 13B fine-tuned LLaMA 2\nmodel significantly outperforms Llama-2-13B-Chat and a prompted baseline, and\nperforms as well as GPT-4 in on our curated test set assessing the Pink\nElephant Problem.",
      "upvotes": 9
    },
    {
      "title": "LiRank: Industrial Large Scale Ranking Models at LinkedIn",
      "url": "https://huggingface.co/papers/2402.06859",
      "authors": [
        "Fedor Borisyuk",
        "Mingzhou Zhou",
        "Siyu Zhu",
        "Birjodh Tiwana",
        "Ganesh Parameswaran",
        "Siddharth Dangi",
        "Lars Hertel",
        "Qiang Xiao",
        "Yunbo Ouyang",
        "Aman Gupta",
        "Sheallika Singh",
        "Dan Liu",
        "Hailing Cheng",
        "Lei Le",
        "Jonathan Hung",
        "Sathiya Keerthi",
        "Ruoyan Wang",
        "Fengyu Zhang",
        "Mohit Kothari",
        "Chen Zhu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.06859.pdf",
      "abstract": "We present LiRank, a large-scale ranking framework at LinkedIn that brings to\nproduction state-of-the-art modeling architectures and optimization methods. We\nunveil several modeling improvements, including Residual DCN, which adds\nattention and residual connections to the famous DCNv2 architecture. We share\ninsights into combining and tuning SOTA architectures to create a unified\nmodel, including Dense Gating, Transformers and Residual DCN. We also propose\nnovel techniques for calibration and describe how we productionalized deep\nlearning based explore/exploit methods. To enable effective, production-grade\nserving of large ranking models, we detail how to train and compress models\nusing quantization and vocabulary compression. We provide details about the\ndeployment setup for large-scale use cases of Feed ranking, Jobs\nRecommendations, and Ads click-through rate (CTR) prediction. We summarize our\nlearnings from various A/B tests by elucidating the most effective technical\napproaches. These ideas have contributed to relative metrics improvements\nacross the board at LinkedIn: +0.5% member sessions in the Feed, +1.76%\nqualified job applications for Jobs search and recommendations, and +4.3% for\nAds CTR. We hope this work can provide practical insights and solutions for\npractitioners interested in leveraging large-scale deep ranking systems.",
      "upvotes": 8
    },
    {
      "title": "Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping",
      "url": "https://huggingface.co/papers/2402.07610",
      "authors": [
        "Haoyu Wang",
        "Guozheng Ma",
        "Ziqiao Meng",
        "Li Shen",
        "Zhong Zhang",
        "Liu Liu",
        "Tingyang Xu",
        "Xueqian Wang",
        "Peilin Zhao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07610.pdf",
      "abstract": "Self-alignment is an effective way to reduce the cost of human annotation\nwhile ensuring promising model capability. However, most current methods\ncomplete the data collection and training steps in a single round, which may\noverlook the continuously improving ability of self-aligned models. This gives\nrise to a key query: What if we do multi-time bootstrapping self-alignment?\nDoes this strategy enhance model performance or lead to rapid degradation? In\nthis paper, our pioneering exploration delves into the impact of bootstrapping\nself-alignment on large language models. Our findings reveal that bootstrapping\nself-alignment markedly surpasses the single-round approach, by guaranteeing\ndata diversity from in-context learning. To further exploit the capabilities of\nbootstrapping, we investigate and adjust the training order of data, which\nyields improved performance of the model. Drawing on these findings, we propose\nStep-On-Feet Tuning (SOFT) which leverages model's continuously enhanced\nfew-shot ability to boost zero or one-shot performance. Based on easy-to-hard\ntraining recipe, we propose SOFT+ which further boost self-alignment's\nperformance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across\nvarious classification and generation tasks, highlighting the potential of\nbootstrapping self-alignment on continually enhancing model alignment\nperformance.",
      "upvotes": 7
    },
    {
      "title": "GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting",
      "url": "https://huggingface.co/papers/2402.07207",
      "authors": [
        "Xiaoyu Zhou",
        "Xingjian Ran",
        "Jinlin He",
        "Zhiwei Lin",
        "Yongtao Wang",
        "Deqing Sun",
        "Ming-Hsuan Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07207.pdf",
      "abstract": "We present GALA3D, generative 3D GAussians with LAyout-guided control, for\neffective compositional text-to-3D generation. We first utilize large language\nmodels (LLMs) to generate the initial layout and introduce a layout-guided 3D\nGaussian representation for 3D content generation with adaptive geometric\nconstraints. We then propose an object-scene compositional optimization\nmechanism with conditioned diffusion to collaboratively generate realistic 3D\nscenes with consistent geometry, texture, scale, and accurate interactions\namong multiple objects while simultaneously adjusting the coarse layout priors\nextracted from the LLMs to align with the generated scene. Experiments show\nthat GALA3D is a user-friendly, end-to-end framework for state-of-the-art\nscene-level 3D content generation and controllable editing while ensuring the\nhigh fidelity of object-level entities within the scene. Source codes and\nmodels will be available at https://gala3d.github.io/.",
      "upvotes": 7
    },
    {
      "title": "Policy Improvement using Language Feedback Models",
      "url": "https://huggingface.co/papers/2402.07876",
      "authors": [
        "Dipendra Misra",
        "Xingdi Yuan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07876.pdf",
      "abstract": "We introduce Language Feedback Models (LFMs) that identify desirable\nbehaviour - actions that help achieve tasks specified in the instruction - for\nimitation learning in instruction following. To train LFMs, we obtain feedback\nfrom Large Language Models (LLMs) on visual trajectories verbalized to language\ndescriptions. First, by using LFMs to identify desirable behaviour to imitate,\nwe improve in task-completion rate over strong behavioural cloning baselines on\nthree distinct language grounding environments (Touchdown, ScienceWorld, and\nALFWorld). Second, LFMs outperform using LLMs as experts to directly predict\nactions, when controlling for the number of LLM output tokens. Third, LFMs\ngeneralize to unseen environments, improving task-completion rate by 3.5-12.0%\nthrough one round of adaptation. Finally, LFM can be modified to provide\nhuman-interpretable feedback without performance loss, allowing human\nverification of desirable behaviour for imitation learning.",
      "upvotes": 5
    }
  ]
}