{
  "date": "2024-07-06",
  "papers": [
    {
      "title": "Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion",
      "url": "https://huggingface.co/papers/2407.01392",
      "authors": [
        "Diego Marti Monso"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01392.pdf",
      "abstract": "This paper presents Diffusion Forcing, a new training paradigm where a\ndiffusion model is trained to denoise a set of tokens with independent\nper-token noise levels. We apply Diffusion Forcing to sequence generative\nmodeling by training a causal next-token prediction model to generate one or\nseveral future tokens without fully diffusing past ones. Our approach is shown\nto combine the strengths of next-token prediction models, such as\nvariable-length generation, with the strengths of full-sequence diffusion\nmodels, such as the ability to guide sampling to desirable trajectories. Our\nmethod offers a range of additional capabilities, such as (1) rolling-out\nsequences of continuous tokens, such as video, with lengths past the training\nhorizon, where baselines diverge and (2) new sampling and guiding schemes that\nuniquely profit from Diffusion Forcing's variable-horizon and causal\narchitecture, and which lead to marked performance gains in decision-making and\nplanning tasks. In addition to its empirical success, our method is proven to\noptimize a variational lower bound on the likelihoods of all subsequences of\ntokens drawn from the true joint distribution. Project website:\nhttps://boyuan.space/diffusion-forcing/",
      "upvotes": 39
    },
    {
      "title": "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models",
      "url": "https://huggingface.co/papers/2407.01906",
      "authors": [
        "Zihan Wang",
        "Deli Chen",
        "Damai Dai",
        "Runxin Xu",
        "Zhuoshu Li",
        "Y. Wu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01906.pdf",
      "abstract": "Parameter-efficient fine-tuning (PEFT) is crucial for customizing Large\nLanguage Models (LLMs) with constrained resources. Although there have been\nvarious PEFT methods for dense-architecture LLMs, PEFT for sparse-architecture\nLLMs is still underexplored. In this work, we study the PEFT method for LLMs\nwith the Mixture-of-Experts (MoE) architecture and the contents of this work\nare mainly threefold: (1) We investigate the dispersion degree of the activated\nexperts in customized tasks, and found that the routing distribution for a\nspecific task tends to be highly concentrated, while the distribution of\nactivated experts varies significantly across different tasks. (2) We propose\nExpert-Specialized Fine-Tuning, or ESFT, which tunes the experts most relevant\nto downstream tasks while freezing the other experts and modules; experimental\nresults demonstrate that our method not only improves the tuning efficiency,\nbut also matches or even surpasses the performance of full-parameter\nfine-tuning. (3) We further analyze the impact of the MoE architecture on\nexpert-specialized fine-tuning. We find that MoE models with finer-grained\nexperts are more advantageous in selecting the combination of experts that are\nmost relevant to downstream tasks, thereby enhancing both the training\nefficiency and effectiveness.",
      "upvotes": 34
    },
    {
      "title": "Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages",
      "url": "https://huggingface.co/papers/2407.03321",
      "authors": [
        "Michael L. Littman",
        "Stephen H. Bach"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03321.pdf",
      "abstract": "Many recent works have explored using language models for planning problems.\nOne line of research focuses on translating natural language descriptions of\nplanning tasks into structured planning languages, such as the planning domain\ndefinition language (PDDL). While this approach is promising, accurately\nmeasuring the quality of generated PDDL code continues to pose significant\nchallenges. First, generated PDDL code is typically evaluated using planning\nvalidators that check whether the problem can be solved with a planner. This\nmethod is insufficient because a language model might generate valid PDDL code\nthat does not align with the natural language description of the task. Second,\nexisting evaluation sets often have natural language descriptions of the\nplanning task that closely resemble the ground truth PDDL, reducing the\nchallenge of the task. To bridge this gap, we introduce \\benchmarkName, a\nbenchmark designed to evaluate language models' ability to generate PDDL code\nfrom natural language descriptions of planning tasks. We begin by creating a\nPDDL equivalence algorithm that rigorously evaluates the correctness of PDDL\ncode generated by language models by flexibly comparing it against a ground\ntruth PDDL. Then, we present a dataset of 132,037 text-to-PDDL pairs across\n13 different tasks, with varying levels of difficulty. Finally, we evaluate\nseveral API-access and open-weight language models that reveal this task's\ncomplexity. For example, 87.6% of the PDDL problem descriptions generated by\nGPT-4o are syntactically parseable, 82.2% are valid, solve-able problems,\nbut only 35.1% are semantically correct, highlighting the need for a more\nrigorous benchmark for this problem.",
      "upvotes": 15
    }
  ]
}