{
  "date": "2024-04-29",
  "papers": [
    {
      "title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
      "url": "https://huggingface.co/papers/2404.16873",
      "authors": [
        "Anselm Paulus"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.16873.pdf",
      "abstract": "While recently Large Language Models (LLMs) have achieved remarkable\nsuccesses, they are vulnerable to certain jailbreaking attacks that lead to\ngeneration of inappropriate or harmful content. Manual red-teaming requires\nfinding adversarial prompts that cause such jailbreaking, e.g. by appending a\nsuffix to a given instruction, which is inefficient and time-consuming. On the\nother hand, automatic adversarial prompt generation often leads to semantically\nmeaningless attacks that can easily be detected by perplexity-based filters,\nmay require gradient information from the TargetLLM, or do not scale well due\nto time-consuming discrete optimization processes over the token space. In this\npaper, we present a novel method that uses another LLM, called the AdvPrompter,\nto generate human-readable adversarial prompts in seconds, sim800times\nfaster than existing optimization-based approaches. We train the AdvPrompter\nusing a novel algorithm that does not require access to the gradients of the\nTargetLLM. This process alternates between two steps: (1) generating\nhigh-quality target adversarial suffixes by optimizing the AdvPrompter\npredictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated\nadversarial suffixes. The trained AdvPrompter generates suffixes that veil the\ninput instruction without changing its meaning, such that the TargetLLM is\nlured to give a harmful response. Experimental results on popular open source\nTargetLLMs show state-of-the-art results on the AdvBench dataset, that also\ntransfer to closed-source black-box LLM APIs. Further, we demonstrate that by\nfine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made\nmore robust against jailbreaking attacks while maintaining performance, i.e.\nhigh MMLU scores.",
      "upvotes": 28
    },
    {
      "title": "MaPa: Text-driven Photorealistic Material Painting for 3D Shapes",
      "url": "https://huggingface.co/papers/2404.17569",
      "authors": [
        "Shangzhan Zhang",
        "Tao Xu",
        "Tianrun Chen",
        "Nan Xue",
        "Yujun Shen",
        "Hujun Bao",
        "Ruizhen Hu",
        "Xiaowei Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.17569.pdf",
      "abstract": "This paper aims to generate materials for 3D meshes from text descriptions.\nUnlike existing methods that synthesize texture maps, we propose to generate\nsegment-wise procedural material graphs as the appearance representation, which\nsupports high-quality rendering and provides substantial flexibility in\nediting. Instead of relying on extensive paired data, i.e., 3D meshes with\nmaterial graphs and corresponding text descriptions, to train a material graph\ngenerative model, we propose to leverage the pre-trained 2D diffusion model as\na bridge to connect the text and material graphs. Specifically, our approach\ndecomposes a shape into a set of segments and designs a segment-controlled\ndiffusion model to synthesize 2D images that are aligned with mesh parts. Based\non generated images, we initialize parameters of material graphs and fine-tune\nthem through the differentiable rendering module to produce materials in\naccordance with the textual description. Extensive experiments demonstrate the\nsuperior performance of our framework in photorealism, resolution, and\neditability over existing methods. Project page:\nhttps://zhanghe3z.github.io/MaPa/",
      "upvotes": 12
    }
  ]
}