{
  "date": "2024-11-10",
  "papers": [
    {
      "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
      "url": "https://huggingface.co/papers/2411.04905",
      "authors": [
        "Siming Huang",
        "Tianhao Cheng",
        "Jason Klein Liu",
        "Jiaran Hao",
        "Liuyihan Song",
        "Yang Xu",
        "J. Yang",
        "J. H. Liu",
        "Chenchen Zhang",
        "Linzheng Chai",
        "Ruifeng Yuan",
        "Zhaoxiang Zhang",
        "Jie Fu",
        "Qian Liu",
        "Ge Zhang",
        "Yuan Qi",
        "Yinghui Xu",
        "Wei Chu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04905.pdf",
      "abstract": "Large language models (LLMs) for code have become indispensable in various\ndomains, including code generation, reasoning tasks and agent systems.While\nopen-access code LLMs are increasingly approaching the performance levels of\nproprietary models, high-quality code LLMs suitable for rigorous scientific\ninvestigation, particularly those with reproducible data processing pipelines\nand transparent training protocols, remain limited. The scarcity is due to\nvarious challenges, including resource constraints, ethical considerations, and\nthe competitive advantages of keeping models advanced. To address the gap, we\nintroduce OpenCoder, a top-tier code LLM that not only achieves performance\ncomparable to leading models but also serves as an ``open cookbook'' for the\nresearch community. Unlike most prior efforts, we release not only model\nweights and inference code, but also the reproducible training data, complete\ndata processing pipeline, rigorous experimental ablation results, and detailed\ntraining protocols for open scientific research. Through this comprehensive\nrelease, we identify the key ingredients for building a top-tier code LLM: (1)\ncode optimized heuristic rules for data cleaning and methods for data\ndeduplication, (2) recall of text corpus related to code and (3) high-quality\nsynthetic data in both annealing and supervised fine-tuning stages. By offering\nthis level of openness, we aim to broaden access to all aspects of a top-tier\ncode LLM, with OpenCoder serving as both a powerful model and an open\nfoundation to accelerate research, and enable reproducible advancements in code\nAI.",
      "upvotes": 91
    },
    {
      "title": "ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning",
      "url": "https://huggingface.co/papers/2411.05003",
      "authors": [
        "David Junhao Zhang",
        "Roni Paiss",
        "Shiran Zada",
        "Nikhil Karnad",
        "David E. Jacobs",
        "Yael Pritch",
        "Inbar Mosseri",
        "Mike Zheng Shou",
        "Neal Wadhwa",
        "Nataniel Ruiz"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.05003.pdf",
      "abstract": "Recently, breakthroughs in video modeling have allowed for controllable\ncamera trajectories in generated videos. However, these methods cannot be\ndirectly applied to user-provided videos that are not generated by a video\nmodel. In this paper, we present ReCapture, a method for generating new videos\nwith novel camera trajectories from a single user-provided video. Our method\nallows us to re-generate the reference video, with all its existing scene\nmotion, from vastly different angles and with cinematic camera motion. Notably,\nusing our method we can also plausibly hallucinate parts of the scene that were\nnot observable in the reference video. Our method works by (1) generating a\nnoisy anchor video with a new camera trajectory using multiview diffusion\nmodels or depth-based point cloud rendering and then (2) regenerating the\nanchor video into a clean and temporally consistent reangled video using our\nproposed masked video fine-tuning technique.",
      "upvotes": 62
    },
    {
      "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
      "url": "https://huggingface.co/papers/2411.04965",
      "authors": [
        "Hongyu Wang",
        "Shuming Ma",
        "Furu Wei"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04965.pdf",
      "abstract": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
      "upvotes": 58
    },
    {
      "title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models",
      "url": "https://huggingface.co/papers/2411.04996",
      "authors": [
        "Lili Yu",
        "Liang Luo",
        "Srinivasan Iyer",
        "Ning Dong",
        "Chunting Zhou",
        "Gargi Ghosh",
        "Mike Lewis",
        "Wen-tau Yih",
        "Luke Zettlemoyer",
        "Xi Victoria Lin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04996.pdf",
      "abstract": "The development of large language models (LLMs) has expanded to multi-modal\nsystems capable of processing text, images, and speech within a unified\nframework. Training these models demands significantly larger datasets and\ncomputational resources compared to text-only LLMs. To address the scaling\nchallenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal\ntransformer architecture that significantly reduces pretraining computational\ncosts. MoT decouples non-embedding parameters of the model by modality --\nincluding feed-forward networks, attention matrices, and layer normalization --\nenabling modality-specific processing with global self-attention over the full\ninput sequence. We evaluate MoT across multiple settings and model scales. In\nthe Chameleon 7B setting (autoregressive text-and-image generation), MoT\nmatches the dense baseline's performance using only 55.8\\% of the FLOPs. When\nextended to include speech, MoT reaches speech performance comparable to the\ndense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where\ntext and image are trained with different objectives, a 7B MoT model matches\nthe image modality performance of the dense baseline with one third of the\nFLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image\ngeneration metrics. System profiling further highlights MoT's practical\nbenefits, achieving dense baseline image quality in 47.2\\% of the wall-clock\ntime and text quality in 75.6\\% of the wall-clock time (measured on AWS\np4de.24xlarge instances with NVIDIA A100 GPUs).",
      "upvotes": 40
    },
    {
      "title": "DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion",
      "url": "https://huggingface.co/papers/2411.04928",
      "authors": [
        "Wenqiang Sun",
        "Shuo Chen",
        "Fangfu Liu",
        "Zilong Chen",
        "Yueqi Duan",
        "Jun Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04928.pdf",
      "abstract": "In this paper, we introduce DimensionX, a framework designed to\ngenerate photorealistic 3D and 4D scenes from just a single image with video\ndiffusion. Our approach begins with the insight that both the spatial structure\nof a 3D scene and the temporal evolution of a 4D scene can be effectively\nrepresented through sequences of video frames. While recent video diffusion\nmodels have shown remarkable success in producing vivid visuals, they face\nlimitations in directly recovering 3D/4D scenes due to limited spatial and\ntemporal controllability during generation. To overcome this, we propose\nST-Director, which decouples spatial and temporal factors in video diffusion by\nlearning dimension-aware LoRAs from dimension-variant data. This controllable\nvideo diffusion approach enables precise manipulation of spatial structure and\ntemporal dynamics, allowing us to reconstruct both 3D and 4D representations\nfrom sequential frames with the combination of spatial and temporal dimensions.\nAdditionally, to bridge the gap between generated videos and real-world scenes,\nwe introduce a trajectory-aware mechanism for 3D generation and an\nidentity-preserving denoising strategy for 4D generation. Extensive experiments\non various real-world and synthetic datasets demonstrate that DimensionX\nachieves superior results in controllable video generation, as well as in 3D\nand 4D scene generation, compared with previous methods.",
      "upvotes": 37
    },
    {
      "title": "M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding",
      "url": "https://huggingface.co/papers/2411.04952",
      "authors": [
        "Jaemin Cho",
        "Debanjan Mahata",
        "Ozan Irsoy",
        "Yujie He",
        "Mohit Bansal"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04952.pdf",
      "abstract": "Document visual question answering (DocVQA) pipelines that answer questions\nfrom documents have broad applications. Existing methods focus on handling\nsingle-page documents with multi-modal language models (MLMs), or rely on\ntext-based retrieval-augmented generation (RAG) that uses text extraction tools\nsuch as optical character recognition (OCR). However, there are difficulties in\napplying these methods in real-world scenarios: (a) questions often require\ninformation across different pages or documents, where MLMs cannot handle many\nlong documents; (b) documents often have important information in visual\nelements such as figures, but text extraction tools ignore them. We introduce\nM3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various\ndocument contexts (closed-domain and open-domain), question hops (single-hop\nand multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG\nfinds relevant documents and answers questions using a multi-modal retriever\nand an MLM, so that it can efficiently handle single or many documents while\npreserving visual information. Since previous DocVQA datasets ask questions in\nthe context of a specific document, we also present M3DocVQA, a new benchmark\nfor evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.\nIn three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results\nshow that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance\nthan many strong baselines, including state-of-the-art performance in\nMP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and\nretrieval models. Lastly, we qualitatively show that M3DocRAG can successfully\nhandle various scenarios, such as when relevant information exists across\nmultiple pages and when answer evidence only exists in images.",
      "upvotes": 23
    },
    {
      "title": "TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation",
      "url": "https://huggingface.co/papers/2411.04709",
      "authors": [
        "Wenhao Wang",
        "Yi Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04709.pdf",
      "abstract": "Video generation models are revolutionizing content creation, with\nimage-to-video models drawing increasing attention due to their enhanced\ncontrollability, visual consistency, and practical applications. However,\ndespite their popularity, these models rely on user-provided text and image\nprompts, and there is currently no dedicated dataset for studying these\nprompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of\nover 1.70 million unique user-provided Text and Image Prompts specifically for\nImage-to-Video generation. Additionally, we provide the corresponding generated\nvideos from five state-of-the-art image-to-video models. We begin by outlining\nthe time-consuming and costly process of curating this large-scale dataset.\nNext, we compare TIP-I2V to two popular prompt datasets, VidProM\n(text-to-video) and DiffusionDB (text-to-image), highlighting differences in\nboth basic and semantic information. This dataset enables advancements in\nimage-to-video research. For instance, to develop better models, researchers\ncan use the prompts in TIP-I2V to analyze user preferences and evaluate the\nmulti-dimensional performance of their trained models; and to enhance model\nsafety, they may focus on addressing the misinformation issue caused by\nimage-to-video models. The new research inspired by TIP-I2V and the differences\nwith existing datasets emphasize the importance of a specialized image-to-video\nprompt dataset. The project is publicly available at https://tip-i2v.github.io.",
      "upvotes": 23
    },
    {
      "title": "VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos",
      "url": "https://huggingface.co/papers/2411.04923",
      "authors": [
        "Shehan Munasinghe",
        "Hanan Gani",
        "Wenqi Zhu",
        "Jiale Cao",
        "Eric Xing",
        "Fahad Shahbaz Khan",
        "Salman Khan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04923.pdf",
      "abstract": "Fine-grained alignment between videos and text is challenging due to complex\nspatial and temporal dynamics in videos. Existing video-based Large Multimodal\nModels (LMMs) handle basic conversations but struggle with precise pixel-level\ngrounding in videos. To address this, we introduce VideoGLaMM, a LMM designed\nfor fine-grained pixel-level grounding in videos based on user-provided textual\ninputs. Our design seamlessly connects three key components: a Large Language\nModel, a dual vision encoder that emphasizes both spatial and temporal details,\nand a spatio-temporal decoder for accurate mask generation. This connection is\nfacilitated via tunable V-L and L-V adapters that enable close Vision-Language\n(VL) alignment. The architecture is trained to synchronize both spatial and\ntemporal elements of video content with textual instructions. To enable\nfine-grained grounding, we curate a multimodal dataset featuring detailed\nvisually-grounded conversations using a semiautomatic annotation pipeline,\nresulting in a diverse set of 38k video-QA triplets along with 83k objects and\n671k masks. We evaluate VideoGLaMM on three challenging tasks: Grounded\nConversation Generation, Visual Grounding, and Referring Video Segmentation.\nExperimental results show that our model consistently outperforms existing\napproaches across all three tasks.",
      "upvotes": 20
    },
    {
      "title": "Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?",
      "url": "https://huggingface.co/papers/2411.05000",
      "authors": [
        "Jonathan Roberts",
        "Kai Han",
        "Samuel Albanie"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.05000.pdf",
      "abstract": "As the context limits of Large Language Models (LLMs) increase, the range of\npossible applications and downstream functions broadens. In many real-world\ntasks, decisions depend on details scattered across collections of often\ndisparate documents containing mostly irrelevant information. Long-context LLMs\nappear well-suited to this form of complex information retrieval and reasoning,\nwhich has traditionally proven costly and time-consuming. However, although the\ndevelopment of longer context models has seen rapid gains in recent years, our\nunderstanding of how effectively LLMs use their context has not kept pace. To\naddress this, we conduct a set of retrieval experiments designed to evaluate\nthe capabilities of 17 leading LLMs, such as their ability to follow threads of\ninformation through the context window. Strikingly, we find that many models\nare remarkably threadsafe: capable of simultaneously following multiple threads\nwithout significant loss in performance. Still, for many models, we find the\neffective context limit is significantly shorter than the supported context\nlength, with accuracy decreasing as the context window grows. Our study also\nhighlights the important point that token counts from different tokenizers\nshould not be directly compared -- they often correspond to substantially\ndifferent numbers of written characters. We release our code and long-context\nexperimental data.",
      "upvotes": 18
    },
    {
      "title": "Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model",
      "url": "https://huggingface.co/papers/2411.04496",
      "authors": [
        "Young-Jun Lee",
        "Dokyong Lee",
        "Junyoung Youn",
        "Kyeongjin Oh",
        "Ho-Jin Choi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04496.pdf",
      "abstract": "To increase social bonding with interlocutors, humans naturally acquire the\nability to respond appropriately in a given situation by considering which\nconversational skill is most suitable for the response - a process we call\nskill-of-mind. For large language model (LLM)-based conversational agents,\nplanning appropriate conversational skills, as humans do, is challenging due to\nthe complexity of social dialogue, especially in interactive scenarios. To\naddress this, we propose a skill-of-mind-annotated conversation dataset, named\nMultifaceted Skill-of-Mind, which includes multi-turn and multifaceted\nconversational skills across various interactive scenarios (e.g., long-term,\ncounseling, task-oriented), grounded in diverse social contexts (e.g.,\ndemographics, persona, rules of thumb). This dataset consists of roughly 100K\nconversations. Using this dataset, we introduce a new family of\nskill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B\nparameters. With extensive experiments, these models successfully demonstrate\nthe skill-of-mind process and exhibit strong generalizability in inferring\nmultifaceted skills across a variety of domains. Moreover, we show that Thanos\nsignificantly enhances the quality of responses generated by LLM-based\nconversational agents and promotes prosocial behavior in human evaluations.",
      "upvotes": 18
    },
    {
      "title": "Analyzing The Language of Visual Tokens",
      "url": "https://huggingface.co/papers/2411.05001",
      "authors": [
        "David M. Chan",
        "Rodolfo Corona",
        "Joonyong Park",
        "Cheol Jun Cho",
        "Yutong Bai",
        "Trevor Darrell"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.05001.pdf",
      "abstract": "With the introduction of transformer-based models for vision and language\ntasks, such as LLaVA and Chameleon, there has been renewed interest in the\ndiscrete tokenized representation of images. These models often treat image\npatches as discrete tokens, analogous to words in natural language, learning\njoint alignments between visual and human languages. However, little is known\nabout the statistical behavior of these visual languages - whether they follow\nsimilar frequency distributions, grammatical structures, or topologies as\nnatural languages. In this paper, we take a natural-language-centric approach\nto analyzing discrete visual languages and uncover striking similarities and\nfundamental differences. We demonstrate that, although visual languages adhere\nto Zipfian distributions, higher token innovation drives greater entropy and\nlower compression, with tokens predominantly representing object parts,\nindicating intermediate granularity. We also show that visual languages lack\ncohesive grammatical structures, leading to higher perplexity and weaker\nhierarchical organization compared to natural languages. Finally, we\ndemonstrate that, while vision models align more closely with natural languages\nthan other models, this alignment remains significantly weaker than the\ncohesion found within natural languages. Through these experiments, we\ndemonstrate how understanding the statistical properties of discrete visual\nlanguages can inform the design of more effective computer vision models.",
      "upvotes": 16
    },
    {
      "title": "DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation",
      "url": "https://huggingface.co/papers/2411.04999",
      "authors": [
        "Peiqi Liu",
        "Zhanqiu Guo",
        "Mohit Warke",
        "Soumith Chintala",
        "Chris Paxton",
        "Nur Muhammad Mahi Shafiullah",
        "Lerrel Pinto"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04999.pdf",
      "abstract": "Significant progress has been made in open-vocabulary mobile manipulation,\nwhere the goal is for a robot to perform tasks in any environment given a\nnatural language description. However, most current systems assume a static\nenvironment, which limits the system's applicability in real-world scenarios\nwhere environments frequently change due to human intervention or the robot's\nown actions. In this work, we present DynaMem, a new approach to open-world\nmobile manipulation that uses a dynamic spatio-semantic memory to represent a\nrobot's environment. DynaMem constructs a 3D data structure to maintain a\ndynamic memory of point clouds, and answers open-vocabulary object localization\nqueries using multimodal LLMs or open-vocabulary features generated by\nstate-of-the-art vision-language models. Powered by DynaMem, our robots can\nexplore novel environments, search for objects not found in memory, and\ncontinuously update the memory as objects move, appear, or disappear in the\nscene. We run extensive experiments on the Stretch SE3 robots in three real and\nnine offline scenes, and achieve an average pick-and-drop success rate of 70%\non non-stationary objects, which is more than a 2x improvement over\nstate-of-the-art static systems. Our code as well as our experiment and\ndeployment videos are open sourced and can be found on our project website:\nhttps://dynamem.github.io/",
      "upvotes": 16
    },
    {
      "title": "SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models",
      "url": "https://huggingface.co/papers/2411.05007",
      "authors": [
        "Muyang Li",
        "Yujun Lin",
        "Zhekai Zhang",
        "Tianle Cai",
        "Xiuyu Li",
        "Junxian Guo",
        "Enze Xie",
        "Chenlin Meng",
        "Jun-Yan Zhu",
        "Song Han"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.05007.pdf",
      "abstract": "Diffusion models have been proven highly effective at generating high-quality\nimages. However, as these models grow larger, they require significantly more\nmemory and suffer from higher latency, posing substantial challenges for\ndeployment. In this work, we aim to accelerate diffusion models by quantizing\ntheir weights and activations to 4 bits. At such an aggressive level, both\nweights and activations are highly sensitive, where conventional post-training\nquantization methods for large language models like smoothing become\ninsufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit\nquantization paradigm. Different from smoothing which redistributes outliers\nbetween weights and activations, our approach absorbs these outliers using a\nlow-rank branch. We first consolidate the outliers by shifting them from\nactivations to weights, then employ a high-precision low-rank branch to take in\nthe weight outliers with Singular Value Decomposition (SVD). This process eases\nthe quantization on both sides. However, na\\\"{\\i}vely running the low-rank\nbranch independently incurs significant overhead due to extra data movement of\nactivations, negating the quantization speedup. To address this, we co-design\nan inference engine Nunchaku that fuses the kernels of the low-rank branch into\nthose of the low-bit branch to cut off redundant memory access. It can also\nseamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for\nre-quantization. Extensive experiments on SDXL, PixArt-Sigma, and FLUX.1\nvalidate the effectiveness of SVDQuant in preserving image quality. We reduce\nthe memory usage for the 12B FLUX.1 models by 3.5times, achieving\n3.0times speedup over the 4-bit weight-only quantized baseline on the 16GB\nlaptop 4090 GPU, paving the way for more interactive applications on PCs. Our\nquantization library and inference engine are open-sourced.",
      "upvotes": 15
    },
    {
      "title": "GazeGen: Gaze-Driven User Interaction for Visual Content Generation",
      "url": "https://huggingface.co/papers/2411.04335",
      "authors": [
        "He-Yen Hsieh",
        "Ziyun Li",
        "Sai Qian Zhang",
        "Wei-Te Mark Ting",
        "Kao-Den Chang",
        "Barbara De Salvo",
        "Chiao Liu",
        "H. T. Kung"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04335.pdf",
      "abstract": "We present GazeGen, a user interaction system that generates visual content\n(images and videos) for locations indicated by the user's eye gaze. GazeGen\nallows intuitive manipulation of visual content by targeting regions of\ninterest with gaze. Using advanced techniques in object detection and\ngenerative AI, GazeGen performs gaze-controlled image adding/deleting,\nrepositioning, and surface material changes of image objects, and converts\nstatic images into videos. Central to GazeGen is the DFT Gaze (Distilled and\nFine-Tuned Gaze) agent, an ultra-lightweight model with only 281K parameters,\nperforming accurate real-time gaze predictions tailored to individual users'\neyes on small edge devices. GazeGen is the first system to combine visual\ncontent generation with real-time gaze estimation, made possible exclusively by\nDFT Gaze. This real-time gaze estimation enables various visual content\ngeneration tasks, all controlled by the user's gaze. The input for DFT Gaze is\nthe user's eye images, while the inputs for visual content generation are the\nuser's view and the predicted gaze point from DFT Gaze. To achieve efficient\ngaze predictions, we derive the small model from a large model (10x larger) via\nnovel knowledge distillation and personal adaptation techniques. We integrate\nknowledge distillation with a masked autoencoder, developing a compact yet\npowerful gaze estimation model. This model is further fine-tuned with Adapters,\nenabling highly accurate and personalized gaze predictions with minimal user\ninput. DFT Gaze ensures low-latency and precise gaze tracking, supporting a\nwide range of gaze-driven tasks. We validate the performance of DFT Gaze on AEA\nand OpenEDS2020 benchmarks, demonstrating low angular gaze error and low\nlatency on the edge device (Raspberry Pi 4). Furthermore, we describe\napplications of GazeGen, illustrating its versatility and effectiveness in\nvarious usage scenarios.",
      "upvotes": 13
    },
    {
      "title": "RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval",
      "url": "https://huggingface.co/papers/2411.04752",
      "authors": [
        "Aniket Deroy",
        "Subhankar Maity"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04752.pdf",
      "abstract": "Code-mixing, the integration of lexical and grammatical elements from\nmultiple languages within a single sentence, is a widespread linguistic\nphenomenon, particularly prevalent in multilingual societies. In India, social\nmedia users frequently engage in code-mixed conversations using the Roman\nscript, especially among migrant communities who form online groups to share\nrelevant local information. This paper focuses on the challenges of extracting\nrelevant information from code-mixed conversations, specifically within Roman\ntransliterated Bengali mixed with English. This study presents a novel approach\nto address these challenges by developing a mechanism to automatically identify\nthe most relevant answers from code-mixed conversations. We have experimented\nwith a dataset comprising of queries and documents from Facebook, and Query\nRelevance files (QRels) to aid in this task. Our results demonstrate the\neffectiveness of our approach in extracting pertinent information from complex,\ncode-mixed digital conversations, contributing to the broader field of natural\nlanguage processing in multilingual and informal text environments. We use\nGPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant\ndocuments to frame a mathematical model which helps to detect relevant\ndocuments corresponding to a query.",
      "upvotes": 13
    },
    {
      "title": "M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models",
      "url": "https://huggingface.co/papers/2411.04075",
      "authors": [
        "Chuhan Li",
        "Ziyao Shangguan",
        "Yilun Zhao",
        "Deyuan Li",
        "Yixin Liu",
        "Arman Cohan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04075.pdf",
      "abstract": "Existing benchmarks for evaluating foundation models mainly focus on\nsingle-document, text-only tasks. However, they often fail to fully capture the\ncomplexity of research workflows, which typically involve interpreting\nnon-textual data and gathering information across multiple documents. To\naddress this gap, we introduce M3SciQA, a multi-modal, multi-document\nscientific question answering benchmark designed for a more comprehensive\nevaluation of foundation models. M3SciQA consists of 1,452 expert-annotated\nquestions spanning 70 natural language processing paper clusters, where each\ncluster represents a primary paper along with all its cited documents,\nmirroring the workflow of comprehending a single paper by requiring multi-modal\nand multi-document data. With M3SciQA, we conduct a comprehensive evaluation of\n18 foundation models. Our results indicate that current foundation models still\nsignificantly underperform compared to human experts in multi-modal information\nretrieval and in reasoning across multiple scientific documents. Additionally,\nwe explore the implications of these findings for the future advancement of\napplying foundation models in multi-modal scientific literature analysis.",
      "upvotes": 12
    },
    {
      "title": "Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models",
      "url": "https://huggingface.co/papers/2411.05005",
      "authors": [
        "Shuhong Zheng",
        "Zhipeng Bao",
        "Ruoyu Zhao",
        "Martial Hebert",
        "Yu-Xiong Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.05005.pdf",
      "abstract": "Beyond high-fidelity image synthesis, diffusion models have recently\nexhibited promising results in dense visual perception tasks. However, most\nexisting work treats diffusion models as a standalone component for perception\ntasks, employing them either solely for off-the-shelf data augmentation or as\nmere feature extractors. In contrast to these isolated and thus sub-optimal\nefforts, we introduce a unified, versatile, diffusion-based framework,\nDiff-2-in-1, that can simultaneously handle both multi-modal data generation\nand dense visual perception, through a unique exploitation of the\ndiffusion-denoising process. Within this framework, we further enhance\ndiscriminative visual perception via multi-modal generation, by utilizing the\ndenoising network to create multi-modal data that mirror the distribution of\nthe original training set. Importantly, Diff-2-in-1 optimizes the utilization\nof the created diverse and faithful data by leveraging a novel self-improving\nlearning mechanism. Comprehensive experimental evaluations validate the\neffectiveness of our framework, showcasing consistent performance improvements\nacross various discriminative backbones and high-quality multi-modal data\ngeneration characterized by both realism and usefulness.",
      "upvotes": 12
    },
    {
      "title": "SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation",
      "url": "https://huggingface.co/papers/2411.04989",
      "authors": [
        "Koichi Namekata",
        "Sherwin Bahmani",
        "Ziyi Wu",
        "Yash Kant",
        "Igor Gilitschenski",
        "David B. Lindell"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04989.pdf",
      "abstract": "Methods for image-to-video generation have achieved impressive,\nphoto-realistic quality. However, adjusting specific elements in generated\nvideos, such as object motion or camera movement, is often a tedious process of\ntrial and error, e.g., involving re-generating videos with different random\nseeds. Recent techniques address this issue by fine-tuning a pre-trained model\nto follow conditioning signals, such as bounding boxes or point trajectories.\nYet, this fine-tuning procedure can be computationally expensive, and it\nrequires datasets with annotated object motion, which can be difficult to\nprocure. In this work, we introduce SG-I2V, a framework for controllable\nimage-to-video generation that is self-guidedx2013offering\nzero-shot control by relying solely on the knowledge present in a pre-trained\nimage-to-video diffusion model without the need for fine-tuning or external\nknowledge. Our zero-shot method outperforms unsupervised baselines while being\ncompetitive with supervised models in terms of visual quality and motion\nfidelity.",
      "upvotes": 12
    }
  ]
}