{
  "date": "2024-07-30",
  "papers": [
    {
      "title": "SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain",
      "url": "https://huggingface.co/papers/2407.19584",
      "authors": [
        "Telmo Pires",
        "Etienne Malaboeuf",
        "Gabriel Hautreux",
        "Johanne Charpentier"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19584.pdf",
      "abstract": "In this paper, we introduce SaulLM-54B and SaulLM-141B, two large language\nmodels (LLMs) tailored for the legal sector. These models, which feature\narchitectures of 54 billion and 141 billion parameters, respectively, are based\non the Mixtral architecture. The development of SaulLM-54B and SaulLM-141B is\nguided by large-scale domain adaptation, divided into three strategies: (1) the\nexploitation of continued pretraining involving a base corpus that includes\nover 540 billion of legal tokens, (2) the implementation of a specialized legal\ninstruction-following protocol, and (3) the alignment of model outputs with\nhuman preferences in legal interpretations. The integration of synthetically\ngenerated data in the second and third steps enhances the models' capabilities\nin interpreting and processing legal texts, effectively reaching\nstate-of-the-art performance and outperforming previous open-source models on\nLegalBench-Instruct. This work explores the trade-offs involved in\ndomain-specific adaptation at this scale, offering insights that may inform\nfuture studies on domain adaptation using strong decoder models. Building upon\nSaulLM-7B, this study refines the approach to produce an LLM better equipped\nfor legal tasks. We are releasing base, instruct, and aligned versions on top\nof SaulLM-54B and SaulLM-141B under the MIT License to facilitate reuse and\ncollaborative research.",
      "upvotes": 61
    },
    {
      "title": "Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification",
      "url": "https://huggingface.co/papers/2407.19340",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2407.19340.pdf",
      "abstract": "Major Depressive Disorder (MDD) is a pervasive mental health condition that\naffects 300 million people worldwide. This work presents a novel, BiLSTM-based\ntri-modal model-level fusion architecture for the binary classification of\ndepression from clinical interview recordings. The proposed architecture\nincorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses\na two-shot learning based GPT-4 model to process text data. This is the first\nwork to incorporate large language models into a multi-modal architecture for\nthis task. It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge\ncross-validation split and Leave-One-Subject-Out cross-validation split,\nsurpassing all baseline models and multiple state-of-the-art models. In\nLeave-One-Subject-Out testing, it achieves an accuracy of 91.01%, an F1-Score\nof 85.95%, a precision of 80%, and a recall of 92.86%.",
      "upvotes": 56
    },
    {
      "title": "SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages",
      "url": "https://huggingface.co/papers/2407.19672",
      "authors": [
        "Yew Ken Chia"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19672.pdf",
      "abstract": "Large Language Models (LLMs) have shown remarkable abilities across various\ntasks, yet their development has predominantly centered on high-resource\nlanguages like English and Chinese, leaving low-resource languages underserved.\nTo address this disparity, we present SeaLLMs 3, the latest iteration of the\nSeaLLMs model family, tailored for Southeast Asian languages. This region,\ncharacterized by its rich linguistic diversity, has lacked adequate language\ntechnology support. SeaLLMs 3 aims to bridge this gap by covering a\ncomprehensive range of languages spoken in this region, including English,\nChinese, Indonesian, Vietnamese, Thai, Tagalog, Malay, Burmese, Khmer, Lao,\nTamil, and Javanese. Leveraging efficient language enhancement techniques and a\nspecially constructed instruction tuning dataset, SeaLLMs 3 significantly\nreduces training costs while maintaining high performance and versatility. Our\nmodel excels in tasks such as world knowledge, mathematical reasoning,\ntranslation, and instruction following, achieving state-of-the-art performance\namong similarly sized models. Additionally, we prioritized safety and\nreliability by addressing both general and culture-specific considerations and\nincorporated mechanisms to reduce hallucinations. This work underscores the\nimportance of inclusive AI, showing that advanced LLM capabilities can benefit\nunderserved linguistic and cultural communities.",
      "upvotes": 54
    },
    {
      "title": "FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention",
      "url": "https://huggingface.co/papers/2407.19918",
      "authors": [
        "Yuanzhi Liang",
        "Yi Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19918.pdf",
      "abstract": "Video diffusion models have made substantial progress in various video\ngeneration applications. However, training models for long video generation\ntasks require significant computational and data resources, posing a challenge\nto developing long video diffusion models. This paper investigates a\nstraightforward and training-free approach to extend an existing short video\ndiffusion model (e.g. pre-trained on 16-frame videos) for consistent long video\ngeneration (e.g. 128 frames). Our preliminary observation has found that\ndirectly applying the short video diffusion model to generate long videos can\nlead to severe video quality degradation. Further investigation reveals that\nthis degradation is primarily due to the distortion of high-frequency\ncomponents in long videos, characterized by a decrease in spatial\nhigh-frequency components and an increase in temporal high-frequency\ncomponents. Motivated by this, we propose a novel solution named FreeLong to\nbalance the frequency distribution of long video features during the denoising\nprocess. FreeLong blends the low-frequency components of global video features,\nwhich encapsulate the entire video sequence, with the high-frequency components\nof local video features that focus on shorter subsequences of frames. This\napproach maintains global consistency while incorporating diverse and\nhigh-quality spatiotemporal details from local videos, enhancing both the\nconsistency and fidelity of long video generation. We evaluated FreeLong on\nmultiple base video diffusion models and observed significant improvements.\nAdditionally, our method supports coherent multi-prompt generation, ensuring\nboth visual coherence and seamless transitions between scenes.",
      "upvotes": 47
    },
    {
      "title": "Theia: Distilling Diverse Vision Foundation Models for Robot Learning",
      "url": "https://huggingface.co/papers/2407.20179",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2407.20179.pdf",
      "abstract": "Vision-based robot policy learning, which maps visual inputs to actions,\nnecessitates a holistic understanding of diverse visual tasks beyond\nsingle-task needs like classification or segmentation. Inspired by this, we\nintroduce Theia, a vision foundation model for robot learning that distills\nmultiple off-the-shelf vision foundation models trained on varied vision tasks.\nTheia's rich visual representations encode diverse visual knowledge, enhancing\ndownstream robot learning. Extensive experiments demonstrate that Theia\noutperforms its teacher models and prior robot learning models using less\ntraining data and smaller model sizes. Additionally, we quantify the quality of\npre-trained visual representations and hypothesize that higher entropy in\nfeature norm distributions leads to improved robot learning performance. Code\nand models are available at https://github.com/bdaiinstitute/theia.",
      "upvotes": 45
    },
    {
      "title": "MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains",
      "url": "https://huggingface.co/papers/2407.18961",
      "authors": [
        "Shuang Ma",
        "Feng Nan",
        "Shen Ma",
        "Vik Kamath",
        "Mathias Berglund",
        "Tobias Gindele",
        "Juergen Wiest",
        "Xiaoming Wang",
        "Jiulong Shan",
        "Meng Cao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.18961.pdf",
      "abstract": "Recent advances in large language models (LLMs) have increased the demand for\ncomprehensive benchmarks to evaluate their capabilities as human-like agents.\nExisting benchmarks, while useful, often focus on specific application\nscenarios, emphasizing task completion but failing to dissect the underlying\nskills that drive these outcomes. This lack of granularity makes it difficult\nto deeply discern where failures stem from. Additionally, setting up these\nenvironments requires considerable effort, and issues of unreliability and\nreproducibility sometimes arise, especially in interactive tasks. To address\nthese limitations, we introduce the Massive Multitask Agent Understanding\n(MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need\nfor complex environment setups. It evaluates models across five domains,\nincluding teal{Tool-use}, teal{Directed Acyclic Graph\n(DAG) QA}, teal{Data Science and Machine Learning coding},\nteal{Contest-level programming} and teal{Mathematics},\nand covers five essential capabilities: orange{Understanding},\norange{Reasoning}, orange{Planning},\norange{Problem-solving}, and orange{Self-correction}.\nWith a total of 20 meticulously designed tasks encompassing over 3K distinct\nprompts, MMAU provides a comprehensive framework for evaluating the strengths\nand limitations of LLM agents. By testing 18 representative models on MMAU, we\nprovide deep and insightful analyses. Ultimately, MMAU not only sheds light on\nthe capabilities and limitations of LLM agents but also enhances the\ninterpretability of their performance. Datasets and evaluation scripts of MMAU\nare released at https://github.com/apple/axlearn/docs/research/mmau.",
      "upvotes": 38
    },
    {
      "title": "MindSearch: Mimicking Human Minds Elicits Deep AI Searcher",
      "url": "https://huggingface.co/papers/2407.20183",
      "authors": [
        "Qiuchen Wang",
        "Kai Chen",
        "Feng Zhao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.20183.pdf",
      "abstract": "Information seeking and integration is a complex cognitive task that consumes\nenormous time and effort. Inspired by the remarkable progress of Large Language\nModels, recent works attempt to solve this task by combining LLMs and search\nengines. However, these methods still obtain unsatisfying performance due to\nthree challenges: (1) complex requests often cannot be accurately and\ncompletely retrieved by the search engine once (2) corresponding information to\nbe integrated is spread over multiple web pages along with massive noise, and\n(3) a large number of web pages with long contents may quickly exceed the\nmaximum context length of LLMs. Inspired by the cognitive process when humans\nsolve these problems, we introduce MindSearch to mimic the human minds in web\ninformation seeking and integration, which can be instantiated by a simple yet\neffective LLM-based multi-agent framework. The WebPlanner models the human mind\nof multi-step information seeking as a dynamic graph construction process: it\ndecomposes the user query into atomic sub-questions as nodes in the graph and\nprogressively extends the graph based on the search result from WebSearcher.\nTasked with each sub-question, WebSearcher performs hierarchical information\nretrieval with search engines and collects valuable information for WebPlanner.\nThe multi-agent design of MindSearch enables the whole framework to seek and\nintegrate information parallelly from larger-scale (e.g., more than 300) web\npages in 3 minutes, which is worth 3 hours of human effort. MindSearch\ndemonstrates significant improvement in the response quality in terms of depth\nand breadth, on both close-set and open-set QA problems. Besides, responses\nfrom MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web\nand Perplexity.ai applications, which implies that MindSearch can already\ndeliver a competitive solution to the proprietary AI search engine.",
      "upvotes": 37
    },
    {
      "title": "Mixture of Nested Experts: Adaptive Processing of Visual Tokens",
      "url": "https://huggingface.co/papers/2407.19985",
      "authors": [
        "Shyamal Buch",
        "Prateek Jain"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19985.pdf",
      "abstract": "The visual medium (images and videos) naturally contains a large amount of\ninformation redundancy, thereby providing a great opportunity for leveraging\nefficiency in processing. While Vision Transformer (ViT) based models scale\neffectively to large data regimes, they fail to capitalize on this inherent\nredundancy, leading to higher computational costs. Mixture of Experts (MoE)\nnetworks demonstrate scalability while maintaining same inference-time costs,\nbut they come with a larger parameter footprint. We present Mixture of Nested\nExperts (MoNE), which utilizes a nested structure for experts, wherein\nindividual experts fall on an increasing compute-accuracy curve. Given a\ncompute budget, MoNE learns to dynamically choose tokens in a priority order,\nand thus redundant tokens are processed through cheaper nested experts. Using\nthis framework, we achieve equivalent performance as the baseline models, while\nreducing inference time compute by over two-fold. We validate our approach on\nstandard image and video datasets - ImageNet-21K, Kinetics400, and\nSomething-Something-v2. We further highlight MoNE's adaptability by\nshowcasing its ability to maintain strong performance across different\ninference-time compute budgets on videos, using only a single trained model.",
      "upvotes": 34
    },
    {
      "title": "Diffusion Feedback Helps CLIP See Better",
      "url": "https://huggingface.co/papers/2407.20171",
      "authors": [
        "Jing Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.20171.pdf",
      "abstract": "Contrastive Language-Image Pre-training (CLIP), which excels at abstracting\nopen-world representations across domains and modalities, has become a\nfoundation for a variety of vision and multimodal tasks. However, recent\nstudies reveal that CLIP has severe visual shortcomings, such as which can\nhardly distinguish orientation, quantity, color, structure, etc. These visual\nshortcomings also limit the perception capabilities of multimodal large\nlanguage models (MLLMs) built on CLIP. The main reason could be that the\nimage-text pairs used to train CLIP are inherently biased, due to the lack of\nthe distinctiveness of the text and the diversity of images. In this work, we\npresent a simple post-training approach for CLIP models, which largely\novercomes its visual shortcomings via a self-supervised diffusion process. We\nintroduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP.\nSpecifically, DIVA leverages generative feedback from text-to-image diffusion\nmodels to optimize CLIP representations, with only images (without\ncorresponding text). We demonstrate that DIVA improves CLIP's performance on\nthe challenging MMVP-VLM benchmark which assesses fine-grained visual abilities\nto a large extent (e.g., 3-7%), and enhances the performance of MLLMs and\nvision models on multimodal understanding and segmentation tasks. Extensive\nevaluation on 29 image classification and retrieval benchmarks confirms that\nour framework preserves CLIP's strong zero-shot capabilities. The code will be\navailable at https://github.com/baaivision/DIVA.",
      "upvotes": 34
    },
    {
      "title": "Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning",
      "url": "https://huggingface.co/papers/2407.18248",
      "authors": [
        "Shichen Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.18248.pdf",
      "abstract": "Effective training of language models (LMs) for mathematical reasoning tasks\ndemands high-quality supervised fine-tuning data. Besides obtaining annotations\nfrom human experts, a common alternative is sampling from larger and more\npowerful LMs. However, this knowledge distillation approach can be costly and\nunstable, particularly when relying on closed-source, proprietary LMs like\nGPT-4, whose behaviors are often unpredictable. In this work, we demonstrate\nthat the reasoning abilities of small-scale LMs can be enhanced through\nself-training, a process where models learn from their own outputs. We also\nshow that the conventional self-training can be further augmented by a\npreference learning algorithm called Direct Preference Optimization (DPO). By\nintegrating DPO into self-training, we leverage preference data to guide LMs\ntowards more accurate and diverse chain-of-thought reasoning. We evaluate our\nmethod across various mathematical reasoning tasks using different base models.\nOur experiments show that this approach not only improves LMs' reasoning\nperformance but also offers a more cost-effective and scalable solution\ncompared to relying on large proprietary LMs.",
      "upvotes": 30
    },
    {
      "title": "Cycle3D: High-quality and Consistent Image-to-3D Generation via Generation-Reconstruction Cycle",
      "url": "https://huggingface.co/papers/2407.19548",
      "authors": [
        "Yatian Pang",
        "Bin Lin",
        "Li Yuan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19548.pdf",
      "abstract": "Recent 3D large reconstruction models typically employ a two-stage process,\nincluding first generate multi-view images by a multi-view diffusion model, and\nthen utilize a feed-forward model to reconstruct images to 3D content.However,\nmulti-view diffusion models often produce low-quality and inconsistent images,\nadversely affecting the quality of the final 3D reconstruction. To address this\nissue, we propose a unified 3D generation framework called Cycle3D, which\ncyclically utilizes a 2D diffusion-based generation module and a feed-forward\n3D reconstruction module during the multi-step diffusion process. Concretely,\n2D diffusion model is applied for generating high-quality texture, and the\nreconstruction model guarantees multi-view consistency.Moreover, 2D diffusion\nmodel can further control the generated content and inject reference-view\ninformation for unseen views, thereby enhancing the diversity and texture\nconsistency of 3D generation during the denoising process. Extensive\nexperiments demonstrate the superior ability of our method to create 3D content\nwith high-quality and consistency compared with state-of-the-art baselines.",
      "upvotes": 22
    },
    {
      "title": "Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models",
      "url": "https://huggingface.co/papers/2407.19474",
      "authors": [
        "Idan Szpektor",
        "Yuval Elovici"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19474.pdf",
      "abstract": "Imagine observing someone scratching their arm; to understand why, additional\ncontext would be necessary. However, spotting a mosquito nearby would\nimmediately offer a likely explanation for the person's discomfort, thereby\nalleviating the need for further information. This example illustrates how\nsubtle visual cues can challenge our cognitive skills and demonstrates the\ncomplexity of interpreting visual scenarios. To study these skills, we present\nVisual Riddles, a benchmark aimed to test vision and language models on visual\nriddles requiring commonsense and world knowledge. The benchmark comprises 400\nvisual riddles, each featuring a unique image created by a variety of\ntext-to-image models, question, ground-truth answer, textual hint, and\nattribution. Human evaluation reveals that existing models lag significantly\nbehind human performance, which is at 82\\% accuracy, with Gemini-Pro-1.5\nleading with 40\\% accuracy. Our benchmark comes with automatic evaluation tasks\nto make assessment scalable. These findings underscore the potential of Visual\nRiddles as a valuable resource for enhancing vision and language models'\ncapabilities in interpreting complex visual scenarios.",
      "upvotes": 22
    },
    {
      "title": "3D Question Answering for City Scene Understanding",
      "url": "https://huggingface.co/papers/2407.17398",
      "authors": [
        "Yaoxian Song",
        "Xiaofei Yang",
        "Qiang Wang",
        "Tiefeng Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.17398.pdf",
      "abstract": "3D multimodal question answering (MQA) plays a crucial role in scene\nunderstanding by enabling intelligent agents to comprehend their surroundings\nin 3D environments. While existing research has primarily focused on indoor\nhousehold tasks and outdoor roadside autonomous driving tasks, there has been\nlimited exploration of city-level scene understanding tasks. Furthermore,\nexisting research faces challenges in understanding city scenes, due to the\nabsence of spatial semantic information and human-environment interaction\ninformation at the city level.To address these challenges, we investigate 3D\nMQA from both dataset and method perspectives. From the dataset perspective, we\nintroduce a novel 3D MQA dataset named City-3DQA for city-level scene\nunderstanding, which is the first dataset to incorporate scene semantic and\nhuman-environment interactive tasks within the city. From the method\nperspective, we propose a Scene graph enhanced City-level Understanding method\n(Sg-CityU), which utilizes the scene graph to introduce the spatial semantic. A\nnew benchmark is reported and our proposed Sg-CityU achieves accuracy of 63.94\n% and 63.76 % in different settings of City-3DQA. Compared to indoor 3D MQA\nmethods and zero-shot using advanced large language models (LLMs), Sg-CityU\ndemonstrates state-of-the-art (SOTA) performance in robustness and\ngeneralization.",
      "upvotes": 21
    },
    {
      "title": "ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation",
      "url": "https://huggingface.co/papers/2407.19835",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2407.19835.pdf",
      "abstract": "Classical Arabic represents a significant era, encompassing the golden age of\nArab culture, philosophy, and scientific literature. With a broad consensus on\nthe importance of translating these literatures to enrich knowledge\ndissemination across communities, the advent of large language models (LLMs)\nand translation systems offers promising tools to facilitate this goal.\nHowever, we have identified a scarcity of translation datasets in Classical\nArabic, which are often limited in scope and topics, hindering the development\nof high-quality translation systems. In response, we present the ATHAR dataset,\ncomprising 66,000 high-quality Classical Arabic to English translation samples\nthat cover a wide array of subjects including science, culture, and philosophy.\nFurthermore, we assess the performance of current state-of-the-art LLMs under\nvarious settings, concluding that there is a need for such datasets in current\nsystems. Our findings highlight how models can benefit from fine-tuning or\nincorporating this dataset into their pretraining pipelines. The dataset is\npublicly available on the HuggingFace Data Hub at\nhttps://huggingface.co/datasets/mohamed-khalil/ATHAR.",
      "upvotes": 20
    },
    {
      "title": "ImagiNet: A Multi-Content Dataset for Generalizable Synthetic Image Detection via Contrastive Learning",
      "url": "https://huggingface.co/papers/2407.20020",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2407.20020.pdf",
      "abstract": "Generative models, such as diffusion models (DMs), variational autoencoders\n(VAEs), and generative adversarial networks (GANs), produce images with a level\nof authenticity that makes them nearly indistinguishable from real photos and\nartwork. While this capability is beneficial for many industries, the\ndifficulty of identifying synthetic images leaves online media platforms\nvulnerable to impersonation and misinformation attempts. To support the\ndevelopment of defensive methods, we introduce ImagiNet, a high-resolution and\nbalanced dataset for synthetic image detection, designed to mitigate potential\nbiases in existing resources. It contains 200K examples, spanning four content\ncategories: photos, paintings, faces, and uncategorized. Synthetic images are\nproduced with open-source and proprietary generators, whereas real counterparts\nof the same content type are collected from public datasets. The structure of\nImagiNet allows for a two-track evaluation system: i) classification as real or\nsynthetic and ii) identification of the generative model. To establish a\nbaseline, we train a ResNet-50 model using a self-supervised contrastive\nobjective (SelfCon) for each track. The model demonstrates state-of-the-art\nperformance and high inference speed across established benchmarks, achieving\nan AUC of up to 0.99 and balanced accuracy ranging from 86% to 95%, even under\nsocial network conditions that involve compression and resizing. Our data and\ncode are available at https://github.com/delyan-boychev/imaginet.",
      "upvotes": 19
    },
    {
      "title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge",
      "url": "https://huggingface.co/papers/2407.19594",
      "authors": [
        "Tianhao Wu",
        "Olga Golovneva",
        "Jing Xu",
        "Yuandong Tian",
        "Jiantao Jiao",
        "Jason Weston",
        "Sainbayar Sukhbaatar"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19594.pdf",
      "abstract": "Large Language Models (LLMs) are rapidly surpassing human knowledge in many\ndomains. While improving these models traditionally relies on costly human\ndata, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs\ncan improve by judging their own responses instead of relying on human\nlabelers. However, existing methods have primarily focused on improving model\nresponses rather than judgment capabilities, resulting in rapid saturation\nduring iterative training. To address this issue, we introduce a novel\nMeta-Rewarding step to the self-improvement process, where the model judges its\nown judgements and uses that feedback to refine its judgment skills.\nSurprisingly, this unsupervised approach improves the model's ability to judge\n{\\em and} follow instructions, as demonstrated by a win rate improvement of\nLlama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on\nArena-Hard. These results strongly suggest the potential for self-improving\nmodels without human supervision.",
      "upvotes": 19
    },
    {
      "title": "Sentiment Analysis of Lithuanian Online Reviews Using Large Language Models",
      "url": "https://huggingface.co/papers/2407.19914",
      "authors": [
        "Mantas Lukoševičius"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19914.pdf",
      "abstract": "Sentiment analysis is a widely researched area within Natural Language\nProcessing (NLP), attracting significant interest due to the advent of\nautomated solutions. Despite this, the task remains challenging because of the\ninherent complexity of languages and the subjective nature of sentiments. It is\neven more challenging for less-studied and less-resourced languages such as\nLithuanian. Our review of existing Lithuanian NLP research reveals that\ntraditional machine learning methods and classification algorithms have limited\neffectiveness for the task. In this work, we address sentiment analysis of\nLithuanian five-star-based online reviews from multiple domains that we collect\nand clean. We apply transformer models to this task for the first time,\nexploring the capabilities of pre-trained multilingual Large Language Models\n(LLMs), specifically focusing on fine-tuning BERT and T5 models. Given the\ninherent difficulty of the task, the fine-tuned models perform quite well,\nespecially when the sentiments themselves are less ambiguous: 80.74% and 89.61%\ntesting recognition accuracy of the most popular one- and five-star reviews\nrespectively. They significantly outperform current commercial state-of-the-art\ngeneral-purpose LLM GPT-4. We openly share our fine-tuned LLMs online.",
      "upvotes": 12
    },
    {
      "title": "Bridging the Gap: Studio-like Avatar Creation from a Monocular Phone Capture",
      "url": "https://huggingface.co/papers/2407.19593",
      "authors": [
        "Zhengyu Yang",
        "Stanislav Pidhorsky"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19593.pdf",
      "abstract": "Creating photorealistic avatars for individuals traditionally involves\nextensive capture sessions with complex and expensive studio devices like the\nLightStage system. While recent strides in neural representations have enabled\nthe generation of photorealistic and animatable 3D avatars from quick phone\nscans, they have the capture-time lighting baked-in, lack facial details and\nhave missing regions in areas such as the back of the ears. Thus, they lag in\nquality compared to studio-captured avatars. In this paper, we propose a method\nthat bridges this gap by generating studio-like illuminated texture maps from\nshort, monocular phone captures. We do this by parameterizing the phone texture\nmaps using the W^+ space of a StyleGAN2, enabling near-perfect\nreconstruction. Then, we finetune a StyleGAN2 by sampling in the W^+\nparameterized space using a very small set of studio-captured textures as an\nadversarial training signal. To further enhance the realism and accuracy of\nfacial details, we super-resolve the output of the StyleGAN2 using carefully\ndesigned diffusion model that is guided by image gradients of the\nphone-captured texture map. Once trained, our method excels at producing\nstudio-like facial texture maps from casual monocular smartphone videos.\nDemonstrating its capabilities, we showcase the generation of photorealistic,\nuniformly lit, complete avatars from monocular phone captures.\nhttp://shahrukhathar.github.io/2024/07/22/Bridging.html{The project page\ncan be found here.}",
      "upvotes": 12
    },
    {
      "title": "WalkTheDog: Cross-Morphology Motion Alignment via Phase Manifolds",
      "url": "https://huggingface.co/papers/2407.18946",
      "authors": [
        "Sebastian Starke",
        "Yuting Ye",
        "Olga Sorkine-Hornung"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.18946.pdf",
      "abstract": "We present a new approach for understanding the periodicity structure and\nsemantics of motion datasets, independently of the morphology and skeletal\nstructure of characters. Unlike existing methods using an overly sparse\nhigh-dimensional latent, we propose a phase manifold consisting of multiple\nclosed curves, each corresponding to a latent amplitude. With our proposed\nvector quantized periodic autoencoder, we learn a shared phase manifold for\nmultiple characters, such as a human and a dog, without any supervision. This\nis achieved by exploiting the discrete structure and a shallow network as\nbottlenecks, such that semantically similar motions are clustered into the same\ncurve of the manifold, and the motions within the same component are aligned\ntemporally by the phase variable. In combination with an improved motion\nmatching framework, we demonstrate the manifold's capability of timing and\nsemantics alignment in several applications, including motion retrieval,\ntransfer and stylization. Code and pre-trained models for this paper are\navailable at https://peizhuoli.github.io/walkthedog.",
      "upvotes": 12
    },
    {
      "title": "TAPTRv2: Attention-based Position Update Improves Tracking Any Point",
      "url": "https://huggingface.co/papers/2407.16291",
      "authors": [
        "Hao Zhang",
        "Shilong Liu",
        "Zhaoyang Zeng",
        "Feng Li",
        "Tianhe Ren",
        "Bohan Li",
        "Lei Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.16291.pdf",
      "abstract": "In this paper, we present TAPTRv2, a Transformer-based approach built upon\nTAPTR for solving the Tracking Any Point (TAP) task. TAPTR borrows designs from\nDEtection TRansformer (DETR) and formulates each tracking point as a point\nquery, making it possible to leverage well-studied operations in DETR-like\nalgorithms. TAPTRv2 improves TAPTR by addressing a critical issue regarding its\nreliance on cost-volume,which contaminates the point query\\'s content feature\nand negatively impacts both visibility prediction and cost-volume computation.\nIn TAPTRv2, we propose a novel attention-based position update (APU) operation\nand use key-aware deformable attention to realize. For each query, this\noperation uses key-aware attention weights to combine their corresponding\ndeformable sampling positions to predict a new query position. This design is\nbased on the observation that local attention is essentially the same as\ncost-volume, both of which are computed by dot-production between a query and\nits surrounding features. By introducing this new operation, TAPTRv2 not only\nremoves the extra burden of cost-volume computation, but also leads to a\nsubstantial performance improvement. TAPTRv2 surpasses TAPTR and achieves\nstate-of-the-art performance on many challenging datasets, demonstrating the\nsuperiority",
      "upvotes": 10
    },
    {
      "title": "VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks",
      "url": "https://huggingface.co/papers/2407.19795",
      "authors": [
        "Junehyoung Kwon",
        "JungMin Yun",
        "Seunguk Yu",
        "YoungBin Kim"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19795.pdf",
      "abstract": "Domain generalizability is a crucial aspect of a deep learning model since it\ndetermines the capability of the model to perform well on data from unseen\ndomains. However, research on the domain generalizability of deep learning\nmodels for vision-language tasks remains limited, primarily because of the lack\nof required datasets. To address these challenges, we propose VolDoGer:\nVision-Language Dataset for Domain Generalization, a dedicated dataset designed\nfor domain generalization that addresses three vision-language tasks: image\ncaptioning, visual question answering, and visual entailment. We constructed\nVolDoGer by extending LLM-based data annotation techniques to vision-language\ntasks, thereby alleviating the burden of recruiting human annotators. We\nevaluated the domain generalizability of various models, ranging from\nfine-tuned models to a recent multimodal large language model, through\nVolDoGer.",
      "upvotes": 10
    }
  ]
}