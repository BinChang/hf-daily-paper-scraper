{
  "date": "2024-06-04",
  "papers": [
    {
      "title": "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark",
      "url": "https://huggingface.co/papers/2406.01574",
      "authors": [
        "Shiguang Guo",
        "Kai Wang",
        "Rongqi Fan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.01574.pdf",
      "abstract": "In the age of large-scale language models, benchmarks like the Massive\nMultitask Language Understanding (MMLU) have been pivotal in pushing the\nboundaries of what AI can achieve in language comprehension and reasoning\nacross diverse domains. However, as models continue to improve, their\nperformance on these benchmarks has begun to plateau, making it increasingly\ndifficult to discern differences in model capabilities. This paper introduces\nMMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven\nMMLU benchmark by integrating more challenging, reasoning-focused questions and\nexpanding the choice set from four to ten options. Additionally, MMLU-Pro\neliminates the trivial and noisy questions in MMLU. Our experimental results\nshow that MMLU-Pro not only raises the challenge, causing a significant drop in\naccuracy by 16% to 33% compared to MMLU but also demonstrates greater stability\nunder varying prompts. With 24 different prompt styles tested, the sensitivity\nof model scores to prompt variations decreased from 4-5% in MMLU to just 2% in\nMMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT)\nreasoning achieved better performance on MMLU-Pro compared to direct answering,\nwhich is in stark contrast to the findings on the original MMLU, indicating\nthat MMLU-Pro includes more complex reasoning questions. Our assessments\nconfirm that MMLU-Pro is a more discriminative benchmark to better track\nprogress in the field.",
      "upvotes": 42
    },
    {
      "title": "Show, Don't Tell: Aligning Language Models with Demonstrated Feedback",
      "url": "https://huggingface.co/papers/2406.00888",
      "authors": [
        "Joey Hejna",
        "Michael Bernstein",
        "Diyi Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.00888.pdf",
      "abstract": "Language models are aligned to emulate the collective voice of many,\nresulting in outputs that align with no one in particular. Steering LLMs away\nfrom generic output is possible through supervised finetuning or RLHF, but\nrequires prohibitively large datasets for new ad-hoc tasks. We argue that it is\ninstead possible to align an LLM to a specific setting by leveraging a very\nsmall number (<10) of demonstrations as feedback. Our method, Demonstration\nITerated Task Optimization (DITTO), directly aligns language model outputs to a\nuser's demonstrated behaviors. Derived using ideas from online imitation\nlearning, DITTO cheaply generates online comparison data by treating users'\ndemonstrations as preferred over output from the LLM and its intermediate\ncheckpoints. We evaluate DITTO's ability to learn fine-grained style and task\nalignment across domains such as news articles, emails, and blog posts.\nAdditionally, we conduct a user study soliciting a range of demonstrations from\nparticipants (N=16). Across our benchmarks and user study, we find that\nwin-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and\nother self-play methods by an average of 19% points. By using demonstrations as\nfeedback directly, DITTO offers a novel method for effective customization of\nLLMs.",
      "upvotes": 30
    },
    {
      "title": "Learning Temporally Consistent Video Depth from Video Diffusion Priors",
      "url": "https://huggingface.co/papers/2406.01493",
      "authors": [
        "Hongyu Zhou",
        "Yujun Shen",
        "Yiyi Liao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.01493.pdf",
      "abstract": "This work addresses the challenge of video depth estimation, which expects\nnot only per-frame accuracy but, more importantly, cross-frame consistency.\nInstead of directly developing a depth estimator from scratch, we reformulate\nthe prediction task into a conditional generation problem. This allows us to\nleverage the prior knowledge embedded in existing video generation models,\nthereby reducing learn- ing difficulty and enhancing generalizability.\nConcretely, we study how to tame the public Stable Video Diffusion (SVD) to\npredict reliable depth from input videos using a mixture of image depth and\nvideo depth datasets. We empirically confirm that a procedural training\nstrategy - first optimizing the spatial layers of SVD and then optimizing the\ntemporal layers while keeping the spatial layers frozen - yields the best\nresults in terms of both spatial accuracy and temporal consistency. We further\nexamine the sliding window strategy for inference on arbitrarily long videos.\nOur observations indicate a trade-off between efficiency and performance, with\na one-frame overlap already producing favorable results. Extensive experimental\nresults demonstrate the superiority of our approach, termed ChronoDepth, over\nexisting alternatives, particularly in terms of the temporal consistency of the\nestimated depth. Additionally, we highlight the benefits of more consistent\nvideo depth in two practical applications: depth-conditioned video generation\nand novel view synthesis. Our project page is available at\nhttps://jhaoshao.github.io/ChronoDepth/{this http URL}.",
      "upvotes": 17
    },
    {
      "title": "Artificial Generational Intelligence: Cultural Accumulation in Reinforcement Learning",
      "url": "https://huggingface.co/papers/2406.00392",
      "authors": [
        "Jakob Foerster"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.00392.pdf",
      "abstract": "Cultural accumulation drives the open-ended and diverse progress in\ncapabilities spanning human history. It builds an expanding body of knowledge\nand skills by combining individual exploration with inter-generational\ninformation transmission. Despite its widespread success among humans, the\ncapacity for artificial learning agents to accumulate culture remains\nunder-explored. In particular, approaches to reinforcement learning typically\nstrive for improvements over only a single lifetime. Generational algorithms\nthat do exist fail to capture the open-ended, emergent nature of cultural\naccumulation, which allows individuals to trade-off innovation and imitation.\nBuilding on the previously demonstrated ability for reinforcement learning\nagents to perform social learning, we find that training setups which balance\nthis with independent learning give rise to cultural accumulation. These\naccumulating agents outperform those trained for a single lifetime with the\nsame cumulative experience. We explore this accumulation by constructing two\nmodels under two distinct notions of a generation: episodic generations, in\nwhich accumulation occurs via in-context learning and train-time generations,\nin which accumulation occurs via in-weights learning. In-context and in-weights\ncultural accumulation can be interpreted as analogous to knowledge and skill\naccumulation, respectively. To the best of our knowledge, this work is the\nfirst to present general models that achieve emergent cultural accumulation in\nreinforcement learning, opening up new avenues towards more open-ended learning\nsystems, as well as presenting new opportunities for modelling human culture.",
      "upvotes": 12
    },
    {
      "title": "ZeroSmooth: Training-free Diffuser Adaptation for High Frame Rate Video Generation",
      "url": "https://huggingface.co/papers/2406.00908",
      "authors": [
        "Shaoshu Yang",
        "Ying Shan",
        "Ran He"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.00908.pdf",
      "abstract": "Video generation has made remarkable progress in recent years, especially\nsince the advent of the video diffusion models. Many video generation models\ncan produce plausible synthetic videos, e.g., Stable Video Diffusion (SVD).\nHowever, most video models can only generate low frame rate videos due to the\nlimited GPU memory as well as the difficulty of modeling a large set of frames.\nThe training videos are always uniformly sampled at a specified interval for\ntemporal compression. Previous methods promote the frame rate by either\ntraining a video interpolation model in pixel space as a postprocessing stage\nor training an interpolation model in latent space for a specific base video\nmodel. In this paper, we propose a training-free video interpolation method for\ngenerative video diffusion models, which is generalizable to different models\nin a plug-and-play manner. We investigate the non-linearity in the feature\nspace of video diffusion models and transform a video model into a\nself-cascaded video diffusion model with incorporating the designed hidden\nstate correction modules. The self-cascaded architecture and the correction\nmodule are proposed to retain the temporal consistency between key frames and\nthe interpolated frames. Extensive evaluations are preformed on multiple\npopular video models to demonstrate the effectiveness of the propose method,\nespecially that our training-free method is even comparable to trained\ninterpolation models supported by huge compute resources and large-scale\ndatasets.",
      "upvotes": 11
    },
    {
      "title": "$Î¼$LO: Compute-Efficient Meta-Generalization of Learned Optimizers",
      "url": "https://huggingface.co/papers/2406.00153",
      "authors": [
        "Edouard Oyallon",
        "Eugene Belilovsky"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.00153.pdf",
      "abstract": "Learned optimizers (LOs) can significantly reduce the wall-clock training\ntime of neural networks, substantially reducing training costs. However, they\noften suffer from poor meta-generalization, especially when training networks\nlarger than those seen during meta-training. To address this, we use the\nrecently proposed Maximal Update Parametrization (muP), which allows\nzero-shot generalization of optimizer hyperparameters from smaller to larger\nmodels. We extend muP theory to learned optimizers, treating the\nmeta-training problem as finding the learned optimizer under muP. Our\nevaluation shows that LOs meta-trained with muP substantially improve\nmeta-generalization as compared to LOs trained under standard parametrization\n(SP). Notably, when applied to large-width models, our best muLO, trained\nfor 103 GPU-hours, matches or exceeds the performance of VeLO, the largest\npublicly available learned optimizer, meta-trained with 4000 TPU-months of\ncompute. Moreover, muLOs demonstrate better generalization than their SP\ncounterparts to deeper networks and to much longer training horizons (25 times\nlonger) than those seen during meta-training.",
      "upvotes": 9
    }
  ]
}