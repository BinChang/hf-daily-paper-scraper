{
  "date": "2024-08-13",
  "papers": [
    {
      "title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
      "url": "https://huggingface.co/papers/2408.06292",
      "authors": [
        "Jeff Clune"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.06292.pdf",
      "abstract": "One of the grand challenges of artificial general intelligence is developing\nagents capable of conducting scientific research and discovering new knowledge.\nWhile frontier models have already been used as aids to human scientists, e.g.\nfor brainstorming ideas, writing code, or prediction tasks, they still conduct\nonly a small part of the scientific process. This paper presents the first\ncomprehensive framework for fully automatic scientific discovery, enabling\nfrontier large language models to perform research independently and\ncommunicate their findings. We introduce The AI Scientist, which generates\nnovel research ideas, writes code, executes experiments, visualizes results,\ndescribes its findings by writing a full scientific paper, and then runs a\nsimulated review process for evaluation. In principle, this process can be\nrepeated to iteratively develop ideas in an open-ended fashion, acting like the\nhuman scientific community. We demonstrate its versatility by applying it to\nthree distinct subfields of machine learning: diffusion modeling,\ntransformer-based language modeling, and learning dynamics. Each idea is\nimplemented and developed into a full paper at a cost of less than $15 per\npaper. To evaluate the generated papers, we design and validate an automated\nreviewer, which we show achieves near-human performance in evaluating paper\nscores. The AI Scientist can produce papers that exceed the acceptance\nthreshold at a top machine learning conference as judged by our automated\nreviewer. This approach signifies the beginning of a new era in scientific\ndiscovery in machine learning: bringing the transformative benefits of AI\nagents to the entire research process of AI itself, and taking us closer to a\nworld where endless affordable creativity and innovation can be unleashed on\nthe world's most challenging problems. Our code is open-sourced at\nhttps://github.com/SakanaAI/AI-Scientist",
      "upvotes": 115
    },
    {
      "title": "Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers",
      "url": "https://huggingface.co/papers/2408.06195",
      "authors": [
        "Fan Yang",
        "Mao Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.06195.pdf",
      "abstract": "This paper introduces rStar, a self-play mutual reasoning approach that\nsignificantly improves reasoning capabilities of small language models (SLMs)\nwithout fine-tuning or superior models. rStar decouples reasoning into a\nself-play mutual generation-discrimination process. First, a target SLM\naugments the Monte Carlo Tree Search (MCTS) with a rich set of human-like\nreasoning actions to construct higher quality reasoning trajectories. Next,\nanother SLM, with capabilities similar to the target SLM, acts as a\ndiscriminator to verify each trajectory generated by the target SLM. The\nmutually agreed reasoning trajectories are considered mutual consistent, thus\nare more likely to be correct. Extensive experiments across five SLMs\ndemonstrate rStar can effectively solve diverse reasoning problems, including\nGSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K\naccuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for\nMistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be\navailable at https://github.com/zhentingqi/rStar.",
      "upvotes": 61
    },
    {
      "title": "ControlNeXt: Powerful and Efficient Control for Image and Video Generation",
      "url": "https://huggingface.co/papers/2408.06070",
      "authors": [
        "Jian Wang",
        "Wenbo Li",
        "Ming-Chang Yang",
        "Jiaya Jia"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.06070.pdf",
      "abstract": "Diffusion models have demonstrated remarkable and robust abilities in both\nimage and video generation. To achieve greater control over generated results,\nresearchers introduce additional architectures, such as ControlNet, Adapters\nand ReferenceNet, to integrate conditioning controls. However, current\ncontrollable generation methods often require substantial additional\ncomputational resources, especially for video generation, and face challenges\nin training or exhibit weak control. In this paper, we propose ControlNeXt: a\npowerful and efficient method for controllable image and video generation. We\nfirst design a more straightforward and efficient architecture, replacing heavy\nadditional branches with minimal additional cost compared to the base model.\nSuch a concise structure also allows our method to seamlessly integrate with\nother LoRA weights, enabling style alteration without the need for additional\ntraining. As for training, we reduce up to 90% of learnable parameters compared\nto the alternatives. Furthermore, we propose another method called Cross\nNormalization (CN) as a replacement for Zero-Convolution' to achieve fast and\nstable training convergence. We have conducted various experiments with\ndifferent base models across images and videos, demonstrating the robustness of\nour method.",
      "upvotes": 52
    },
    {
      "title": "Med42-v2: A Suite of Clinical LLMs",
      "url": "https://huggingface.co/papers/2408.06142",
      "authors": [
        "Shadab Khan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.06142.pdf",
      "abstract": "Med42-v2 introduces a suite of clinical large language models (LLMs) designed\nto address the limitations of generic models in healthcare settings. These\nmodels are built on Llama3 architecture and fine-tuned using specialized\nclinical data. They underwent multi-stage preference alignment to effectively\nrespond to natural prompts. While generic models are often preference-aligned\nto avoid answering clinical queries as a precaution, Med42-v2 is specifically\ntrained to overcome this limitation, enabling its use in clinical settings.\nMed42-v2 models demonstrate superior performance compared to the original\nLlama3 models in both 8B and 70B parameter configurations and GPT-4 across\nvarious medical benchmarks. These LLMs are developed to understand clinical\nqueries, perform reasoning tasks, and provide valuable assistance in clinical\nenvironments. The models are now publicly available at\nhttps://huggingface.co/m42-health{https://huggingface.co/m42-health}.",
      "upvotes": 50
    },
    {
      "title": "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer",
      "url": "https://huggingface.co/papers/2408.06072",
      "authors": [
        "Ming Ding",
        "Xiaohan Zhang",
        "Xiaotao Gu",
        "Weihan Wang",
        "Ting Liu",
        "Bin Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.06072.pdf",
      "abstract": "We introduce CogVideoX, a large-scale diffusion transformer model designed\nfor generating videos based on text prompts. To efficently model video data, we\npropose to levearge a 3D Variational Autoencoder (VAE) to compress videos along\nboth spatial and temporal dimensions. To improve the text-video alignment, we\npropose an expert transformer with the expert adaptive LayerNorm to facilitate\nthe deep fusion between the two modalities. By employing a progressive training\ntechnique, CogVideoX is adept at producing coherent, long-duration videos\ncharacterized by significant motions. In addition, we develop an effective\ntext-video data processing pipeline that includes various data preprocessing\nstrategies and a video captioning method. It significantly helps enhance the\nperformance of CogVideoX, improving both generation quality and semantic\nalignment. Results show that CogVideoX demonstrates state-of-the-art\nperformance across both multiple machine metrics and human evaluations. The\nmodel weights of both the 3D Causal VAE and CogVideoX are publicly available at\nhttps://github.com/THUDM/CogVideo.",
      "upvotes": 35
    },
    {
      "title": "FruitNeRF: A Unified Neural Radiance Field based Fruit Counting Framework",
      "url": "https://huggingface.co/papers/2408.06190",
      "authors": [
        "Andreas Gilson",
        "Ute Schmidt",
        "Marc Stamminger"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.06190.pdf",
      "abstract": "We introduce FruitNeRF, a unified novel fruit counting framework that\nleverages state-of-the-art view synthesis methods to count any fruit type\ndirectly in 3D. Our framework takes an unordered set of posed images captured\nby a monocular camera and segments fruit in each image. To make our system\nindependent of the fruit type, we employ a foundation model that generates\nbinary segmentation masks for any fruit. Utilizing both modalities, RGB and\nsemantic, we train a semantic neural radiance field. Through uniform volume\nsampling of the implicit Fruit Field, we obtain fruit-only point clouds. By\napplying cascaded clustering on the extracted point cloud, our approach\nachieves precise fruit count.The use of neural radiance fields provides\nsignificant advantages over conventional methods such as object tracking or\noptical flow, as the counting itself is lifted into 3D. Our method prevents\ndouble counting fruit and avoids counting irrelevant fruit.We evaluate our\nmethodology using both real-world and synthetic datasets. The real-world\ndataset consists of three apple trees with manually counted ground truths, a\nbenchmark apple dataset with one row and ground truth fruit location, while the\nsynthetic dataset comprises various fruit types including apple, plum, lemon,\npear, peach, and mango.Additionally, we assess the performance of fruit\ncounting using the foundation model compared to a U-Net.",
      "upvotes": 17
    },
    {
      "title": "VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents",
      "url": "https://huggingface.co/papers/2408.06327",
      "authors": [
        "Iat Long Iong",
        "Yifan Xu",
        "Xinyi Liu",
        "Xinyue Yang",
        "Xueqiao Sun",
        "Hao Yu",
        "Hanchen Zhang",
        "Ming Ding"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.06327.pdf",
      "abstract": "Large Multimodal Models (LMMs) have ushered in a new era in artificial\nintelligence, merging capabilities in both language and vision to form highly\ncapable Visual Foundation Agents. These agents are postulated to excel across a\nmyriad of tasks, potentially approaching general artificial intelligence.\nHowever, existing benchmarks fail to sufficiently challenge or showcase the\nfull potential of LMMs in complex, real-world environments. To address this\ngap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering\nbenchmark specifically designed to train and evaluate LMMs as visual foundation\nagents across diverse scenarios, including Embodied, Graphical User Interface,\nand Visual Design, with tasks formulated to probe the depth of LMMs'\nunderstanding and interaction capabilities. Through rigorous testing across\nnine proprietary LMM APIs and eight open models, we demonstrate the\nconsiderable yet still developing agent capabilities of these models.\nAdditionally, VAB constructs a trajectory training set constructed through\nhybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and\nHuman Demonstrations, promoting substantial performance improvements in LMMs\nthrough behavior cloning. Our work not only aims to benchmark existing models\nbut also provides a solid foundation for future development into visual\nfoundation agents. Code, train \\& test data, and part of fine-tuned open LMMs\nare available at https://github.com/THUDM/VisualAgentBench.",
      "upvotes": 15
    },
    {
      "title": "HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors",
      "url": "https://huggingface.co/papers/2408.06019",
      "authors": [
        "Weiyi Zhang",
        "Xu Chang",
        "Yang Zhao",
        "Zheng Lv",
        "Xiaoyuan Zhang",
        "Guidong Wang",
        "Lan Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.06019.pdf",
      "abstract": "In this paper, we present a novel 3D head avatar creation approach capable of\ngeneralizing from few-shot in-the-wild data with high-fidelity and animatable\nrobustness. Given the underconstrained nature of this problem, incorporating\nprior knowledge is essential. Therefore, we propose a framework comprising\nprior learning and avatar creation phases. The prior learning phase leverages\n3D head priors derived from a large-scale multi-view dynamic dataset, and the\navatar creation phase applies these priors for few-shot personalization. Our\napproach effectively captures these priors by utilizing a Gaussian\nSplatting-based auto-decoder network with part-based dynamic modeling. Our\nmethod employs identity-shared encoding with personalized latent codes for\nindividual identities to learn the attributes of Gaussian primitives. During\nthe avatar creation phase, we achieve fast head avatar personalization by\nleveraging inversion and fine-tuning strategies. Extensive experiments\ndemonstrate that our model effectively exploits head priors and successfully\ngeneralizes them to few-shot personalization, achieving photo-realistic\nrendering quality, multi-view consistency, and stable animation.",
      "upvotes": 13
    },
    {
      "title": "UniPortrait: A Unified Framework for Identity-Preserving Single- and Multi-Human Image Personalization",
      "url": "https://huggingface.co/papers/2408.05939",
      "authors": [
        "Junjie He"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.05939.pdf",
      "abstract": "This paper presents UniPortrait, an innovative human image personalization\nframework that unifies single- and multi-ID customization with high face\nfidelity, extensive facial editability, free-form input description, and\ndiverse layout generation. UniPortrait consists of only two plug-and-play\nmodules: an ID embedding module and an ID routing module. The ID embedding\nmodule extracts versatile editable facial features with a decoupling strategy\nfor each ID and embeds them into the context space of diffusion models. The ID\nrouting module then combines and distributes these embeddings adaptively to\ntheir respective regions within the synthesized image, achieving the\ncustomization of single and multiple IDs. With a carefully designed two-stage\ntraining scheme, UniPortrait achieves superior performance in both single- and\nmulti-ID customization. Quantitative and qualitative experiments demonstrate\nthe advantages of our method over existing approaches as well as its good\nscalability, e.g., the universal compatibility with existing generative control\ntools. The project page is at\nhttps://aigcdesigngroup.github.io/UniPortrait-Page/ .",
      "upvotes": 13
    },
    {
      "title": "Your Context Is Not an Array: Unveiling Random Access Limitations in Transformers",
      "url": "https://huggingface.co/papers/2408.05506",
      "authors": [
        "Sunny Panchal",
        "Roland Memisevic"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.05506.pdf",
      "abstract": "Despite their recent successes, Transformer-based large language models show\nsurprising failure modes. A well-known example of such failure modes is their\ninability to length-generalize: solving problem instances at inference time\nthat are longer than those seen during training. In this work, we further\nexplore the root cause of this failure by performing a detailed analysis of\nmodel behaviors on the simple parity task. Our analysis suggests that length\ngeneralization failures are intricately related to a model's inability to\nperform random memory accesses within its context window. We present supporting\nevidence for this hypothesis by demonstrating the effectiveness of\nmethodologies that circumvent the need for indexing or that enable random token\naccess indirectly, through content-based addressing. We further show where and\nhow the failure to perform random memory access manifests through attention map\nvisualizations.",
      "upvotes": 8
    },
    {
      "title": "Body Transformer: Leveraging Robot Embodiment for Policy Learning",
      "url": "https://huggingface.co/papers/2408.06316",
      "authors": [
        "Dun-Ming Huang",
        "Pieter Abbeel"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.06316.pdf",
      "abstract": "In recent years, the transformer architecture has become the de facto\nstandard for machine learning algorithms applied to natural language processing\nand computer vision. Despite notable evidence of successful deployment of this\narchitecture in the context of robot learning, we claim that vanilla\ntransformers do not fully exploit the structure of the robot learning problem.\nTherefore, we propose Body Transformer (BoT), an architecture that leverages\nthe robot embodiment by providing an inductive bias that guides the learning\nprocess. We represent the robot body as a graph of sensors and actuators, and\nrely on masked attention to pool information throughout the architecture. The\nresulting architecture outperforms the vanilla transformer, as well as the\nclassical multilayer perceptron, in terms of task completion, scaling\nproperties, and computational efficiency when representing either imitation or\nreinforcement learning policies. Additional material including the open-source\ncode is available at https://sferrazza.cc/bot_site.",
      "upvotes": 8
    }
  ]
}