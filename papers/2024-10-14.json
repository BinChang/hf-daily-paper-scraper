{
  "date": "2024-10-14",
  "papers": [
    {
      "title": "Baichuan-Omni Technical Report",
      "url": "https://huggingface.co/papers/2410.08565",
      "authors": [
        "Yadong Li",
        "Haoze Sun",
        "Tao Zhang",
        "Wei Song",
        "Yuqi Huo",
        "Song Chen",
        "Xu Li",
        "Da Pan",
        "Xin Wu",
        "Zheng Liang",
        "Jun Liu",
        "Tao Zhang",
        "Keer Lu",
        "Yaqi Zhao",
        "Fan Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08565.pdf",
      "abstract": "The salient multimodal capabilities and interactive experience of GPT-4o\nhighlight its critical role in practical applications, yet it lacks a\nhigh-performing open-source counterpart. In this paper, we introduce\nBaichuan-Omni, the first open-source 7B Multimodal Large Language Model (MLLM)\nadept at concurrently processing and analyzing modalities of image, video,\naudio, and text, while delivering an advanced multimodal interactive experience\nand strong performance. We propose an effective multimodal training schema\nstarting with 7B model and proceeding through two stages of multimodal\nalignment and multitask fine-tuning across audio, image, video, and text modal.\nThis approach equips the language model with the ability to handle visual and\naudio data effectively. Demonstrating strong performance across various\nomni-modal and multimodal benchmarks, we aim for this contribution to serve as\na competitive baseline for the open-source community in advancing multimodal\nunderstanding and real-time interaction.",
      "upvotes": 82
    },
    {
      "title": "Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis",
      "url": "https://huggingface.co/papers/2410.08261",
      "authors": [
        "Qing-Guo Chen",
        "Lei Zhu",
        "Shuicheng Yan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08261.pdf",
      "abstract": "Diffusion models, such as Stable Diffusion, have made significant strides in\nvisual generation, yet their paradigm remains fundamentally different from\nautoregressive language models, complicating the development of unified\nlanguage-vision models. Recent efforts like LlamaGen have attempted\nautoregressive image generation using discrete VQVAE tokens, but the large\nnumber of tokens involved renders this approach inefficient and slow. In this\nwork, we present Meissonic, which elevates non-autoregressive masked image\nmodeling (MIM) text-to-image to a level comparable with state-of-the-art\ndiffusion models like SDXL. By incorporating a comprehensive suite of\narchitectural innovations, advanced positional encoding strategies, and\noptimized sampling conditions, Meissonic substantially improves MIM's\nperformance and efficiency. Additionally, we leverage high-quality training\ndata, integrate micro-conditions informed by human preference scores, and\nemploy feature compression layers to further enhance image fidelity and\nresolution. Our model not only matches but often exceeds the performance of\nexisting models like SDXL in generating high-quality, high-resolution images.\nExtensive experiments validate Meissonic's capabilities, demonstrating its\npotential as a new standard in text-to-image synthesis. We release a model\ncheckpoint capable of producing 1024 times 1024 resolution images.",
      "upvotes": 48
    },
    {
      "title": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization",
      "url": "https://huggingface.co/papers/2410.08815",
      "authors": [
        "Xuanang Chen",
        "Haiyang Yu",
        "Hongyu Lin",
        "Yaojie Lu",
        "Fei Huang",
        "Le Sun"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08815.pdf",
      "abstract": "Retrieval-augmented generation (RAG) is a key means to effectively enhance\nlarge language models (LLMs) in many knowledge-based tasks. However, existing\nRAG methods struggle with knowledge-intensive reasoning tasks, because useful\ninformation required to these tasks are badly scattered. This characteristic\nmakes it difficult for existing RAG methods to accurately identify key\ninformation and perform global reasoning with such noisy augmentation. In this\npaper, motivated by the cognitive theories that humans convert raw information\ninto various structured knowledge when tackling knowledge-intensive reasoning,\nwe proposes a new framework, StructRAG, which can identify the optimal\nstructure type for the task at hand, reconstruct original documents into this\nstructured format, and infer answers based on the resulting structure.\nExtensive experiments across various knowledge-intensive tasks show that\nStructRAG achieves state-of-the-art performance, particularly excelling in\nchallenging scenarios, demonstrating its potential as an effective solution for\nenhancing LLMs in complex real-world applications.",
      "upvotes": 41
    },
    {
      "title": "From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning",
      "url": "https://huggingface.co/papers/2410.06456",
      "authors": [
        "Jun Zhou",
        "Rick Siow Mong Goh",
        "Daniel Shu Wei Ting",
        "Yong Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06456.pdf",
      "abstract": "Large vision language models (VLMs) combine large language models with vision\nencoders, demonstrating promise across various tasks. However, they often\nunderperform in task-specific applications due to domain gaps between\npre-training and fine-tuning. We introduce VITask, a novel framework that\nenhances task-specific adaptability of VLMs by integrating task-specific models\n(TSMs). VITask employs three key strategies: exemplar prompting (EP), response\ndistribution alignment (RDA), and contrastive response tuning (CRT) to improve\nthe task-specific performance of VLMs by adjusting their response\ndistributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to\nadapt without TSMs during inference by learning from exemplar-prompted models.\nCRT further optimizes the ranking of correct image-response pairs, thereby\nreducing the risk of generating undesired responses. Experiments on 12 medical\ndiagnosis datasets across 9 imaging modalities show that VITask outperforms\nboth vanilla instruction-tuned VLMs and TSMs, showcasing its ability to\nintegrate complementary features from both models effectively. Additionally,\nVITask offers practical advantages such as flexible TSM integration and\nrobustness to incomplete instructions, making it a versatile and efficient\nsolution for task-specific VLM tuning. Our code are available at\nhttps://github.com/baiyang4/VITask.",
      "upvotes": 35
    },
    {
      "title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining",
      "url": "https://huggingface.co/papers/2410.08102",
      "authors": [
        "Chi Zhang",
        "Lijun Wu",
        "Wentao Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08102.pdf",
      "abstract": "Efficient data selection is crucial to accelerate the pretraining of large\nlanguage models (LLMs). While various methods have been proposed to enhance\ndata efficiency, limited research has addressed the inherent conflicts between\nthese approaches to achieve optimal data selection for LLM pretraining. To\ntackle this problem, we propose a novel multi-agent collaborative data\nselection mechanism. In this framework, each data selection method serves as an\nindependent agent, and an agent console is designed to dynamically integrate\nthe information from all agents throughout the LLM training process. We conduct\nextensive empirical studies to evaluate our multi-agent framework. The\nexperimental results demonstrate that our approach significantly improves data\nefficiency, accelerates convergence in LLM training, and achieves an average\nperformance gain of 10.5% across multiple language model benchmarks compared to\nthe state-of-the-art methods.",
      "upvotes": 19
    },
    {
      "title": "EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models",
      "url": "https://huggingface.co/papers/2410.07133",
      "authors": [
        "Yuchao Gu",
        "Lingmin Ran",
        "Zhangjie Wu",
        "Junhao Zhang",
        "Yingya Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07133.pdf",
      "abstract": "Recent advancements in generation models have showcased remarkable\ncapabilities in generating fantastic content. However, most of them are trained\non proprietary high-quality data, and some models withhold their parameters and\nonly provide accessible application programming interfaces (APIs), limiting\ntheir benefits for downstream tasks. To explore the feasibility of training a\ntext-to-image generation model comparable to advanced models using publicly\navailable resources, we introduce EvolveDirector. This framework interacts with\nadvanced models through their public APIs to obtain text-image data pairs to\ntrain a base model. Our experiments with extensive data indicate that the model\ntrained on generated data of the advanced model can approximate its generation\ncapability. However, it requires large-scale samples of 10 million or more.\nThis incurs significant expenses in time, computational resources, and\nespecially the costs associated with calling fee-based APIs. To address this\nproblem, we leverage pre-trained large vision-language models (VLMs) to guide\nthe evolution of the base model. VLM continuously evaluates the base model\nduring training and dynamically updates and refines the training dataset by the\ndiscrimination, expansion, deletion, and mutation operations. Experimental\nresults show that this paradigm significantly reduces the required data volume.\nFurthermore, when approaching multiple advanced models, EvolveDirector can\nselect the best samples generated by them to learn powerful and balanced\nabilities. The final trained model Edgen is demonstrated to outperform these\nadvanced models. The code and model weights are available at\nhttps://github.com/showlab/EvolveDirector.",
      "upvotes": 18
    },
    {
      "title": "Mechanistic Permutability: Match Features Across Layers",
      "url": "https://huggingface.co/papers/2410.07656",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.07656.pdf",
      "abstract": "Understanding how features evolve across layers in deep neural networks is a\nfundamental challenge in mechanistic interpretability, particularly due to\npolysemanticity and feature superposition. While Sparse Autoencoders (SAEs)\nhave been used to extract interpretable features from individual layers,\naligning these features across layers has remained an open problem. In this\npaper, we introduce SAE Match, a novel, data-free method for aligning SAE\nfeatures across different layers of a neural network. Our approach involves\nmatching features by minimizing the mean squared error between the folded\nparameters of SAEs, a technique that incorporates activation thresholds into\nthe encoder and decoder weights to account for differences in feature scales.\nThrough extensive experiments on the Gemma 2 language model, we demonstrate\nthat our method effectively captures feature evolution across layers, improving\nfeature matching quality. We also show that features persist over several\nlayers and that our approach can approximate hidden states across layers. Our\nwork advances the understanding of feature dynamics in neural networks and\nprovides a new tool for mechanistic interpretability studies.",
      "upvotes": 16
    },
    {
      "title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness",
      "url": "https://huggingface.co/papers/2410.07035",
      "authors": [
        "Zekun Wang",
        "Feiyu Duan",
        "Yibo Zhang",
        "Ke Xu",
        "Jie Fu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07035.pdf",
      "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious domains, including role-playing, creative writing, mathematical\nreasoning, and coding. Despite these advancements, LLMs still encounter\nchallenges with length control, frequently failing to adhere to specific length\nconstraints due to their token-level operations and insufficient training on\ndata with strict length limitations. We identify this issue as stemming from a\nlack of positional awareness and propose novel approaches--PositionID Prompting\nand PositionID Fine-Tuning--to address it. These methods enhance the model's\nability to continuously monitor and manage text length during generation.\nAdditionally, we introduce PositionID CP Prompting to enable LLMs to perform\ncopy and paste operations accurately. Furthermore, we develop two benchmarks\nfor evaluating length control and copy-paste abilities. Our experiments\ndemonstrate that our methods significantly improve the model's adherence to\nlength constraints and copy-paste accuracy without compromising response\nquality.",
      "upvotes": 16
    },
    {
      "title": "SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights",
      "url": "https://huggingface.co/papers/2410.09008",
      "authors": [
        "Bin Cui",
        "Shuicheng Yan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.09008.pdf",
      "abstract": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown\nsignificant improvements in various reasoning tasks. However, smaller models\nsuch as Llama-3-8B and DeepSeekMath-Base still struggle with complex\nmathematical reasoning because they fail to effectively identify and correct\nreasoning errors. Recent reflection-based methods aim to address these issues\nby enabling self-reflection and self-correction, but they still face challenges\nin independently detecting errors in their reasoning steps. To overcome these\nlimitations, we propose SuperCorrect, a novel two-stage framework that uses a\nlarge teacher model to supervise and correct both the reasoning and reflection\nprocesses of a smaller student model. In the first stage, we extract\nhierarchical high-level and detailed thought templates from the teacher model\nto guide the student model in eliciting more fine-grained reasoning thoughts.\nIn the second stage, we introduce cross-model collaborative direct preference\noptimization (DPO) to enhance the self-correction abilities of the student\nmodel by following the teacher's correction traces during training. This\ncross-model DPO approach teaches the student model to effectively locate and\nresolve erroneous thoughts with error-driven insights from the teacher model,\nbreaking the bottleneck of its thoughts and acquiring new skills and knowledge\nto tackle challenging problems. Extensive experiments consistently demonstrate\nour superiority over previous methods. Notably, our SuperCorrect-7B model\nsignificantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and\nQwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA\nperformance among all 7B models. Code:\nhttps://github.com/YangLing0818/SuperCorrect-llm",
      "upvotes": 16
    },
    {
      "title": "Semantic Score Distillation Sampling for Compositional Text-to-3D Generation",
      "url": "https://huggingface.co/papers/2410.09009",
      "authors": [
        "Zixiang Zhang",
        "Junlin Han",
        "Wentao Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.09009.pdf",
      "abstract": "Generating high-quality 3D assets from textual descriptions remains a pivotal\nchallenge in computer graphics and vision research. Due to the scarcity of 3D\ndata, state-of-the-art approaches utilize pre-trained 2D diffusion priors,\noptimized through Score Distillation Sampling (SDS). Despite progress, crafting\ncomplex 3D scenes featuring multiple objects or intricate interactions is still\ndifficult. To tackle this, recent methods have incorporated box or layout\nguidance. However, these layout-guided compositional methods often struggle to\nprovide fine-grained control, as they are generally coarse and lack\nexpressiveness. To overcome these challenges, we introduce a novel SDS\napproach, Semantic Score Distillation Sampling (SemanticSDS), designed to\neffectively improve the expressiveness and accuracy of compositional text-to-3D\ngeneration. Our approach integrates new semantic embeddings that maintain\nconsistency across different rendering views and clearly differentiate between\nvarious objects and parts. These embeddings are transformed into a semantic\nmap, which directs a region-specific SDS process, enabling precise optimization\nand compositional generation. By leveraging explicit semantic guidance, our\nmethod unlocks the compositional capabilities of existing pre-trained diffusion\nmodels, thereby achieving superior quality in 3D content generation,\nparticularly for complex objects and scenes. Experimental results demonstrate\nthat our SemanticSDS framework is highly effective for generating\nstate-of-the-art complex 3D content. Code:\nhttps://github.com/YangLing0818/SemanticSDS-3D",
      "upvotes": 13
    },
    {
      "title": "KV Prediction for Improved Time to First Token",
      "url": "https://huggingface.co/papers/2410.08391",
      "authors": [
        "Sachin Mehta"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08391.pdf",
      "abstract": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of 15%-50% across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to 30% on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction .",
      "upvotes": 11
    },
    {
      "title": "Think While You Generate: Discrete Diffusion with Planned Denoising",
      "url": "https://huggingface.co/papers/2410.06264",
      "authors": [
        "Juno Nam",
        "Andrew Campbell",
        "Hannes Stärk",
        "Yilun Xu",
        "Tommi Jaakkola",
        "Rafael Gómez-Bombarelli"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06264.pdf",
      "abstract": "Discrete diffusion has achieved state-of-the-art performance, outperforming\nor approaching autoregressive models on standard benchmarks. In this work, we\nintroduce Discrete Diffusion with Planned Denoising (DDPD), a novel framework\nthat separates the generation process into two models: a planner and a\ndenoiser. At inference time, the planner selects which positions to denoise\nnext by identifying the most corrupted positions in need of denoising,\nincluding both initially corrupted and those requiring additional refinement.\nThis plan-and-denoise approach enables more efficient reconstruction during\ngeneration by iteratively identifying and denoising corruptions in the optimal\norder. DDPD outperforms traditional denoiser-only mask diffusion methods,\nachieving superior results on language modeling benchmarks such as text8,\nOpenWebText, and token-based generation on ImageNet 256 times 256. Notably,\nin language modeling, DDPD significantly reduces the performance gap between\ndiffusion-based and autoregressive methods in terms of generative perplexity.\nCode is available at https://github.com/liusulin/DDPD.",
      "upvotes": 9
    },
    {
      "title": "ZeroComp: Zero-shot Object Compositing from Image Intrinsics via Diffusion",
      "url": "https://huggingface.co/papers/2410.08168",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.08168.pdf",
      "abstract": "We present ZeroComp, an effective zero-shot 3D object compositing approach\nthat does not require paired composite-scene images during training. Our method\nleverages ControlNet to condition from intrinsic images and combines it with a\nStable Diffusion model to utilize its scene priors, together operating as an\neffective rendering engine. During training, ZeroComp uses intrinsic images\nbased on geometry, albedo, and masked shading, all without the need for paired\nimages of scenes with and without composite objects. Once trained, it\nseamlessly integrates virtual 3D objects into scenes, adjusting shading to\ncreate realistic composites. We developed a high-quality evaluation dataset and\ndemonstrate that ZeroComp outperforms methods using explicit lighting\nestimations and generative techniques in quantitative and human perception\nbenchmarks. Additionally, ZeroComp extends to real and outdoor image\ncompositing, even when trained solely on synthetic indoor data, showcasing its\neffectiveness in image compositing.",
      "upvotes": 7
    },
    {
      "title": "I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow Transformers with Projected Flow",
      "url": "https://huggingface.co/papers/2410.07536",
      "authors": [
        "Le Zhuo",
        "Qin Qi",
        "Hongsheng Li",
        "Zhanyu Ma",
        "Peng Gao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07536.pdf",
      "abstract": "Rectified Flow Transformers (RFTs) offer superior training and inference\nefficiency, making them likely the most viable direction for scaling up\ndiffusion models. However, progress in generation resolution has been\nrelatively slow due to data quality and training costs. Tuning-free resolution\nextrapolation presents an alternative, but current methods often reduce\ngenerative stability, limiting practical application. In this paper, we review\nexisting resolution extrapolation methods and introduce the I-Max framework to\nmaximize the resolution potential of Text-to-Image RFTs. I-Max features: (i) a\nnovel Projected Flow strategy for stable extrapolation and (ii) an advanced\ninference toolkit for generalizing model knowledge to higher resolutions.\nExperiments with Lumina-Next-2K and Flux.1-dev demonstrate I-Max's ability to\nenhance stability in resolution extrapolation and show that it can bring image\ndetail emergence and artifact correction, confirming the practical value of\ntuning-free resolution extrapolation.",
      "upvotes": 5
    },
    {
      "title": "SimpleStrat: Diversifying Language Model Generation with Stratification",
      "url": "https://huggingface.co/papers/2410.09038",
      "authors": [
        "Justin Wong",
        "Yury Orlovskiy",
        "Michael Luo",
        "Sanjit A. Seshia",
        "Joseph E. Gonzalez"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.09038.pdf",
      "abstract": "Generating diverse responses from large language models (LLMs) is crucial for\napplications such as planning/search and synthetic data generation, where\ndiversity provides distinct answers across generations. Prior approaches rely\non increasing temperature to increase diversity. However, contrary to popular\nbelief, we show not only does this approach produce lower quality individual\ngenerations as temperature increases, but it depends on model's next-token\nprobabilities being similar to the true distribution of answers. We propose\n, an alternative approach that uses the language model itself to\npartition the space into strata. At inference, a random stratum is selected and\na sample drawn from within the strata. To measure diversity, we introduce\nCoverageQA, a dataset of underspecified questions with multiple equally\nplausible answers, and assess diversity by measuring KL Divergence between the\noutput distribution and uniform distribution over valid ground truth answers.\nAs computing probability per response/solution for proprietary models is\ninfeasible, we measure recall on ground truth solutions. Our evaluation show\nusing SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36\naverage reduction in KL Divergence compared to Llama 3.",
      "upvotes": 4
    },
    {
      "title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection",
      "url": "https://huggingface.co/papers/2410.09045",
      "authors": [
        "Runsheng Huang",
        "Yue Yang",
        "Chris Callison-Burch"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.09045.pdf",
      "abstract": "The proliferation of inflammatory or misleading \"fake\" news content has\nbecome increasingly common in recent years. Simultaneously, it has become\neasier than ever to use AI tools to generate photorealistic images depicting\nany scene imaginable. Combining these two -- AI-generated fake news content --\nis particularly potent and dangerous. To combat the spread of AI-generated fake\nnews, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real\nand AI-generated image-caption pairs from state-of-the-art generators. We find\nthat our dataset poses a significant challenge to humans (60% F-1) and\nstate-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a\nmulti-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art\nbaselines on image-caption pairs from out-of-domain image generators and news\npublishers. We release our code and data to aid future work on detecting\nAI-generated content.",
      "upvotes": 4
    },
    {
      "title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners",
      "url": "https://huggingface.co/papers/2410.09037",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.09037.pdf",
      "abstract": "Large Language Models (LLMs) have displayed remarkable performances across\nvarious complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently,\nstudies have proposed a Knowledge Distillation (KD) approach, reasoning\ndistillation, which transfers such reasoning ability of LLMs through\nfine-tuning language models of multi-step rationales generated by LLM teachers.\nHowever, they have inadequately considered two challenges regarding\ninsufficient distillation sets from the LLM teacher model, in terms of 1) data\nquality and 2) soft label provision. In this paper, we propose Mentor-KD, which\neffectively distills the multi-step reasoning capability of LLMs to smaller LMs\nwhile addressing the aforementioned challenges. Specifically, we exploit a\nmentor, intermediate-sized task-specific fine-tuned model, to augment\nadditional CoT annotations and provide soft labels for the student model during\nreasoning distillation. We conduct extensive experiments and confirm\nMentor-KD's effectiveness across various models and complex reasoning tasks.",
      "upvotes": 4
    },
    {
      "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models",
      "url": "https://huggingface.co/papers/2410.07331",
      "authors": [
        "Yiming Huang",
        "Yan Yu",
        "Yitong Zhang",
        "Shizhu He",
        "Lifu Huang",
        "Jun Zhao",
        "Kang Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07331.pdf",
      "abstract": "We introduce DA-Code, a code generation benchmark specifically designed to\nassess LLMs on agent-based data science tasks. This benchmark features three\ncore elements: First, the tasks within DA-Code are inherently challenging,\nsetting them apart from traditional code generation tasks and demanding\nadvanced coding skills in grounding and planning. Second, examples in DA-Code\nare all based on real and diverse data, covering a wide range of complex data\nwrangling and analytics tasks. Third, to solve the tasks, the models must\nutilize complex data science programming languages, to perform intricate data\nprocessing and derive the answers. We set up the benchmark in a controllable\nand executable environment that aligns with real-world data analysis scenarios\nand is scalable. The annotators meticulously design the evaluation suite to\nensure the accuracy and robustness of the evaluation. We develop the DA-Agent\nbaseline. Experiments show that although the baseline performs better than\nother existing frameworks, using the current best LLMs achieves only 30.5%\naccuracy, leaving ample room for improvement. We release our benchmark at\nhttps://da-code-bench.github.io.",
      "upvotes": 4
    },
    {
      "title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment",
      "url": "https://huggingface.co/papers/2410.08193",
      "authors": [
        "Udari Madhushani Sehwag",
        "Alec Koppel",
        "Sicheng Zhu",
        "Sumitra Ganesh"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08193.pdf",
      "abstract": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining.",
      "upvotes": 3
    },
    {
      "title": "Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting",
      "url": "https://huggingface.co/papers/2410.08612",
      "authors": [
        "Kamal Basha",
        "Athira Nambiar"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08612.pdf",
      "abstract": "Sonar image synthesis is crucial for advancing applications in underwater\nexploration, marine biology, and defence. Traditional methods often rely on\nextensive and costly data collection using sonar sensors, jeopardizing data\nquality and diversity. To overcome these limitations, this study proposes a new\nsonar image synthesis framework, Synth-SONAR leveraging diffusion models and\nGPT prompting. The key novelties of Synth-SONAR are threefold: First, by\nintegrating Generative AI-based style injection techniques along with publicly\navailable real/simulated data, thereby producing one of the largest sonar data\ncorpus for sonar research. Second, a dual text-conditioning sonar diffusion\nmodel hierarchy synthesizes coarse and fine-grained sonar images with enhanced\nquality and diversity. Third, high-level (coarse) and low-level (detailed)\ntext-based sonar generation methods leverage advanced semantic information\navailable in visual language models (VLMs) and GPT-prompting. During inference,\nthe method generates diverse and realistic sonar images from textual prompts,\nbridging the gap between textual descriptions and sonar image generation. This\nmarks the application of GPT-prompting in sonar imagery for the first time, to\nthe best of our knowledge. Synth-SONAR achieves state-of-the-art results in\nproducing high-quality synthetic sonar datasets, significantly enhancing their\ndiversity and realism.",
      "upvotes": 1
    }
  ]
}