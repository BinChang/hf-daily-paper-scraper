{
  "date": "2024-09-24",
  "papers": [
    {
      "title": "RACER: Rich Language-Guided Failure Recovery Policies for Imitation Learning",
      "url": "https://huggingface.co/papers/2409.14674",
      "authors": [
        "Joyce Chai"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14674.pdf",
      "abstract": "Developing robust and correctable visuomotor policies for robotic\nmanipulation is challenging due to the lack of self-recovery mechanisms from\nfailures and the limitations of simple language instructions in guiding robot\nactions. To address these issues, we propose a scalable data generation\npipeline that automatically augments expert demonstrations with failure\nrecovery trajectories and fine-grained language annotations for training. We\nthen introduce Rich languAge-guided failure reCovERy (RACER), a\nsupervisor-actor framework, which combines failure recovery data with rich\nlanguage descriptions to enhance robot control. RACER features a\nvision-language model (VLM) that acts as an online supervisor, providing\ndetailed language guidance for error correction and task execution, and a\nlanguage-conditioned visuomotor policy as an actor to predict the next actions.\nOur experimental results show that RACER outperforms the state-of-the-art\nRobotic View Transformer (RVT) on RLbench across various evaluation settings,\nincluding standard long-horizon tasks, dynamic goal-change tasks and zero-shot\nunseen tasks, achieving superior performance in both simulated and real world\nenvironments. Videos and code are available at:\nhttps://rich-language-failure-recovery.github.io.",
      "upvotes": 41
    },
    {
      "title": "A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?",
      "url": "https://huggingface.co/papers/2409.15277",
      "authors": [
        "Juncheng Wu",
        "Haoqin Tu",
        "Siwei Yang",
        "Qiao Jin",
        "Cihang Xie",
        "Yuyin Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15277.pdf",
      "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across\nvarious domains and tasks, pushing the boundaries of our knowledge in learning\nand cognition. The latest model, OpenAI's o1, stands out as the first LLM with\nan internalized chain-of-thought technique using reinforcement learning\nstrategies. While it has demonstrated surprisingly strong capabilities on\nvarious general language tasks, its performance in specialized fields such as\nmedicine remains unknown. To this end, this report provides a comprehensive\nexploration of o1 on different medical scenarios, examining 3 key aspects:\nunderstanding, reasoning, and multilinguality. Specifically, our evaluation\nencompasses 6 tasks using data from 37 medical datasets, including two newly\nconstructed and more challenging question-answering (QA) tasks based on\nprofessional medical quizzes from the New England Journal of Medicine (NEJM)\nand The Lancet. These datasets offer greater clinical relevance compared to\nstandard medical QA benchmarks such as MedQA, translating more effectively into\nreal-world clinical utility. Our analysis of o1 suggests that the enhanced\nreasoning ability of LLMs may (significantly) benefit their capability to\nunderstand various medical instructions and reason through complex clinical\nscenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an average\nof 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios.\nBut meanwhile, we identify several weaknesses in both the model capability and\nthe existing evaluation protocols, including hallucination, inconsistent\nmultilingual ability, and discrepant metrics for evaluation. We release our raw\ndata and model outputs at https://ucsc-vlaa.github.io/o1_medicine/ for future\nresearch.",
      "upvotes": 34
    },
    {
      "title": "Phantom of Latent for Large Language and Vision Models",
      "url": "https://huggingface.co/papers/2409.14713",
      "authors": [
        "Sangyun Chung",
        "Beomchan Park",
        "Yong Man Ro"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14713.pdf",
      "abstract": "The success of visual instruction tuning has accelerated the development of\nlarge language and vision models (LLVMs). Following the scaling laws of\ninstruction-tuned large language models (LLMs), LLVMs either have further\nincreased their sizes, reaching 26B, 34B, and even 80B parameters. While this\nincrease in model size has yielded significant performance gains, it demands\nsubstantially more hardware resources for both training and inference.\nConsequently, there naturally exists a strong need for efficient LLVMs that\nachieve the performance of larger models while being smaller in size. To\nachieve this need, we present a new efficient LLVM family with model sizes of\n0.5B, 1.8B, 3.8B, and 7B parameters, Phantom, which significantly enhances\nlearning capabilities within limited structures. By temporarily increasing the\nlatent hidden dimension during multi-head self-attention (MHSA), we make LLVMs\nprepare to look and understand much more vision-language knowledge on the\nlatent, without substantially increasing physical model sizes. To maximize its\nadvantage, we introduce Phantom Optimization (PO) using both autoregressive\nsupervised fine-tuning (SFT) and direct preference optimization (DPO)-like\nconcept, which effectively follows correct answers while eliminating incorrect\nand ambiguous ones. Phantom outperforms numerous larger open- and closed-source\nLLVMs, positioning itself as a leading solution in the landscape of efficient\nLLVMs.",
      "upvotes": 27
    },
    {
      "title": "PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language Instructions",
      "url": "https://huggingface.co/papers/2409.15278",
      "authors": [
        "Xinyu Wei",
        "Renrui Zhang",
        "Le Zhuo",
        "Junlin Xie",
        "Yu Qiao",
        "Peng Gao",
        "Hongsheng Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15278.pdf",
      "abstract": "This paper presents a versatile image-to-image visual assistant, PixWizard,\ndesigned for image generation, manipulation, and translation based on free-from\nlanguage instructions. To this end, we tackle a variety of vision tasks into a\nunified image-text-to-image generation framework and curate an Omni\nPixel-to-Pixel Instruction-Tuning Dataset. By constructing detailed instruction\ntemplates in natural language, we comprehensively include a large set of\ndiverse vision tasks such as text-to-image generation, image restoration, image\ngrounding, dense image prediction, image editing, controllable generation,\ninpainting/outpainting, and more. Furthermore, we adopt Diffusion Transformers\n(DiT) as our foundation model and extend its capabilities with a flexible any\nresolution mechanism, enabling the model to dynamically process images based on\nthe aspect ratio of the input, closely aligning with human perceptual\nprocesses. The model also incorporates structure-aware and semantic-aware\nguidance to facilitate effective fusion of information from the input image.\nOur experiments demonstrate that PixWizard not only shows impressive generative\nand understanding abilities for images with diverse resolutions but also\nexhibits promising generalization capabilities with unseen tasks and human\ninstructions. The code and related resources are available at\nhttps://github.com/AFeng-x/PixWizard",
      "upvotes": 22
    },
    {
      "title": "Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs",
      "url": "https://huggingface.co/papers/2409.14988",
      "authors": [
        "Tathagata Raha",
        "Muhammad Umar Salman",
        "Shadab Khan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14988.pdf",
      "abstract": "Large Language Models (LLMs) have demonstrated significant potential in\ntransforming clinical applications. In this study, we investigate the efficacy\nof four techniques in adapting LLMs for clinical use-cases: continuous\npretraining, instruct fine-tuning, NEFTune, and prompt engineering. We employ\nthese methods on Mistral 7B and Mixtral 8x7B models, leveraging a large-scale\nclinical pretraining dataset of 50 billion tokens and an instruct fine-tuning\ndataset of 500 million tokens. Our evaluation across various clinical tasks\nreveals the impact of each technique. While continuous pretraining beyond 250\nbillion tokens yields marginal improvements on its own, it establishes a strong\nfoundation for instruct fine-tuning. Notably, NEFTune, designed primarily to\nenhance generation quality, surprisingly demonstrates additional gains on our\nbenchmark. Complex prompt engineering methods further enhance performance.\nThese findings show the importance of tailoring fine-tuning strategies and\nexploring innovative techniques to optimize LLM performance in the clinical\ndomain.",
      "upvotes": 21
    },
    {
      "title": "Reflecting Reality: Enabling Diffusion Models to Produce Faithful Mirror Reflections",
      "url": "https://huggingface.co/papers/2409.14677",
      "authors": [
        "Ankit Dhiman",
        "Rishubh Parihar",
        "Lokesh R Boregowda",
        "R Venkatesh Babu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14677.pdf",
      "abstract": "We tackle the problem of generating highly realistic and plausible mirror\nreflections using diffusion-based generative models. We formulate this problem\nas an image inpainting task, allowing for more user control over the placement\nof mirrors during the generation process. To enable this, we create SynMirror,\na large-scale dataset of diverse synthetic scenes with objects placed in front\nof mirrors. SynMirror contains around 198K samples rendered from 66K unique 3D\nobjects, along with their associated depth maps, normal maps and instance-wise\nsegmentation masks, to capture relevant geometric properties of the scene.\nUsing this dataset, we propose a novel depth-conditioned inpainting method\ncalled MirrorFusion, which generates high-quality geometrically consistent and\nphoto-realistic mirror reflections given an input image and a mask depicting\nthe mirror region. MirrorFusion outperforms state-of-the-art methods on\nSynMirror, as demonstrated by extensive quantitative and qualitative analysis.\nTo the best of our knowledge, we are the first to successfully tackle the\nchallenging problem of generating controlled and faithful mirror reflections of\nan object in a scene using diffusion based models. SynMirror and MirrorFusion\nopen up new avenues for image editing and augmented reality applications for\npractitioners and researchers alike.",
      "upvotes": 14
    },
    {
      "title": "Style over Substance: Failure Modes of LLM Judges in Alignment Benchmarking",
      "url": "https://huggingface.co/papers/2409.15268",
      "authors": [
        "Micah Goldblum",
        "Teresa Datta",
        "Sanjana Nambiar",
        "Raz Besaleli",
        "Samuel Dooley",
        "Max Cembalest",
        "John P. Dickerson"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15268.pdf",
      "abstract": "The release of ChatGPT in November 2022 sparked an explosion of interest in\npost-training and an avalanche of new preference optimization (PO) methods.\nThese methods claim superior alignment by virtue of better correspondence with\nhuman pairwise preferences, often measured by LLM judges. In this work, we\nattempt to answer the following question -- do LLM-judge preferences translate\nto progress on other, more concrete metrics for alignment, and if not, why not?\nWe define a concrete metric for alignment, and introduce SOS-Bench, the largest\nstandardized, reproducible LLM meta-benchmark to date. We find that (1)\nLLM-judgments do not correlate with concrete measures of safety, world\nknowledge, and instruction following; (2) LLM judges have powerful implicit\nbiases, prioritizing style over factuality and safety; and (3) the supervised\nfine-tuning (SFT) stage of post-training, and not the PO stage, has the\ngreatest impact on alignment, with data scaling and prompt diversity as the\ndriving factors. Our codebase and complete results can be found at\nhttps://github.com/penfever/sos-bench.",
      "upvotes": 11
    },
    {
      "title": "MaterialFusion: Enhancing Inverse Rendering with Material Diffusion Priors",
      "url": "https://huggingface.co/papers/2409.15273",
      "authors": [
        "Yehonathan Litman",
        "Or Patashnik",
        "Kangle Deng",
        "Aviral Agrawal",
        "Rushikesh Zawar",
        "Fernando De la Torre",
        "Shubham Tulsiani"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15273.pdf",
      "abstract": "Recent works in inverse rendering have shown promise in using multi-view\nimages of an object to recover shape, albedo, and materials. However, the\nrecovered components often fail to render accurately under new lighting\nconditions due to the intrinsic challenge of disentangling albedo and material\nproperties from input images. To address this challenge, we introduce\nMaterialFusion, an enhanced conventional 3D inverse rendering pipeline that\nincorporates a 2D prior on texture and material properties. We present\nStableMaterial, a 2D diffusion model prior that refines multi-lit data to\nestimate the most likely albedo and material from given input appearances. This\nmodel is trained on albedo, material, and relit image data derived from a\ncurated dataset of approximately ~12K artist-designed synthetic Blender objects\ncalled BlenderVault. we incorporate this diffusion prior with an inverse\nrendering framework where we use score distillation sampling (SDS) to guide the\noptimization of the albedo and materials, improving relighting performance in\ncomparison with previous work. We validate MaterialFusion's relighting\nperformance on 4 datasets of synthetic and real objects under diverse\nillumination conditions, showing our diffusion-aided approach significantly\nimproves the appearance of reconstructed objects under novel lighting\nconditions. We intend to publicly release our BlenderVault dataset to support\nfurther research in this field.",
      "upvotes": 10
    },
    {
      "title": "Zero-shot Cross-lingual Voice Transfer for TTS",
      "url": "https://huggingface.co/papers/2409.13910",
      "authors": [
        "Fadi Biadsy",
        "Youzheng Chen",
        "Isaac Elias",
        "Kyle Kastner",
        "Gary Wang",
        "Andrew Rosenberg",
        "Bhuvana Ramabhadran"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.13910.pdf",
      "abstract": "In this paper, we introduce a zero-shot Voice Transfer (VT) module that can\nbe seamlessly integrated into a multi-lingual Text-to-speech (TTS) system to\ntransfer an individual's voice across languages. Our proposed VT module\ncomprises a speaker-encoder that processes reference speech, a bottleneck\nlayer, and residual adapters, connected to preexisting TTS layers. We compare\nthe performance of various configurations of these components and report Mean\nOpinion Score (MOS) and Speaker Similarity across languages. Using a single\nEnglish reference speech per speaker, we achieve an average voice transfer\nsimilarity score of 73% across nine target languages. Vocal characteristics\ncontribute significantly to the construction and perception of individual\nidentity. The loss of one's voice, due to physical or neurological conditions,\ncan lead to a profound sense of loss, impacting one's core identity. As a case\nstudy, we demonstrate that our approach can not only transfer typical speech\nbut also restore the voices of individuals with dysarthria, even when only\natypical speech samples are available - a valuable utility for those who have\nnever had typical speech or banked their voice. Cross-lingual typical audio\nsamples, plus videos demonstrating voice restoration for dysarthric speakers\nare available here\n(google.github.io/tacotron/publications/zero_shot_voice_transfer).",
      "upvotes": 7
    },
    {
      "title": "MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting",
      "url": "https://huggingface.co/papers/2409.14393",
      "authors": [
        "Chen Tessler",
        "Yunrong Guo",
        "Ofir Nabati",
        "Xue Bin Peng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14393.pdf",
      "abstract": "Crafting a single, versatile physics-based controller that can breathe life\ninto interactive characters across a wide spectrum of scenarios represents an\nexciting frontier in character animation. An ideal controller should support\ndiverse control modalities, such as sparse target keyframes, text instructions,\nand scene information. While previous works have proposed physically simulated,\nscene-aware control models, these systems have predominantly focused on\ndeveloping controllers that each specializes in a narrow set of tasks and\ncontrol modalities. This work presents MaskedMimic, a novel approach that\nformulates physics-based character control as a general motion inpainting\nproblem. Our key insight is to train a single unified model to synthesize\nmotions from partial (masked) motion descriptions, such as masked keyframes,\nobjects, text descriptions, or any combination thereof. This is achieved by\nleveraging motion tracking data and designing a scalable training method that\ncan effectively utilize diverse motion descriptions to produce coherent\nanimations. Through this process, our approach learns a physics-based\ncontroller that provides an intuitive control interface without requiring\ntedious reward engineering for all behaviors of interest. The resulting\ncontroller supports a wide range of control modalities and enables seamless\ntransitions between disparate tasks. By unifying character control through\nmotion inpainting, MaskedMimic creates versatile virtual characters. These\ncharacters can dynamically adapt to complex scenes and compose diverse motions\non demand, enabling more interactive and immersive experiences.",
      "upvotes": 7
    },
    {
      "title": "An adapted large language model facilitates multiple medical tasks in diabetes care",
      "url": "https://huggingface.co/papers/2409.13191",
      "authors": [
        "Muyang He",
        "Yutong Chen",
        "Qian Yang",
        "Yanzhe Hong",
        "Jiaping Lu",
        "Xiaoying Li",
        "Ying Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.13191.pdf",
      "abstract": "Diabetes is a chronic disease that poses a significant global health burden,\nand optimizing diabetes management requires multi-stakeholder collaboration.\nLarge language models (LLMs) have shown promise in various healthcare\nscenarios, but their effectiveness across a diverse range of diabetes tasks\nremains unproven. In this study, we introduced a framework to train and\nvalidate diabetes-specific LLMs. We first developed a comprehensive data\nprocessing pipeline that includes data collection, filtering, augmentation and\nrefinement. This approach contributes to creating a high-quality,\ndiabetes-specific dataset, and several evaluation benchmarks entirely from\nscratch. Utilizing the collected training dataset, we fine-tuned a\ndiabetes-specific LLM family that demonstrated state-of-the-art proficiency in\nunderstanding and processing various diabetes tasks compared to other LLMs.\nFurthermore, clinical studies showed the potential applications of our models\nin diabetes care, including providing personalized healthcare, assisting\nmedical education, and streamlining clinical tasks. In conclusion, our study\nintroduced a framework to develop and evaluate a diabetes-specific LLM family,\nand highlighted its potential to enhance clinical practice and provide\npersonalized, data-driven support for diabetes support when facing different\nend users. The code is provided via GitHub at\nhttps://github.com/waltonfuture/Diabetica.",
      "upvotes": 6
    },
    {
      "title": "SpaceBlender: Creating Context-Rich Collaborative Spaces Through Generative 3D Scene Blending",
      "url": "https://huggingface.co/papers/2409.13926",
      "authors": [
        "Shwetha Rajaram",
        "Nicolai Marquardt",
        "Andrew D. Wilson"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.13926.pdf",
      "abstract": "There is increased interest in using generative AI to create 3D spaces for\nVirtual Reality (VR) applications. However, today's models produce artificial\nenvironments, falling short of supporting collaborative tasks that benefit from\nincorporating the user's physical context. To generate environments that\nsupport VR telepresence, we introduce SpaceBlender, a novel pipeline that\nutilizes generative AI techniques to blend users' physical surroundings into\nunified virtual spaces. This pipeline transforms user-provided 2D images into\ncontext-rich 3D environments through an iterative process consisting of depth\nestimation, mesh alignment, and diffusion-based space completion guided by\ngeometric priors and adaptive text prompts. In a preliminary within-subjects\nstudy, where 20 participants performed a collaborative VR affinity diagramming\ntask in pairs, we compared SpaceBlender with a generic virtual environment and\na state-of-the-art scene generation framework, evaluating its ability to create\nvirtual spaces suitable for collaboration. Participants appreciated the\nenhanced familiarity and context provided by SpaceBlender but also noted\ncomplexities in the generative environments that could detract from task focus.\nDrawing on participant feedback, we propose directions for improving the\npipeline and discuss the value and design of blended spaces for different\nscenarios.",
      "upvotes": 5
    },
    {
      "title": "A Case Study of Web App Coding with OpenAI Reasoning Models",
      "url": "https://huggingface.co/papers/2409.13773",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2409.13773.pdf",
      "abstract": "This paper presents a case study of coding tasks by the latest reasoning\nmodels of OpenAI, i.e. o1-preview and o1-mini, in comparison with other\nfrontier models. The o1 models deliver SOTA results for WebApp1K, a single-task\nbenchmark. To this end, we introduce WebApp1K-Duo, a harder benchmark doubling\nnumber of tasks and test cases. The new benchmark causes the o1 model\nperformances to decline significantly, falling behind Claude 3.5. Moreover,\nthey consistently fail when confronted with atypical yet correct test cases, a\ntrap non-reasoning models occasionally avoid. We hypothesize that the\nperformance variability is due to instruction comprehension. Specifically, the\nreasoning mechanism boosts performance when all expectations are captured,\nmeanwhile exacerbates errors when key expectations are missed, potentially\nimpacted by input lengths. As such, we argue that the coding success of\nreasoning models hinges on the top-notch base model and SFT to ensure\nmeticulous adherence to instructions.",
      "upvotes": 4
    },
    {
      "title": "Self-Supervised Audio-Visual Soundscape Stylization",
      "url": "https://huggingface.co/papers/2409.14340",
      "authors": [
        "Tingle Li",
        "Renhao Wang",
        "Po-Yao Huang",
        "Andrew Owens",
        "Gopala Anumanchipalli"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14340.pdf",
      "abstract": "Speech sounds convey a great deal of information about the scenes, resulting\nin a variety of effects ranging from reverberation to additional ambient\nsounds. In this paper, we manipulate input speech to sound as though it was\nrecorded within a different scene, given an audio-visual conditional example\nrecorded from that scene. Our model learns through self-supervision, taking\nadvantage of the fact that natural video contains recurring sound events and\ntextures. We extract an audio clip from a video and apply speech enhancement.\nWe then train a latent diffusion model to recover the original speech, using\nanother audio-visual clip taken from elsewhere in the video as a conditional\nhint. Through this process, the model learns to transfer the conditional\nexample's sound properties to the input speech. We show that our model can be\nsuccessfully trained using unlabeled, in-the-wild videos, and that an\nadditional visual signal can improve its sound prediction abilities. Please see\nour project webpage for video results:\nhttps://tinglok.netlify.app/files/avsoundscape/",
      "upvotes": 2
    }
  ]
}