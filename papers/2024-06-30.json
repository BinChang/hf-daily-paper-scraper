{
  "date": "2024-06-30",
  "papers": [
    {
      "title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding",
      "url": "https://huggingface.co/papers/2406.19389",
      "authors": [
        "Tao Zhang",
        "Hao Fei",
        "Shunping Ji",
        "Chen Change Loy",
        "Shuicheng Yan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.19389.pdf",
      "abstract": "Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research.",
      "upvotes": 51
    },
    {
      "title": "Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs",
      "url": "https://huggingface.co/papers/2406.18629",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.18629.pdf",
      "abstract": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs) due to the extensive and precise chain of reasoning required for\naccuracy. Ensuring the correctness of each reasoning step is critical. To\naddress this, we aim to enhance the robustness and factuality of LLMs by\nlearning from human feedback. However, Direct Preference Optimization (DPO) has\nshown limited benefits for long-chain mathematical reasoning, as models\nemploying DPO struggle to identify detailed errors in incorrect answers. This\nlimitation stems from a lack of fine-grained process supervision. We propose a\nsimple, effective, and data-efficient method called Step-DPO, which treats\nindividual reasoning steps as units for preference optimization rather than\nevaluating answers holistically. Additionally, we have developed a data\nconstruction pipeline for Step-DPO, enabling the creation of a high-quality\ndataset containing 10K step-wise preference pairs. We also observe that in DPO,\nself-generated data is more effective than data generated by humans or GPT-4,\ndue to the latter's out-of-distribution nature. Our findings demonstrate that\nas few as 10K preference data pairs and fewer than 500 Step-DPO training steps\ncan yield a nearly 3% gain in accuracy on MATH for models with over 70B\nparameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves\nscores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively,\nsurpassing a series of closed-source models, including GPT-4-1106,\nClaude-3-Opus, and Gemini-1.5-Pro. Our code, data, and models are available at\nhttps://github.com/dvlab-research/Step-DPO.",
      "upvotes": 40
    },
    {
      "title": "MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data",
      "url": "https://huggingface.co/papers/2406.18790",
      "authors": [
        "Alexander Peysakhovich"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.18790.pdf",
      "abstract": "We train a model to generate images from multimodal prompts of interleaved\ntext and images such as \"a <picture of a man> man and his <picture of a dog>\ndog in an <picture of a cartoon> animated style.\" We bootstrap a multimodal\ndataset by extracting semantically meaningful image crops corresponding to\nwords in the image captions of synthetically generated and publicly available\ntext-image data. Our model, MUMU, is composed of a vision-language model\nencoder with a diffusion decoder and is trained on a single 8xH100 GPU node.\nDespite being only trained on crops from the same image, MUMU learns to compose\ninputs from different images into a coherent output. For example, an input of a\nrealistic person and a cartoon will output the same person in the cartoon\nstyle, and an input of a standing subject and a scooter will output the subject\nriding the scooter. As a result, our model generalizes to tasks such as style\ntransfer and character consistency. Our results show the promise of using\nmultimodal models as general purpose controllers for image generation.",
      "upvotes": 33
    },
    {
      "title": "Simulating Classroom Education with LLM-Empowered Agents",
      "url": "https://huggingface.co/papers/2406.19226",
      "authors": [
        "Daniel Zhang-Li",
        "Linlu Gong",
        "Jinchang Zhou",
        "Lei Hou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.19226.pdf",
      "abstract": "Large language models (LLMs) have been employed in various intelligent\neducational tasks to assist teaching. While preliminary explorations have\nfocused on independent LLM-empowered agents for specific educational tasks, the\npotential for LLMs within a multi-agent collaborative framework to simulate a\nclassroom with real user participation remains unexplored. In this work, we\npropose SimClass, a multi-agent classroom simulation framework involving user\nparticipation. We recognize representative class roles and introduce a novel\nclass control mechanism for automatic classroom teaching, and conduct user\nexperiments in two real-world courses. Utilizing the Flanders Interactive\nAnalysis System and Community of Inquiry theoretical frame works from\neducational analysis, we demonstrate that LLMs can simulate traditional\nclassroom interaction patterns effectively while enhancing user's experience.\nWe also observe emergent group behaviors among agents in SimClass, where agents\ncollaborate to create enlivening interactions in classrooms to improve user\nlearning process. We hope this work pioneers the application of LLM-empowered\nmulti-agent systems in virtual classroom teaching.",
      "upvotes": 29
    },
    {
      "title": "SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation",
      "url": "https://huggingface.co/papers/2406.19215",
      "authors": [
        "Weijian Qi",
        "Linmei Hu",
        "Weichuan Liu",
        "Lei Hou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.19215.pdf",
      "abstract": "This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel\nadaptive RAG model that extracts self-aware uncertainty of LLMs from their\ninternal states. SeaKR activates retrieval when the LLMs present high\nself-aware uncertainty for generation. To effectively integrate retrieved\nknowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty\nto preserve the snippet that reduces their uncertainty to the utmost. To\nfacilitate solving complex tasks that require multiple retrievals, SeaKR\nutilizes their self-aware uncertainty to choose among different reasoning\nstrategies. Our experiments on both complex and simple Question Answering\ndatasets show that SeaKR outperforms existing adaptive RAG methods. We release\nour code at https://github.com/THU-KEG/SeaKR.",
      "upvotes": 29
    },
    {
      "title": "Aligning Teacher with Student Preferences for Tailored Training Data Generation",
      "url": "https://huggingface.co/papers/2406.19227",
      "authors": [
        "Yantao Liu",
        "Zhao Zhang",
        "Lei Hou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.19227.pdf",
      "abstract": "Large Language Models (LLMs) have shown significant promise as copilots in\nvarious tasks. Local deployment of LLMs on edge devices is necessary when\nhandling privacy-sensitive data or latency-sensitive tasks. The computational\nconstraints of such devices make direct deployment of powerful large-scale LLMs\nimpractical, necessitating the Knowledge Distillation from large-scale models\nto lightweight models. Lots of work has been done to elicit diversity and\nquality training examples from LLMs, but little attention has been paid to\naligning teacher instructional content based on student preferences, akin to\n\"responsive teaching\" in pedagogy. Thus, we propose ARTE, dubbed Aligning\nTeacheR with StudenT PreferencEs, a framework that aligns the teacher model\nwith student preferences to generate tailored training examples for Knowledge\nDistillation. Specifically, we elicit draft questions and rationales from the\nteacher model, then collect student preferences on these questions and\nrationales using students' performance with in-context learning as a proxy, and\nfinally align the teacher model with student preferences. In the end, we repeat\nthe first step with the aligned teacher model to elicit tailored training\nexamples for the student model on the target task. Extensive experiments on\nacademic benchmarks demonstrate the superiority of ARTE over existing\ninstruction-tuning datasets distilled from powerful LLMs. Moreover, we\nthoroughly investigate the generalization of ARTE, including the generalization\nof fine-tuned student models in reasoning ability and the generalization of\naligned teacher models to generate tailored training data across tasks and\nstudents. In summary, our contributions lie in proposing a novel framework for\ntailored training example generation, demonstrating its efficacy in\nexperiments, and investigating the generalization of both student & aligned\nteacher models in ARTE.",
      "upvotes": 24
    },
    {
      "title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark",
      "url": "https://huggingface.co/papers/2406.19314",
      "authors": [
        "Ben Feuer",
        "Siddhartha Jain",
        "Ravid Shwartz-Ziv",
        "Micah Goldblum"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.19314.pdf",
      "abstract": "Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as\nBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions\nof tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.",
      "upvotes": 19
    },
    {
      "title": "Dataset Size Recovery from LoRA Weights",
      "url": "https://huggingface.co/papers/2406.19395",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.19395.pdf",
      "abstract": "Model inversion and membership inference attacks aim to reconstruct and\nverify the data which a model was trained on. However, they are not guaranteed\nto find all training samples as they do not know the size of the training set.\nIn this paper, we introduce a new task: dataset size recovery, that aims to\ndetermine the number of samples used to train a model, directly from its\nweights. We then propose DSiRe, a method for recovering the number of images\nused to fine-tune a model, in the common case where fine-tuning uses LoRA. We\ndiscover that both the norm and the spectrum of the LoRA matrices are closely\nlinked to the fine-tuning dataset size; we leverage this finding to propose a\nsimple yet effective prediction algorithm. To evaluate dataset size recovery of\nLoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of\nover 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.\nOur best classifier can predict the number of fine-tuning images with a mean\nabsolute error of 0.36 images, establishing the feasibility of this attack.",
      "upvotes": 18
    },
    {
      "title": "Can LLMs Learn by Teaching? A Preliminary Study",
      "url": "https://huggingface.co/papers/2406.14629",
      "authors": [
        "Matthew B. Blaschko",
        "Guohao Dai",
        "Huazhong Yang",
        "Yu Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14629.pdf",
      "abstract": "Teaching to improve student models (e.g., knowledge distillation) is an\nextensively studied methodology in LLMs. However, for humans, teaching not only\nimproves students but also improves teachers. We ask: Can LLMs also learn by\nteaching (LbT)? If yes, we can potentially unlock the possibility of\ncontinuously advancing the models without solely relying on human-produced data\nor stronger models. In this paper, we provide a preliminary exploration of this\nambitious agenda. We show that LbT ideas can be incorporated into existing LLM\ntraining/prompting pipelines and provide noticeable improvements. Specifically,\nwe design three methods, each mimicking one of the three levels of LbT in\nhumans: observing students' feedback, learning from the feedback, and learning\niteratively, with the goals of improving answer accuracy without training and\nimproving models' inherent capability with fine-tuning. The findings are\nencouraging. For example, similar to LbT in human, we see that: (1) LbT can\ninduce weak-to-strong generalization: strong models can improve themselves by\nteaching other weak models; (2) Diversity in students might help: teaching\nmultiple students could be better than teaching one student or the teacher\nitself. We hope that this early promise can inspire future research on LbT and\nmore broadly adopting the advanced techniques in education to improve LLMs. The\ncode is available at https://github.com/imagination-research/lbt.",
      "upvotes": 17
    },
    {
      "title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression",
      "url": "https://huggingface.co/papers/2406.14909",
      "authors": [
        "Shengen Yan",
        "Guohao Dai",
        "Huazhong Yang",
        "Yu Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14909.pdf",
      "abstract": "Sparse attention can effectively mitigate the significant memory and\nthroughput demands of Large Language Models (LLMs) in long contexts. Existing\nmethods typically employ a uniform sparse attention mask, applying the same\nsparse pattern across different attention heads and input lengths. However,\nthis uniform approach fails to capture the diverse attention patterns inherent\nin LLMs, ignoring their distinct accuracy-latency trade-offs. To address this\nchallenge, we propose the Mixture of Attention (MoA), which automatically\ntailors distinct sparse attention configurations to different heads and layers.\nMoA constructs and navigates a search space of various attention patterns and\ntheir scaling rules relative to input sequence lengths. It profiles the model,\nevaluates potential configurations, and pinpoints the optimal sparse attention\ncompression plan. MoA adapts to varying input sizes, revealing that some\nattention heads expand their focus to accommodate longer sequences, while other\nheads consistently concentrate on fixed-length local contexts. Experiments show\nthat MoA increases the effective context length by 3.9times with the same\naverage attention span, boosting retrieval accuracy by 1.5-7.1times over the\nuniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models.\nMoreover, MoA narrows the capability gaps between sparse and dense models,\nreducing the maximum relative performance drop from 9%-36% to within 5%\nacross two long-context understanding benchmarks. MoA achieves a\n1.2-1.4times GPU memory reduction and boosts decode throughput by 5.5-6.7\ntimes for 7B and 13B dense models on a single GPU, with minimal impact on\nperformance.",
      "upvotes": 13
    },
    {
      "title": "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models",
      "url": "https://huggingface.co/papers/2406.10900",
      "authors": [
        "Xiyang Wu",
        "Dianqi Li",
        "Shuaiyi Huang",
        "Xiaoyu Liu",
        "Xijun Wang",
        "Ruiqi Xian",
        "Abhinav Shrivastava",
        "Furong Huang",
        "Jordan Lee Boyd-Graber",
        "Dinesh Manocha"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10900.pdf",
      "abstract": "Large vision-language models (LVLMs) hallucinate: certain context cues in an\nimage may trigger the language module's overconfident and incorrect reasoning\non abnormal or hypothetical objects. Though a few benchmarks have been\ndeveloped to investigate LVLM hallucinations, they mainly rely on hand-crafted\ncorner cases whose fail patterns may hardly generalize, and finetuning on them\ncould undermine their validity. These motivate us to develop the first\nautomatic benchmark generation approach, AUTOHALLUSION, that harnesses a few\nprincipal strategies to create diverse hallucination examples. It probes the\nlanguage modules in LVLMs for context cues and uses them to synthesize images\nby: (1) adding objects abnormal to the context cues; (2) for two co-occurring\nobjects, keeping one and excluding the other; or (3) removing objects closely\ntied to the context cues. It then generates image-based questions whose\nground-truth answers contradict the language module's prior. A model has to\novercome contextual biases and distractions to reach correct answers, while\nincorrect or inconsistent answers indicate hallucinations. AUTOHALLUSION\nenables us to create new benchmarks at the minimum cost and thus overcomes the\nfragility of hand-crafted benchmarks. It also reveals common failure patterns\nand reasons, providing key insights to detect, avoid, or control\nhallucinations. Comprehensive evaluations of top-tier LVLMs, e.g.,\nGPT-4V(ision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, show a 97.7% and\n98.7% success rate of hallucination induction on synthetic and real-world\ndatasets of AUTOHALLUSION, paving the way for a long battle against\nhallucinations.",
      "upvotes": 11
    },
    {
      "title": "Is Programming by Example solved by LLMs?",
      "url": "https://huggingface.co/papers/2406.08316",
      "authors": [
        "Kevin Ellis"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08316.pdf",
      "abstract": "Programming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI\nperspective, PBE corresponds to a very general form of few-shot inductive\ninference. Given the success of Large Language Models (LLMs) in code-generation\ntasks, we investigate here the extent to which LLMs can be said to have\n`solved' PBE. We experiment on classic domains such as lists and strings, and\nan uncommon graphics programming domain not well represented in typical\npretraining data. We find that pretrained models are not effective at PBE, but\nthat they can be fine-tuned for much higher performance, provided the test\nproblems are in-distribution. We analyze empirically what causes these models\nto succeed and fail, and take steps toward understanding how to achieve better\nout-of-distribution generalization. Collectively these results suggest that\nLLMs make strong progress toward solving the typical suite of PBE tasks,\npotentially increasing the flexibility and applicability of PBE systems, while\nalso identifying ways in which LLMs still fall short.",
      "upvotes": 11
    },
    {
      "title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding",
      "url": "https://huggingface.co/papers/2406.19263",
      "authors": [
        "Lei Ding",
        "Ching-Chen Kuo",
        "Shan Jiang",
        "Yang Zhao",
        "Xinze Guan",
        "Jie Yang",
        "Yi Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.19263.pdf",
      "abstract": "Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the\ncorresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other\nscreen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:\nscreen-point-and-read.github.io",
      "upvotes": 9
    },
    {
      "title": "T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings",
      "url": "https://huggingface.co/papers/2406.19223",
      "authors": [
        "Björn Deiseroth",
        "Patrick Schramowski",
        "Kristian Kersting",
        "Samuel Weinbach"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.19223.pdf",
      "abstract": "Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning.",
      "upvotes": 8
    },
    {
      "title": "ArzEn-LLM: Code-Switched Egyptian Arabic-English Translation and Speech Recognition Using LLMs",
      "url": "https://huggingface.co/papers/2406.18120",
      "authors": [
        "Mennatullah Ali"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.18120.pdf",
      "abstract": "Motivated by the widespread increase in the phenomenon of code-switching\nbetween Egyptian Arabic and English in recent times, this paper explores the\nintricacies of machine translation (MT) and automatic speech recognition (ASR)\nsystems, focusing on translating code-switched Egyptian Arabic-English to\neither English or Egyptian Arabic. Our goal is to present the methodologies\nemployed in developing these systems, utilizing large language models such as\nLLama and Gemma. In the field of ASR, we explore the utilization of the Whisper\nmodel for code-switched Egyptian Arabic recognition, detailing our experimental\nprocedures including data preprocessing and training techniques. Through the\nimplementation of a consecutive speech-to-text translation system that\nintegrates ASR with MT, we aim to overcome challenges posed by limited\nresources and the unique characteristics of the Egyptian Arabic dialect.\nEvaluation against established metrics showcases promising results, with our\nmethodologies yielding a significant improvement of 56% in English\ntranslation over the state-of-the-art and 9.3% in Arabic translation. Since\ncode-switching is deeply inherent in spoken languages, it is crucial that ASR\nsystems can effectively handle this phenomenon. This capability is crucial for\nenabling seamless interaction in various domains, including business\nnegotiations, cultural exchanges, and academic discourse. Our models and code\nare available as open-source resources. Code:\nhttp://github.com/ahmedheakl/arazn-llm}, Models:\nhttp://huggingface.co/collections/ahmedheakl/arazn-llm-662ceaf12777656607b9524e.",
      "upvotes": 6
    },
    {
      "title": "Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation",
      "url": "https://huggingface.co/papers/2406.18676",
      "authors": [
        "Yutao Zhu",
        "Chenghao Zhang",
        "Zechen Wang",
        "Ji-Rong Wen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.18676.pdf",
      "abstract": "Retrieval-augmented generation (RAG) has demonstrated effectiveness in\nmitigating the hallucination problem of large language models (LLMs). However,\nthe difficulty of aligning the retriever with the diverse LLMs' knowledge\npreferences inevitably poses an inevitable challenge in developing a reliable\nRAG system. To address this issue, we propose DPA-RAG, a universal framework\ndesigned to align diverse knowledge preferences within RAG systems.\nSpecifically, we initially introduce a preference knowledge construction\npipline and incorporate five novel query augmentation strategies to alleviate\npreference data scarcity. Based on preference data, DPA-RAG accomplishes both\nexternal and internal preference alignment: 1) It jointly integrate pair-wise,\npoint-wise, and contrastive preference alignment abilities into the reranker,\nachieving external preference alignment among RAG components. 2) It further\nintroduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT),\nenabling LLMs to implicitly capture knowledge aligned with their reasoning\npreferences, achieving LLMs' internal alignment. Experimental results across\nfour knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all\nbaselines and seamlessly integrates both black-box and open-sourced LLM\nreaders. Further qualitative analysis and discussions also provide empirical\nguidance for achieving reliable RAG systems. Our code is publicly available at\nhttps://github.com/dongguanting/DPA-RAG.",
      "upvotes": 5
    },
    {
      "title": "Benchmarking Mental State Representations in Language Models",
      "url": "https://huggingface.co/papers/2406.17513",
      "authors": [
        "Lei Shi",
        "Andreas Bulling"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17513.pdf",
      "abstract": "While numerous works have assessed the generative performance of language\nmodels (LMs) on tasks requiring Theory of Mind reasoning, research into the\nmodels' internal representation of mental states remains limited. Recent work\nhas used probing to demonstrate that LMs can represent beliefs of themselves\nand others. However, these claims are accompanied by limited evaluation, making\nit difficult to assess how mental state representations are affected by model\ndesign and training choices. We report an extensive benchmark with various LM\ntypes with different model sizes, fine-tuning approaches, and prompt designs to\nstudy the robustness of mental state representations and memorisation issues\nwithin the probes. Our results show that the quality of models' internal\nrepresentations of the beliefs of others increases with model size and, more\ncrucially, with fine-tuning. We are the first to study how prompt variations\nimpact probing performance on theory of mind tasks. We demonstrate that models'\nrepresentations are sensitive to prompt variations, even when such variations\nshould be beneficial. Finally, we complement previous activation editing\nexperiments on Theory of Mind tasks and show that it is possible to improve\nmodels' reasoning performance by steering their activations without the need to\ntrain any probe.",
      "upvotes": 3
    },
    {
      "title": "ResumeAtlas: Revisiting Resume Classification with Large-Scale Datasets and Large Language Models",
      "url": "https://huggingface.co/papers/2406.18125",
      "authors": [
        "Noran Mohamed",
        "Ali Sharkaway",
        "Ahmed Zaky"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.18125.pdf",
      "abstract": "The increasing reliance on online recruitment platforms coupled with the\nadoption of AI technologies has highlighted the critical need for efficient\nresume classification methods. However, challenges such as small datasets, lack\nof standardized resume templates, and privacy concerns hinder the accuracy and\neffectiveness of existing classification models. In this work, we address these\nchallenges by presenting a comprehensive approach to resume classification. We\ncurated a large-scale dataset of 13,389 resumes from diverse sources and\nemployed Large Language Models (LLMs) such as BERT and Gemma1.1 2B for\nclassification. Our results demonstrate significant improvements over\ntraditional machine learning approaches, with our best model achieving a top-1\naccuracy of 92\\% and a top-5 accuracy of 97.5\\%. These findings underscore the\nimportance of dataset quality and advanced model architectures in enhancing the\naccuracy and robustness of resume classification systems, thus advancing the\nfield of online recruitment practices.",
      "upvotes": 3
    }
  ]
}