{
  "date": "2024-07-18",
  "papers": [
    {
      "title": "Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language Models",
      "url": "https://huggingface.co/papers/2407.12327",
      "authors": [
        "Aaryan Bhagat"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12327.pdf",
      "abstract": "Post-training quantization is the leading method for addressing\nmemory-related bottlenecks in LLM inference, but unfortunately, it suffers from\nsignificant performance degradation below 4-bit precision. An alternative\napproach involves training compressed models directly at a low bitwidth (e.g.,\nbinary or ternary models). However, the performance, training dynamics, and\nscaling trends of such models are not yet well understood. To address this\nissue, we train and openly release the Spectra LLM suite consisting of 54\nlanguage models ranging from 99M to 3.9B parameters, trained on 300B tokens.\nSpectra includes FloatLMs, post-training quantized QuantLMs (3, 4, 6, and 8\nbits), and ternary LLMs (TriLMs) - our improved architecture for ternary\nlanguage modeling, which significantly outperforms previously proposed ternary\nmodels of a given size (in bits), matching half-precision models at scale. For\nexample, TriLM 3.9B is (bit-wise) smaller than the half-precision FloatLM 830M,\nbut matches half-precision FloatLM 3.9B in commonsense reasoning and knowledge\nbenchmarks. However, TriLM 3.9B is also as toxic and stereotyping as FloatLM\n3.9B, a model six times larger in size. Additionally, TriLM 3.9B lags behind\nFloatLM in perplexity on validation splits and web-based corpora but performs\nbetter on less noisy datasets like Lambada and PennTreeBank.\n  To enhance understanding of low-bitwidth models, we are releasing 500+\nintermediate checkpoints of the Spectra suite at\nhttps://github.com/NolanoOrg/SpectraSuite{https://github.com/NolanoOrg/SpectraSuite}.",
      "upvotes": 77
    },
    {
      "title": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression",
      "url": "https://huggingface.co/papers/2407.12077",
      "authors": [
        "Daniel Goldstein"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12077.pdf",
      "abstract": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use.",
      "upvotes": 54
    },
    {
      "title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases",
      "url": "https://huggingface.co/papers/2407.12784",
      "authors": [
        "Zhen Xiang",
        "Chaowei Xiao",
        "Dawn Song",
        "Bo Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12784.pdf",
      "abstract": "LLM agents have demonstrated remarkable performance across various\napplications, primarily due to their advanced capabilities in reasoning,\nutilizing external knowledge and tools, calling APIs, and executing actions to\ninteract with environments. Current agents typically utilize a memory module or\na retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and\ninstances with similar embeddings from knowledge bases to inform task planning\nand execution. However, the reliance on unverified knowledge bases raises\nsignificant concerns about their safety and trustworthiness. To uncover such\nvulnerabilities, we propose a novel red teaming approach AgentPoison, the first\nbackdoor attack targeting generic and RAG-based LLM agents by poisoning their\nlong-term memory or RAG knowledge base. In particular, we form the trigger\ngeneration process as a constrained optimization to optimize backdoor triggers\nby mapping the triggered instances to a unique embedding space, so as to ensure\nthat whenever a user instruction contains the optimized backdoor trigger, the\nmalicious demonstrations are retrieved from the poisoned memory or knowledge\nbase with high probability. In the meantime, benign instructions without the\ntrigger will still maintain normal performance. Unlike conventional backdoor\nattacks, AgentPoison requires no additional model training or fine-tuning, and\nthe optimized backdoor trigger exhibits superior transferability, in-context\ncoherence, and stealthiness. Extensive experiments demonstrate AgentPoison's\neffectiveness in attacking three types of real-world LLM agents: RAG-based\nautonomous driving agent, knowledge-intensive QA agent, and healthcare\nEHRAgent. On each agent, AgentPoison achieves an average attack success rate\nhigher than 80% with minimal impact on benign performance (less than 1%) with a\npoison rate less than 0.1%.",
      "upvotes": 48
    },
    {
      "title": "E5-V: Universal Embeddings with Multimodal Large Language Models",
      "url": "https://huggingface.co/papers/2407.12580",
      "authors": [
        "Ting Jiang",
        "Minghui Song",
        "Zihan Zhang",
        "Weiwei Deng",
        "Feng Sun",
        "Qi Zhang",
        "Deqing Wang",
        "Fuzhen Zhuang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12580.pdf",
      "abstract": "Multimodal large language models (MLLMs) have shown promising advancements in\ngeneral visual and language understanding. However, the representation of\nmultimodal information using MLLMs remains largely unexplored. In this work, we\nintroduce a new framework, E5-V, designed to adapt MLLMs for achieving\nuniversal multimodal embeddings. Our findings highlight the significant\npotential of MLLMs in representing multimodal inputs compared to previous\napproaches. By leveraging MLLMs with prompts, E5-V effectively bridges the\nmodality gap between different types of inputs, demonstrating strong\nperformance in multimodal embeddings even without fine-tuning. We propose a\nsingle modality training approach for E5-V, where the model is trained\nexclusively on text pairs. This method demonstrates significant improvements\nover traditional multimodal training on image-text pairs, while reducing\ntraining costs by approximately 95%. Additionally, this approach eliminates the\nneed for costly multimodal training data collection. Extensive experiments\nacross four types of tasks demonstrate the effectiveness of E5-V. As a\nuniversal multimodal model, E5-V not only achieves but often surpasses\nstate-of-the-art performance in each task, despite being trained on a single\nmodality.",
      "upvotes": 39
    },
    {
      "title": "LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models",
      "url": "https://huggingface.co/papers/2407.12772",
      "authors": [
        "Joshua Adrian Cahyono",
        "Kairui Hu",
        "Shuai Liu",
        "Jingkang Yang",
        "Chunyuan Li",
        "Ziwei Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12772.pdf",
      "abstract": "The advances of large foundation models necessitate wide-coverage, low-cost,\nand zero-contamination benchmarks. Despite continuous exploration of language\nmodel evaluations, comprehensive studies on the evaluation of Large Multi-modal\nModels (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified\nand standardized multimodal benchmark framework with over 50 tasks and more\nthan 10 models to promote transparent and reproducible evaluations. Although\nLMMS-EVAL offers comprehensive coverage, we find it still falls short in\nachieving low cost and zero contamination. To approach this evaluation\ntrilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that\nemphasizes both coverage and efficiency. Additionally, we present Multimodal\nLIVEBENCH that utilizes continuously updating news and online forums to assess\nmodels' generalization abilities in the wild, featuring a low-cost and\nzero-contamination evaluation approach. In summary, our work highlights the\nimportance of considering the evaluation trilemma and provides practical\nsolutions to navigate the trade-offs in evaluating large multi-modal models,\npaving the way for more effective and reliable benchmarking of LMMs. We\nopensource our codebase and maintain leaderboard of LIVEBENCH at\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval and\nhttps://huggingface.co/spaces/lmms-lab/LiveBench.",
      "upvotes": 33
    },
    {
      "title": "Patch-Level Training for Large Language Models",
      "url": "https://huggingface.co/papers/2407.12665",
      "authors": [
        "Chenze Shao",
        "Fandong Meng",
        "Jie Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12665.pdf",
      "abstract": "As Large Language Models (LLMs) achieve remarkable progress in language\nunderstanding and generation, their training efficiency has become a critical\nconcern. Traditionally, LLMs are trained to predict the next token in a\nsequence. Despite the success of token-level training, it suffers from\nconsiderable computational costs due to the need to process an extensive number\nof tokens. To mitigate this issue, this paper introduces patch-level training\nfor LLMs, which reduces the sequence length by compressing multiple tokens into\na single patch. During patch-level training, we feed the language model shorter\nsequences of patches and train it to predict the next patch, thereby processing\nthe majority of the training data at a significantly reduced computational\ncost. Following this, the model continues token-level training on the remaining\ntraining data to align with the inference mode. Experiments on a diverse range\nof models (370M-2.7B parameters) demonstrate that patch-level training can\nreduce overall computational costs to 0.5times, without compromising the\nmodel performance compared to token-level training. Source code:\nhttps://github.com/shaochenze/PatchTrain.",
      "upvotes": 16
    },
    {
      "title": "IMAGDressing-v1: Customizable Virtual Dressing",
      "url": "https://huggingface.co/papers/2407.12705",
      "authors": [
        "Fei Shen",
        "Xin Jiang",
        "Xin He",
        "Hu Ye",
        "Cong Wang",
        "Xiaoyu Du",
        "Zechao Li",
        "Jinghui Tang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12705.pdf",
      "abstract": "Latest advances have achieved realistic virtual try-on (VTON) through\nlocalized garment inpainting using latent diffusion models, significantly\nenhancing consumers' online shopping experience. However, existing VTON\ntechnologies neglect the need for merchants to showcase garments\ncomprehensively, including flexible control over garments, optional faces,\nposes, and scenes. To address this issue, we define a virtual dressing (VD)\ntask focused on generating freely editable human images with fixed garments and\noptional conditions. Meanwhile, we design a comprehensive affinity metric index\n(CAMI) to evaluate the consistency between generated images and reference\ngarments. Then, we propose IMAGDressing-v1, which incorporates a garment UNet\nthat captures semantic features from CLIP and texture features from VAE. We\npresent a hybrid attention module, including a frozen self-attention and a\ntrainable cross-attention, to integrate garment features from the garment UNet\ninto a frozen denoising UNet, ensuring users can control different scenes\nthrough text. IMAGDressing-v1 can be combined with other extension plugins,\nsuch as ControlNet and IP-Adapter, to enhance the diversity and controllability\nof generated images. Furthermore, to address the lack of data, we release the\ninteractive garment pairing (IGPair) dataset, containing over 300,000 pairs of\nclothing and dressed images, and establish a standard pipeline for data\nassembly. Extensive experiments demonstrate that our IMAGDressing-v1 achieves\nstate-of-the-art human image synthesis performance under various controlled\nconditions. The code and model will be available at\nhttps://github.com/muzishen/IMAGDressing.",
      "upvotes": 12
    },
    {
      "title": "VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control",
      "url": "https://huggingface.co/papers/2407.12781",
      "authors": [
        "Sherwin Bahmani",
        "Ivan Skorokhodov",
        "Aliaksandr Siarohin",
        "Willi Menapace",
        "Guocheng Qian",
        "Michael Vasilkovsky",
        "Hsin-Ying Lee",
        "Chaoyang Wang",
        "Jiaxu Zou",
        "Andrea Tagliasacchi",
        "David B. Lindell",
        "Sergey Tulyakov"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12781.pdf",
      "abstract": "Modern text-to-video synthesis models demonstrate coherent, photorealistic\ngeneration of complex videos from a text description. However, most existing\nmodels lack fine-grained control over camera movement, which is critical for\ndownstream applications related to content creation, visual effects, and 3D\nvision. Recently, new methods demonstrate the ability to generate videos with\ncontrollable camera poses these techniques leverage pre-trained U-Net-based\ndiffusion models that explicitly disentangle spatial and temporal generation.\nStill, no existing approach enables camera control for new, transformer-based\nvideo diffusion models that process spatial and temporal information jointly.\nHere, we propose to tame video transformers for 3D camera control using a\nControlNet-like conditioning mechanism that incorporates spatiotemporal camera\nembeddings based on Plucker coordinates. The approach demonstrates\nstate-of-the-art performance for controllable video generation after\nfine-tuning on the RealEstate10K dataset. To the best of our knowledge, our\nwork is the first to enable camera control for transformer-based video\ndiffusion models.",
      "upvotes": 12
    },
    {
      "title": "Case2Code: Learning Inductive Reasoning with Synthetic Data",
      "url": "https://huggingface.co/papers/2407.12504",
      "authors": [
        "Yichuan Ma",
        "Peiji Li",
        "Demin Song",
        "Qinyuan Cheng",
        "Shimin Li",
        "Xiaonan Li",
        "Pengyu Wang",
        "Qipeng Guo",
        "Hang Yan",
        "Xipeng Qiu",
        "Xuanjing Huang",
        "Dahua Lin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12504.pdf",
      "abstract": "Complex reasoning is an impressive ability shown by large language models\n(LLMs). Most LLMs are skilled in deductive reasoning, such as chain-of-thought\nprompting or iterative tool-using to solve challenging tasks step-by-step. In\nthis paper, we hope to focus on evaluating and teaching LLMs to conduct\ninductive reasoning, that is, LLMs are supposed to infer underlying rules by\nobserving examples or sequential transformations. However, collecting\nlarge-scale and diverse human-generated inductive data is challenging. We focus\non data synthesis in the code domain and propose a Case2Code task by\nexploiting the expressiveness and correctness of programs. Specifically, we\ncollect a diverse set of executable programs, synthesize input-output\ntransformations for each program, and force LLMs to infer the underlying code\nimplementations based on the synthetic I/O cases. We first evaluate\nrepresentative LLMs on the synthesized Case2Code task and demonstrate that the\nCase-to-code induction is challenging for LLMs. Then, we synthesize large-scale\nCase2Code training samples to train LLMs to perform inductive reasoning.\nExperimental results show that such induction training benefits not only in\ndistribution Case2Code performance but also enhances various coding abilities\nof trained LLMs, demonstrating the great potential of learning inductive\nreasoning via synthetic data.",
      "upvotes": 7
    },
    {
      "title": "Goldfish: Vision-Language Understanding of Arbitrarily Long Videos",
      "url": "https://huggingface.co/papers/2407.12679",
      "authors": [
        "Kirolos Ataallah",
        "Eslam Abdelrahman",
        "Essam Sleiman",
        "Mingchen Zhuge",
        "Jian Ding",
        "Deyao Zhu",
        "Jürgen Schmidhuber",
        "Mohamed Elhoseiny"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12679.pdf",
      "abstract": "Most current LLM-based models for video understanding can process videos\nwithin minutes. However, they struggle with lengthy videos due to challenges\nsuch as \"noise and redundancy\", as well as \"memory and computation\"\nconstraints. In this paper, we present Goldfish, a methodology tailored for\ncomprehending videos of arbitrary lengths. We also introduce the TVQA-long\nbenchmark, specifically designed to evaluate models' capabilities in\nunderstanding long videos with questions in both vision and text content.\nGoldfish approaches these challenges with an efficient retrieval mechanism that\ninitially gathers the top-k video clips relevant to the instruction before\nproceeding to provide the desired response. This design of the retrieval\nmechanism enables the Goldfish to efficiently process arbitrarily long video\nsequences, facilitating its application in contexts such as movies or\ntelevision series. To facilitate the retrieval process, we developed\nMiniGPT4-Video that generates detailed descriptions for the video clips. In\naddressing the scarcity of benchmarks for long video evaluation, we adapted the\nTVQA short video benchmark for extended content analysis by aggregating\nquestions from entire episodes, thereby shifting the evaluation from partial to\nfull episode comprehension. We attained a 41.78% accuracy rate on the TVQA-long\nbenchmark, surpassing previous methods by 14.94%. Our MiniGPT4-Video also shows\nexceptional performance in short video comprehension, exceeding existing\nstate-of-the-art methods by 3.23%, 2.03%, 16.5% and 23.59% on the MSVD, MSRVTT,\nTGIF, and TVQA short video benchmarks, respectively. These results indicate\nthat our models have significant improvements in both long and short-video\nunderstanding. Our models and code have been made publicly available at\nhttps://vision-cair.github.io/Goldfish_website/",
      "upvotes": 7
    },
    {
      "title": "ThinkGrasp: A Vision-Language System for Strategic Part Grasping in Clutter",
      "url": "https://huggingface.co/papers/2407.11298",
      "authors": [
        "Xupeng Zhu",
        "Ondrej Biza",
        "Shuo Jiang",
        "Linfeng Zhao",
        "Haojie Huang",
        "Yu Qi",
        "Robert Platt"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11298.pdf",
      "abstract": "Robotic grasping in cluttered environments remains a significant challenge\ndue to occlusions and complex object arrangements. We have developed\nThinkGrasp, a plug-and-play vision-language grasping system that makes use of\nGPT-4o's advanced contextual reasoning for heavy clutter environment grasping\nstrategies. ThinkGrasp can effectively identify and generate grasp poses for\ntarget objects, even when they are heavily obstructed or nearly invisible, by\nusing goal-oriented language to guide the removal of obstructing objects. This\napproach progressively uncovers the target object and ultimately grasps it with\na few steps and a high success rate. In both simulated and real experiments,\nThinkGrasp achieved a high success rate and significantly outperformed\nstate-of-the-art methods in heavily cluttered environments or with diverse\nunseen objects, demonstrating strong generalization capabilities.",
      "upvotes": 5
    },
    {
      "title": "AUITestAgent: Automatic Requirements Oriented GUI Function Testing",
      "url": "https://huggingface.co/papers/2407.09018",
      "authors": [
        "Xuan Wang",
        "Yingchuan Wang",
        "Yu Zhang",
        "Shiyu Guo",
        "Chaoyi Chen",
        "Xin Wang",
        "Yangfan Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09018.pdf",
      "abstract": "The Graphical User Interface (GUI) is how users interact with mobile apps. To\nensure it functions properly, testing engineers have to make sure it functions\nas intended, based on test requirements that are typically written in natural\nlanguage. While widely adopted manual testing and script-based methods are\neffective, they demand substantial effort due to the vast number of GUI pages\nand rapid iterations in modern mobile apps. This paper introduces AUITestAgent,\nthe first automatic, natural language-driven GUI testing tool for mobile apps,\ncapable of fully automating the entire process of GUI interaction and function\nverification. Since test requirements typically contain interaction commands\nand verification oracles. AUITestAgent can extract GUI interactions from test\nrequirements via dynamically organized agents. Then, AUITestAgent employs a\nmulti-dimensional data extraction strategy to retrieve data relevant to the\ntest requirements from the interaction trace and perform verification.\nExperiments on customized benchmarks demonstrate that AUITestAgent outperforms\nexisting tools in the quality of generated GUI interactions and achieved the\naccuracy of verifications of 94%. Moreover, field deployment in Meituan has\nshown AUITestAgent's practical usability, with it detecting 4 new functional\nbugs during 10 regression tests in two months.",
      "upvotes": 5
    },
    {
      "title": "Audio Conditioning for Music Generation via Discrete Bottleneck Features",
      "url": "https://huggingface.co/papers/2407.12563",
      "authors": [
        "Yossi Adi",
        "Jade Copet",
        "Axel Roebel",
        "Alexandre Défossez"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12563.pdf",
      "abstract": "While most music generation models use textual or parametric conditioning\n(e.g. tempo, harmony, musical genre), we propose to condition a language model\nbased music generation system with audio input. Our exploration involves two\ndistinct strategies. The first strategy, termed textual inversion, leverages a\npre-trained text-to-music model to map audio input to corresponding\n\"pseudowords\" in the textual embedding space. For the second model we train a\nmusic language model from scratch jointly with a text conditioner and a\nquantized audio feature extractor. At inference time, we can mix textual and\naudio conditioning and balance them thanks to a novel double classifier free\nguidance method. We conduct automatic and human studies that validates our\napproach. We will release the code and we provide music samples on\nhttps://musicgenstyle.github.io in order to show the quality of our model.",
      "upvotes": 5
    },
    {
      "title": "Splatfacto-W: A Nerfstudio Implementation of Gaussian Splatting for Unconstrained Photo Collections",
      "url": "https://huggingface.co/papers/2407.12306",
      "authors": [
        "Congrong Xu",
        "Justin Kerr",
        "Angjoo Kanazawa"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12306.pdf",
      "abstract": "Novel view synthesis from unconstrained in-the-wild image collections remains\na significant yet challenging task due to photometric variations and transient\noccluders that complicate accurate scene reconstruction. Previous methods have\napproached these issues by integrating per-image appearance features embeddings\nin Neural Radiance Fields (NeRFs). Although 3D Gaussian Splatting (3DGS) offers\nfaster training and real-time rendering, adapting it for unconstrained image\ncollections is non-trivial due to the substantially different architecture. In\nthis paper, we introduce Splatfacto-W, an approach that integrates per-Gaussian\nneural color features and per-image appearance embeddings into the\nrasterization process, along with a spherical harmonics-based background model\nto represent varying photometric appearances and better depict backgrounds. Our\nkey contributions include latent appearance modeling, efficient transient\nobject handling, and precise background modeling. Splatfacto-W delivers\nhigh-quality, real-time novel view synthesis with improved scene consistency in\nin-the-wild scenarios. Our method improves the Peak Signal-to-Noise Ratio\n(PSNR) by an average of 5.3 dB compared to 3DGS, enhances training speed by 150\ntimes compared to NeRF-based methods, and achieves a similar rendering speed to\n3DGS. Additional video results and code integrated into Nerfstudio are\navailable at https://kevinxu02.github.io/splatfactow/.",
      "upvotes": 5
    },
    {
      "title": "NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models",
      "url": "https://huggingface.co/papers/2407.12366",
      "authors": [
        "Yicong Hong",
        "Zun Wang",
        "Xin Eric Wang",
        "Qi Wu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12366.pdf",
      "abstract": "Capitalizing on the remarkable advancements in Large Language Models (LLMs),\nthere is a burgeoning initiative to harness LLMs for instruction following\nrobotic navigation. Such a trend underscores the potential of LLMs to\ngeneralize navigational reasoning and diverse language understanding. However,\na significant discrepancy in agent performance is observed when integrating\nLLMs in the Vision-and-Language navigation (VLN) tasks compared to previous\ndownstream specialist models. Furthermore, the inherent capacity of language to\ninterpret and facilitate communication in agent interactions is often\nunderutilized in these integrations. In this work, we strive to bridge the\ndivide between VLN-specialized models and LLM-based navigation paradigms, while\nmaintaining the interpretative prowess of LLMs in generating linguistic\nnavigational reasoning. By aligning visual content in a frozen LLM, we\nencompass visual observation comprehension for LLMs and exploit a way to\nincorporate LLMs and navigation policy networks for effective action\npredictions and navigational reasoning. We demonstrate the data efficiency of\nthe proposed methods and eliminate the gap between LM-based agents and\nstate-of-the-art VLN specialists.",
      "upvotes": 4
    },
    {
      "title": "Practical Unlearning for Large Language Models",
      "url": "https://huggingface.co/papers/2407.10223",
      "authors": [
        "Lixu Wang",
        "Chenkai Weng",
        "Xiao Wang",
        "Qi Zhu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.10223.pdf",
      "abstract": "While LLMs have demonstrated impressive performance across various domains\nand tasks, their security issues have become increasingly severe. Machine\nunlearning (MU) has emerged as a promising solution to address these issues by\nremoving the influence of undesired data on the target model without\ncompromising its utility in other aspects. MU typically assumes full access to\nthe original training data to preserve utility, which is difficult to achieve\nin LLM unlearning. Existing LLM unlearning methods often assume access to data\nmost affected by undesired data unlearning. However, this assumption\nunderestimates the entanglement among various LLM capabilities and ignores data\naccess limitations due to various issues. Moreover, these LLM unlearning\nmethods do not sufficiently consider that unlearning requests in real-world\nscenarios are continuously emerging. To overcome these challenges and achieve\npractical LLM unlearning, we propose the O3 framework. The O3 framework\nincludes an Out-Of-Distribution (OOD) detector to measure the similarity\nbetween input and unlearning data, and an Orthogonal low-rank adapter (LoRA)\nfor continuously unlearning requested data. The OOD detector is trained with a\nnovel contrastive entropy loss and utilizes a local-global layer-aggregated\nscoring mechanism. The orthogonal LoRA achieves parameter disentanglement among\ncontinual unlearning requests. During inference, our O3 framework can smartly\ndecide whether and to what extent to load the unlearning LoRA based on the OOD\ndetector's predictions. Notably, O3's effectiveness does not rely on any\nretained data. We conducted extensive experiments on O3 and state-of-the-art\nLLM unlearning methods across three tasks and seven datasets. The results\nindicate that O3 consistently achieves the best trade-off between unlearning\neffectiveness and utility preservation, especially when facing continuous\nunlearning requests.",
      "upvotes": 4
    },
    {
      "title": "The Art of Saying No: Contextual Noncompliance in Language Models",
      "url": "https://huggingface.co/papers/2407.12043",
      "authors": [
        "Vidhisha Balachandran",
        "Pradeep Dasigi",
        "Abhilasha Ravichander",
        "Sarah Wiegreffe",
        "Nouha Dziri",
        "Khyathi Chandu",
        "Jack Hessel",
        "Yulia Tsvetkov",
        "Noah A. Smith",
        "Yejin Choi",
        "Hannaneh Hajishirzi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12043.pdf",
      "abstract": "Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities.",
      "upvotes": 4
    },
    {
      "title": "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection",
      "url": "https://huggingface.co/papers/2407.11854",
      "authors": [
        "Marc-André Carbonneau",
        "Ben Swanson"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11854.pdf",
      "abstract": "Grammatical Error Detection (GED) methods rely heavily on human annotated\nerror corpora. However, these annotations are unavailable in many low-resource\nlanguages. In this paper, we investigate GED in this context. Leveraging the\nzero-shot cross-lingual transfer capabilities of multilingual pre-trained\nlanguage models, we train a model using data from a diverse set of languages to\ngenerate synthetic errors in other languages. These synthetic error corpora are\nthen used to train a GED model. Specifically we propose a two-stage fine-tuning\npipeline where the GED model is first fine-tuned on multilingual synthetic data\nfrom target languages followed by fine-tuning on human-annotated GED corpora\nfrom source languages. This approach outperforms current state-of-the-art\nannotation-free GED methods. We also analyse the errors produced by our method\nand other strong baselines, finding that our approach produces errors that are\nmore diverse and more similar to human errors.",
      "upvotes": 2
    },
    {
      "title": "Towards Understanding Unsafe Video Generation",
      "url": "https://huggingface.co/papers/2407.12581",
      "authors": [
        "Aiping Xiong",
        "Yang Zhang",
        "Tianhao Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12581.pdf",
      "abstract": "Video generation models (VGMs) have demonstrated the capability to synthesize\nhigh-quality output. It is important to understand their potential to produce\nunsafe content, such as violent or terrifying videos. In this work, we provide\na comprehensive understanding of unsafe video generation.\n  First, to confirm the possibility that these models could indeed generate\nunsafe videos, we choose unsafe content generation prompts collected from 4chan\nand Lexica, and three open-source SOTA VGMs to generate unsafe videos. After\nfiltering out duplicates and poorly generated content, we created an initial\nset of 2112 unsafe videos from an original pool of 5607 videos. Through\nclustering and thematic coding analysis of these generated videos, we identify\n5 unsafe video categories: Distorted/Weird, Terrifying, Pornographic,\nViolent/Bloody, and Political. With IRB approval, we then recruit online\nparticipants to help label the generated videos. Based on the annotations\nsubmitted by 403 participants, we identified 937 unsafe videos from the initial\nvideo set. With the labeled information and the corresponding prompts, we\ncreated the first dataset of unsafe videos generated by VGMs.\n  We then study possible defense mechanisms to prevent the generation of unsafe\nvideos. Existing defense methods in image generation focus on filtering either\ninput prompt or output results. We propose a new approach called Latent\nVariable Defense (LVD), which works within the model's internal sampling\nprocess. LVD can achieve 0.90 defense accuracy while reducing time and\ncomputing resources by 10x when sampling a large number of unsafe prompts.",
      "upvotes": 0
    }
  ]
}