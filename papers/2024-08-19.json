{
  "date": "2024-08-19",
  "papers": [
    {
      "title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models",
      "url": "https://huggingface.co/papers/2408.08872",
      "authors": [
        "Jun Wang",
        "Senthil Purushwalkam",
        "Honglu Zhou",
        "Viraj Prabhu",
        "Yutong Dai",
        "Michael S Ryoo",
        "Shrikant Kendre",
        "Can Qin",
        "Shu Zhang",
        "Chia-Chih Chen",
        "Ning Yu",
        "Juntao Tan",
        "Tulika Manoj Awalgaonkar",
        "Shelby Heinecke",
        "Huan Wang",
        "Yejin Choi",
        "Ludwig Schmidt"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08872.pdf",
      "abstract": "This report introduces xGen-MM (also known as BLIP-3), a framework for\ndeveloping Large Multimodal Models (LMMs). The framework comprises meticulously\ncurated datasets, a training recipe, model architectures, and a resulting suite\nof LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen\ninitiative on foundation AI models. Our models undergo rigorous evaluation\nacross a range of tasks, including both single and multi-image benchmarks. Our\npre-trained base model exhibits strong in-context learning capabilities and the\ninstruction-tuned model demonstrates competitive performance among open-source\nLMMs with similar model sizes. In addition, we introduce a safety-tuned model\nwith DPO, aiming to mitigate harmful behaviors such as hallucinations and\nimprove safety. We open-source our models, curated large-scale datasets, and\nour fine-tuning codebase to facilitate further advancements in LMM research.\nAssociated resources will be available on our project page above.",
      "upvotes": 97
    },
    {
      "title": "JPEG-LM: LLMs as Image Generators with Canonical Codec Representations",
      "url": "https://huggingface.co/papers/2408.08459",
      "authors": [
        "Marjan Ghazvininejad",
        "Pang Wei Koh",
        "Yulia Tsvetkov"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08459.pdf",
      "abstract": "Recent work in image and video generation has been adopting the\nautoregressive LLM architecture due to its generality and potentially easy\nintegration into multi-modal systems. The crux of applying autoregressive\ntraining in language generation to visual generation is discretization --\nrepresenting continuous data like images and videos as discrete tokens. Common\nmethods of discretizing images and videos include modeling raw pixel values,\nwhich are prohibitively lengthy, or vector quantization, which requires\nconvoluted pre-hoc training. In this work, we propose to directly model images\nand videos as compressed files saved on computers via canonical codecs (e.g.,\nJPEG, AVC/H.264). Using the default Llama architecture without any\nvision-specific modifications, we pretrain JPEG-LM from scratch to generate\nimages (and AVC-LM to generate videos as a proof of concept), by directly\noutputting compressed file bytes in JPEG and AVC formats. Evaluation of image\ngeneration shows that this simple and straightforward approach is more\neffective than pixel-based modeling and sophisticated vector quantization\nbaselines (on which our method yields a 31% reduction in FID). Our analysis\nshows that JPEG-LM has an especial advantage over vector quantization models in\ngenerating long-tail visual elements. Overall, we show that using canonical\ncodec representations can help lower the barriers between language generation\nand visual generation, facilitating future research on multi-modal\nlanguage/image/video LLMs.",
      "upvotes": 44
    },
    {
      "title": "Automated Design of Agentic Systems",
      "url": "https://huggingface.co/papers/2408.08435",
      "authors": [
        "Shengran Hu",
        "Cong Lu",
        "Jeff Clune"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08435.pdf",
      "abstract": "Researchers are investing substantial effort in developing powerful\ngeneral-purpose agents, wherein Foundation Models are used as modules within\nagentic systems (e.g. Chain-of-Thought, Self-Reflection, Toolformer). However,\nthe history of machine learning teaches us that hand-designed solutions are\neventually replaced by learned solutions. We formulate a new research area,\nAutomated Design of Agentic Systems (ADAS), which aims to automatically create\npowerful agentic system designs, including inventing novel building blocks\nand/or combining them in new ways. We further demonstrate that there is an\nunexplored yet promising approach within ADAS where agents can be defined in\ncode and new agents can be automatically discovered by a meta agent programming\never better ones in code. Given that programming languages are Turing Complete,\nthis approach theoretically enables the learning of any possible agentic\nsystem: including novel prompts, tool use, control flows, and combinations\nthereof. We present a simple yet effective algorithm named Meta Agent Search to\ndemonstrate this idea, where a meta agent iteratively programs interesting new\nagents based on an ever-growing archive of previous discoveries. Through\nextensive experiments across multiple domains including coding, science, and\nmath, we show that our algorithm can progressively invent agents with novel\ndesigns that greatly outperform state-of-the-art hand-designed agents.\nImportantly, we consistently observe the surprising result that agents invented\nby Meta Agent Search maintain superior performance even when transferred across\ndomains and models, demonstrating their robustness and generality. Provided we\ndevelop it safely, our work illustrates the potential of an exciting new\nresearch direction toward automatically designing ever-more powerful agentic\nsystems to benefit humanity.",
      "upvotes": 38
    },
    {
      "title": "Surgical SAM 2: Real-time Segment Anything in Surgical Video by Efficient Frame Pruning",
      "url": "https://huggingface.co/papers/2408.07931",
      "authors": [
        "Mingxuan Hong",
        "Yueming Jin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07931.pdf",
      "abstract": "Surgical video segmentation is a critical task in computer-assisted surgery\nand is vital for enhancing surgical quality and patient outcomes. Recently, the\nSegment Anything Model 2 (SAM2) framework has shown superior advancements in\nimage and video segmentation. However, SAM2 struggles with efficiency due to\nthe high computational demands of processing high-resolution images and complex\nand long-range temporal dynamics in surgical videos. To address these\nchallenges, we introduce Surgical SAM 2 (SurgSAM-2), an advanced model to\nutilize SAM2 with an Efficient Frame Pruning (EFP) mechanism, to facilitate\nreal-time surgical video segmentation. The EFP mechanism dynamically manages\nthe memory bank by selectively retaining only the most informative frames,\nreducing memory usage and computational cost while maintaining high\nsegmentation accuracy. Our extensive experiments demonstrate that SurgSAM-2\nsignificantly improves both efficiency and segmentation accuracy compared to\nthe vanilla SAM2. Remarkably, SurgSAM-2 achieves a 3times FPS compared with\nSAM2, while also delivering state-of-the-art performance after fine-tuning with\nlower-resolution data. These advancements establish SurgSAM-2 as a leading\nmodel for surgical video analysis, making real-time surgical video segmentation\nin resource-constrained environments a feasible reality.",
      "upvotes": 18
    },
    {
      "title": "TurboEdit: Instant text-based image editing",
      "url": "https://huggingface.co/papers/2408.08332",
      "authors": [
        "Zongze Wu",
        "Nicholas Kolkin",
        "Jonathan Brandt",
        "Richard Zhang",
        "Eli Shechtman"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08332.pdf",
      "abstract": "We address the challenges of precise image inversion and disentangled image\nediting in the context of few-step diffusion models. We introduce an encoder\nbased iterative inversion technique. The inversion network is conditioned on\nthe input image and the reconstructed image from the previous step, allowing\nfor correction of the next reconstruction towards the input image. We\ndemonstrate that disentangled controls can be easily achieved in the few-step\ndiffusion model by conditioning on an (automatically generated) detailed text\nprompt. To manipulate the inverted image, we freeze the noise maps and modify\none attribute in the text prompt (either manually or via instruction based\nediting driven by an LLM), resulting in the generation of a new image similar\nto the input image with only one attribute changed. It can further control the\nediting strength and accept instructive text prompt. Our approach facilitates\nrealistic text-guided image edits in real-time, requiring only 8 number of\nfunctional evaluations (NFEs) in inversion (one-time cost) and 4 NFEs per edit.\nOur method is not only fast, but also significantly outperforms\nstate-of-the-art multi-step diffusion editing techniques.",
      "upvotes": 18
    },
    {
      "title": "Fine-tuning Large Language Models with Human-inspired Learning Strategies in Medical Question Answering",
      "url": "https://huggingface.co/papers/2408.07888",
      "authors": [
        "Andrew M. Bean",
        "Robert McCraith",
        "Adam Mahdi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07888.pdf",
      "abstract": "Training Large Language Models (LLMs) incurs substantial data-related costs,\nmotivating the development of data-efficient training methods through optimised\ndata ordering and selection. Human-inspired learning strategies, such as\ncurriculum learning, offer possibilities for efficient training by organising\ndata according to common human learning practices. Despite evidence that\nfine-tuning with curriculum learning improves the performance of LLMs for\nnatural language understanding tasks, its effectiveness is typically assessed\nusing a single model. In this work, we extend previous research by evaluating\nboth curriculum-based and non-curriculum-based learning strategies across\nmultiple LLMs, using human-defined and automated data labels for medical\nquestion answering. Our results indicate a moderate impact of using\nhuman-inspired learning strategies for fine-tuning LLMs, with maximum accuracy\ngains of 1.77% per model and 1.81% per dataset. Crucially, we demonstrate that\nthe effectiveness of these strategies varies significantly across different\nmodel-dataset combinations, emphasising that the benefits of a specific\nhuman-inspired strategy for fine-tuning LLMs do not generalise. Additionally,\nwe find evidence that curriculum learning using LLM-defined question difficulty\noutperforms human-defined difficulty, highlighting the potential of using\nmodel-generated measures for optimal curriculum design.",
      "upvotes": 10
    },
    {
      "title": "D5RL: Diverse Datasets for Data-Driven Deep Reinforcement Learning",
      "url": "https://huggingface.co/papers/2408.08441",
      "authors": [
        "Rafael Rafailov",
        "Kyle Hatch",
        "Laura Smith",
        "Aviral Kumar",
        "Ilya Kostrikov",
        "Philippe Hansen-Estruch",
        "Victor Kolev",
        "Philip Ball",
        "Jiajun Wu",
        "Chelsea Finn",
        "Sergey Levine"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08441.pdf",
      "abstract": "Offline reinforcement learning algorithms hold the promise of enabling\ndata-driven RL methods that do not require costly or dangerous real-world\nexploration and benefit from large pre-collected datasets. This in turn can\nfacilitate real-world applications, as well as a more standardized approach to\nRL research. Furthermore, offline RL methods can provide effective\ninitializations for online finetuning to overcome challenges with exploration.\nHowever, evaluating progress on offline RL algorithms requires effective and\nchallenging benchmarks that capture properties of real-world tasks, provide a\nrange of task difficulties, and cover a range of challenges both in terms of\nthe parameters of the domain (e.g., length of the horizon, sparsity of rewards)\nand the parameters of the data (e.g., narrow demonstration data or broad\nexploratory data). While considerable progress in offline RL in recent years\nhas been enabled by simpler benchmark tasks, the most widely used datasets are\nincreasingly saturating in performance and may fail to reflect properties of\nrealistic tasks. We propose a new benchmark for offline RL that focuses on\nrealistic simulations of robotic manipulation and locomotion environments,\nbased on models of real-world robotic systems, and comprising a variety of data\nsources, including scripted data, play-style data collected by human\nteleoperators, and other data sources. Our proposed benchmark covers\nstate-based and image-based domains, and supports both offline RL and online\nfine-tuning evaluation, with some of the tasks specifically designed to require\nboth pre-training and fine-tuning. We hope that our proposed benchmark will\nfacilitate further progress on both offline RL and fine-tuning algorithms.\nWebsite with code, examples, tasks, and data is available at\nhttps://sites.google.com/view/d5rl/",
      "upvotes": 6
    }
  ]
}