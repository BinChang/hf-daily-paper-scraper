{
  "date": "2024-07-02",
  "papers": [
    {
      "title": "We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?",
      "url": "https://huggingface.co/papers/2407.01284",
      "authors": [
        "Runqi Qiao",
        "Qiuna Tan",
        "Minhui Wu",
        "Xiaoshuai Song",
        "Zhuoma GongQue",
        "Shanglin Lei",
        "Zhe Wei",
        "Miaoxuan Zhang",
        "Yifan Zhang",
        "Xiao Zong",
        "Muxi Diao",
        "Zhimin Bao",
        "Chen Li",
        "Honggang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01284.pdf",
      "abstract": "Visual mathematical reasoning, as a fundamental visual reasoning ability, has\nreceived widespread attention from the Large Multimodal Models (LMMs)\ncommunity. Existing benchmarks, such as MathVista and MathVerse, focus more on\nthe result-oriented performance but neglect the underlying principles in\nknowledge acquisition and generalization. Inspired by human-like mathematical\nreasoning, we introduce WE-MATH, the first benchmark specifically designed to\nexplore the problem-solving principles beyond end-to-end performance. We\nmeticulously collect and categorize 6.5K visual math problems, spanning 67\nhierarchical knowledge concepts and five layers of knowledge granularity. We\ndecompose composite problems into sub-problems according to the required\nknowledge concepts and introduce a novel four-dimensional metric, namely\nInsufficient Knowledge (IK), Inadequate Generalization (IG), Complete Mastery\n(CM), and Rote Memorization (RM), to hierarchically assess inherent issues in\nLMMs' reasoning process. With WE-MATH, we conduct a thorough evaluation of\nexisting LMMs in visual mathematical reasoning and reveal a negative\ncorrelation between solving steps and problem-specific performance. We confirm\nthe IK issue of LMMs can be effectively improved via knowledge augmentation\nstrategies. More notably, the primary challenge of GPT-4o has significantly\ntransitioned from IK to IG, establishing it as the first LMM advancing towards\nthe knowledge generalization stage. In contrast, other LMMs exhibit a marked\ninclination towards Rote Memorization - they correctly solve composite problems\ninvolving multiple knowledge concepts yet fail to answer sub-problems. We\nanticipate that WE-MATH will open new pathways for advancements in visual\nmathematical reasoning for LMMs. The WE-MATH data and evaluation code are\navailable at https://github.com/We-Math/We-Math.",
      "upvotes": 75
    },
    {
      "title": "ROS-LLM: A ROS framework for embodied AI with task feedback and structured reasoning",
      "url": "https://huggingface.co/papers/2406.19741",
      "authors": [
        "Antoine Grosnit",
        "Matthieu Zimmer",
        "Jinlong Wang",
        "Xinyu Zhang",
        "Yao Zhao",
        "Anbang Zhai",
        "Davide Tateo",
        "Cesar Cadena",
        "Marco Hutter",
        "Jan Peters",
        "Guangjian Tian",
        "Yuzheng Zhuang",
        "Kun Shao",
        "Xingyue Quan",
        "Jianye Hao",
        "Jun Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.19741.pdf",
      "abstract": "We present a framework for intuitive robot programming by non-experts,\nleveraging natural language prompts and contextual information from the Robot\nOperating System (ROS). Our system integrates large language models (LLMs),\nenabling non-experts to articulate task requirements to the system through a\nchat interface. Key features of the framework include: integration of ROS with\nan AI agent connected to a plethora of open-source and commercial LLMs,\nautomatic extraction of a behavior from the LLM output and execution of ROS\nactions/services, support for three behavior modes (sequence, behavior tree,\nstate machine), imitation learning for adding new robot actions to the library\nof possible actions, and LLM reflection via human and environment feedback.\nExtensive experiments validate the framework, showcasing robustness,\nscalability, and versatility in diverse scenarios, including long-horizon\ntasks, tabletop rearrangements, and remote supervisory control. To facilitate\nthe adoption of our framework and support the reproduction of our results, we\nhave made our code open-source. You can access it at:\nhttps://github.com/huawei-noah/HEBO/tree/master/ROSLLM.",
      "upvotes": 59
    },
    {
      "title": "ColPali: Efficient Document Retrieval with Vision Language Models",
      "url": "https://huggingface.co/papers/2407.01449",
      "authors": [
        "Gautier Viaud",
        "CÃ©line Hudelot",
        "Pierre Colombo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01449.pdf",
      "abstract": "Documents are visually rich structures that convey information through text,\nas well as tables, figures, page layouts, or fonts. While modern document\nretrieval systems exhibit strong performance on query-to-text matching, they\nstruggle to exploit visual cues efficiently, hindering their performance on\npractical document retrieval applications such as Retrieval Augmented\nGeneration. To benchmark current systems on visually rich document retrieval,\nwe introduce the Visual Document Retrieval Benchmark ViDoRe, composed of\nvarious page-level retrieving tasks spanning multiple domains, languages, and\nsettings. The inherent shortcomings of modern systems motivate the introduction\nof a new retrieval model architecture, ColPali, which leverages the document\nunderstanding capabilities of recent Vision Language Models to produce\nhigh-quality contextualized embeddings solely from images of document pages.\nCombined with a late interaction matching mechanism, ColPali largely\noutperforms modern document retrieval pipelines while being drastically faster\nand end-to-end trainable.",
      "upvotes": 41
    },
    {
      "title": "LiteSearch: Efficacious Tree Search for LLM",
      "url": "https://huggingface.co/papers/2407.00320",
      "authors": [
        "Ante Wang",
        "Jinsong Su",
        "Dong Yu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00320.pdf",
      "abstract": "Recent research suggests that tree search algorithms (e.g. Monte Carlo Tree\nSearch) can dramatically boost LLM performance on complex mathematical\nreasoning tasks. However, they often require more than 10 times the\ncomputational resources of greedy decoding due to wasteful search strategies,\nmaking them difficult to be deployed in practical applications. This study\nintroduces a novel guided tree search algorithm with dynamic node selection and\nnode-level exploration budget (maximum number of children) calculation to\ntackle this issue. By considering the search progress towards the final answer\n(history) and the guidance from a value network (future) trained without any\nstep-wise annotations, our algorithm iteratively selects the most promising\ntree node before expanding it within the boundaries of the allocated\ncomputational budget. Experiments conducted on the GSM8K and TabMWP datasets\ndemonstrate that our approach not only offers competitive performance but also\nenjoys significantly lower computational costs compared to baseline methods.",
      "upvotes": 37
    },
    {
      "title": "RegMix: Data Mixture as Regression for Language Model Pre-training",
      "url": "https://huggingface.co/papers/2407.01492",
      "authors": [
        "Guangtao Zeng",
        "Tianyu Pang",
        "Jing Jiang",
        "Min Lin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01492.pdf",
      "abstract": "The data mixture for large language model pre-training significantly impacts\nperformance, yet how to determine an effective mixture remains unclear. We\npropose RegMix to automatically identify a high-performing data mixture by\nformulating it as a regression task. RegMix involves training a set of small\nmodels with diverse data mixtures and fitting a regression model to predict\ntheir performance given their respective mixtures. With the fitted regression\nmodel, we simulate the top-ranked mixture and use it to train a large-scale\nmodel with orders of magnitude more compute. To empirically validate RegMix, we\ntrain 512 models with 1M parameters for 1B tokens of different mixtures to fit\nthe regression model and find the optimal mixture. Using this mixture we train\na 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we\nfind performs best among 64 candidate 1B parameter models with other mixtures.\nFurther, our method demonstrates superior performance compared to human\nselection and achieves results that match or surpass DoReMi, while utilizing\nonly 10% of the compute budget. Our experiments also show that (1) Data\nmixtures significantly impact performance with single-task performance\nvariations of up to 14.6%; (2) Web corpora rather than data perceived as\nhigh-quality like Wikipedia have the strongest positive correlation with\ndownstream performance; (3) Domains interact in complex ways often\ncontradicting common sense, thus automatic approaches like RegMix are needed;\n(4) Data mixture effects transcend scaling laws, and our approach captures the\ncomplexity by considering all domains together. Our code is available at\nhttps://github.com/sail-sg/regmix.",
      "upvotes": 34
    },
    {
      "title": "MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation",
      "url": "https://huggingface.co/papers/2407.00468",
      "authors": [
        "Jinsheng Huang",
        "Fu Zeng",
        "Ye Yuan",
        "Zhihui Guo",
        "Yichi Zhang",
        "Jingyang Yuan",
        "Wei Ju",
        "Luchen Liu",
        "Tianyu Liu",
        "Baobao Chang",
        "Ming Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00468.pdf",
      "abstract": "Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding\nand reasoning abilities, often assessed through multiple-choice questions\n(MCQs) that include an image, a question, and several options. However, many\nbenchmarks used for such evaluations suffer from systematic biases. Remarkably,\nLarge Language Models (LLMs) without any visual perception capabilities achieve\nnon-trivial performance, undermining the credibility of these evaluations. To\naddress this issue while maintaining the efficiency of MCQ evaluations, we\npropose MMEvalPro, a benchmark designed to avoid Type-I errors through a\ntrilogy evaluation pipeline and more rigorous metrics. For each original\nquestion from existing benchmarks, human annotators augment it by creating one\nperception question and one knowledge anchor question through a meticulous\nannotation process. MMEvalPro comprises 2,138 question triplets, totaling\n6,414 distinct questions. Two-thirds of these questions are manually labeled\nby human experts, while the rest are sourced from existing benchmarks (MMMU,\nScienceQA, and MathVista). Compared with the existing benchmarks, our\nexperiments with the latest LLMs and LMMs demonstrate that MMEvalPro is more\nchallenging (the best LMM lags behind human performance by 31.73%, compared\nto an average gap of 8.03% in previous benchmarks) and more trustworthy (the\nbest LLM trails the best LMM by 23.09%, whereas the gap for previous\nbenchmarks is just 14.64%). Our in-depth analysis explains the reason for\nthe large performance gap and justifies the trustworthiness of evaluation,\nunderscoring its significant potential for advancing future research.",
      "upvotes": 34
    },
    {
      "title": "Wavelets Are All You Need for Autoregressive Image Generation",
      "url": "https://huggingface.co/papers/2406.19997",
      "authors": [
        "Shai Dekel"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.19997.pdf",
      "abstract": "In this paper, we take a new approach to autoregressive image generation that\nis based on two main ingredients. The first is wavelet image coding, which\nallows to tokenize the visual details of an image from coarse to fine details\nby ordering the information starting with the most significant bits of the most\nsignificant wavelet coefficients. The second is a variant of a language\ntransformer whose architecture is re-designed and optimized for token sequences\nin this 'wavelet language'. The transformer learns the significant statistical\ncorrelations within a token sequence, which are the manifestations of\nwell-known correlations between the wavelet subbands at various resolutions. We\nshow experimental results with conditioning on the generation process.",
      "upvotes": 29
    },
    {
      "title": "Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical Reasoning",
      "url": "https://huggingface.co/papers/2407.00782",
      "authors": [
        "Mingjie Zhan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00782.pdf",
      "abstract": "Direct Preference Optimization (DPO) has proven effective at improving the\nperformance of large language models (LLMs) on downstream tasks such as\nreasoning and alignment. In this work, we propose Step-Controlled DPO (SCDPO),\na method for automatically providing stepwise error supervision by creating\nnegative samples of mathematical reasoning rationales that start making errors\nat a specified step. By applying these samples in DPO training, SCDPO can\nbetter align the model to understand reasoning errors and output accurate\nreasoning steps. We apply SCDPO to both code-integrated and chain-of-thought\nsolutions, empirically showing that it consistently improves the performance\ncompared to naive DPO on three different SFT models, including one existing SFT\nmodel and two models we finetuned. Qualitative analysis of the credit\nassignment of SCDPO and DPO demonstrates the effectiveness of SCDPO at\nidentifying errors in mathematical solutions. We then apply SCDPO to an\nInternLM2-20B model, resulting in a 20B model that achieves high scores of\n88.5% on GSM8K and 58.1% on MATH, rivaling all other open-source LLMs, showing\nthe great potential of our method.",
      "upvotes": 23
    },
    {
      "title": "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP",
      "url": "https://huggingface.co/papers/2407.00402",
      "authors": [
        "Aviya Maimon",
        "Ido Dagan",
        "Reut Tsarfaty"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00402.pdf",
      "abstract": "Improvements in language models' capabilities have pushed their applications\ntowards longer contexts, making long-context evaluation and development an\nactive research area. However, many disparate use-cases are grouped together\nunder the umbrella term of \"long-context\", defined simply by the total length\nof the model's input, including - for example - Needle-in-a-Haystack tasks,\nbook summarization, and information aggregation. Given their varied difficulty,\nin this position paper we argue that conflating different tasks by their\ncontext length is unproductive. As a community, we require a more precise\nvocabulary to understand what makes long-context tasks similar or different. We\npropose to unpack the taxonomy of long-context based on the properties that\nmake them more difficult with longer contexts. We propose two orthogonal axes\nof difficulty: (I) Diffusion: How hard is it to find the necessary information\nin the context? (II) Scope: How much necessary information is there to find? We\nsurvey the literature on long-context, provide justification for this taxonomy\nas an informative descriptor, and situate the literature with respect to it. We\nconclude that the most difficult and interesting settings, whose necessary\ninformation is very long and highly diffused within the input, is severely\nunder-explored. By using a descriptive vocabulary and discussing the relevant\nproperties of difficulty in long-context, we can implement more informed\nresearch in this area. We call for a careful design of tasks and benchmarks\nwith distinctly long context, taking into account the characteristics that make\nit qualitatively different from shorter context.",
      "upvotes": 22
    },
    {
      "title": "InstantStyle-Plus: Style Transfer with Content-Preserving in Text-to-Image Generation",
      "url": "https://huggingface.co/papers/2407.00788",
      "authors": [
        "Renyuan Huang",
        "Qixun Wang",
        "Xu Bai"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00788.pdf",
      "abstract": "Style transfer is an inventive process designed to create an image that\nmaintains the essence of the original while embracing the visual style of\nanother. Although diffusion models have demonstrated impressive generative\npower in personalized subject-driven or style-driven applications, existing\nstate-of-the-art methods still encounter difficulties in achieving a seamless\nbalance between content preservation and style enhancement. For example,\namplifying the style's influence can often undermine the structural integrity\nof the content. To address these challenges, we deconstruct the style transfer\ntask into three core elements: 1) Style, focusing on the image's aesthetic\ncharacteristics; 2) Spatial Structure, concerning the geometric arrangement and\ncomposition of visual elements; and 3) Semantic Content, which captures the\nconceptual meaning of the image. Guided by these principles, we introduce\nInstantStyle-Plus, an approach that prioritizes the integrity of the original\ncontent while seamlessly integrating the target style. Specifically, our method\naccomplishes style injection through an efficient, lightweight process,\nutilizing the cutting-edge InstantStyle framework. To reinforce the content\npreservation, we initiate the process with an inverted content latent noise and\na versatile plug-and-play tile ControlNet for preserving the original image's\nintrinsic layout. We also incorporate a global semantic adapter to enhance the\nsemantic content's fidelity. To safeguard against the dilution of style\ninformation, a style extractor is employed as discriminator for providing\nsupplementary style guidance. Codes will be available at\nhttps://github.com/instantX-research/InstantStyle-Plus.",
      "upvotes": 22
    },
    {
      "title": "DiffIR2VR-Zero: Zero-Shot Video Restoration with Diffusion-based Image Restoration Models",
      "url": "https://huggingface.co/papers/2407.01519",
      "authors": [
        "Chin-Yang Lin",
        "Zhixiang Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01519.pdf",
      "abstract": "This paper introduces a method for zero-shot video restoration using\npre-trained image restoration diffusion models. Traditional video restoration\nmethods often need retraining for different settings and struggle with limited\ngeneralization across various degradation types and datasets. Our approach uses\na hierarchical token merging strategy for keyframes and local frames, combined\nwith a hybrid correspondence mechanism that blends optical flow and\nfeature-based nearest neighbor matching (latent merging). We show that our\nmethod not only achieves top performance in zero-shot video restoration but\nalso significantly surpasses trained models in generalization across diverse\ndatasets and extreme degradations (8times super-resolution and high-standard\ndeviation video denoising). We present evidence through quantitative metrics\nand visual comparisons on various challenging datasets. Additionally, our\ntechnique works with any 2D restoration diffusion model, offering a versatile\nand powerful tool for video enhancement tasks without extensive retraining.\nThis research leads to more efficient and widely applicable video restoration\ntechnologies, supporting advancements in fields that require high-quality video\noutput. See our project page for video results at\nhttps://jimmycv07.github.io/DiffIR2VR_web/.",
      "upvotes": 22
    },
    {
      "title": "RealTalk: Real-time and Realistic Audio-driven Face Generation with 3D Facial Prior-guided Identity Alignment Network",
      "url": "https://huggingface.co/papers/2406.18284",
      "authors": [
        "Zhonggan Ding",
        "Jian Yang",
        "Xiaobin Hu",
        "Jiangning Zhang",
        "Donghao Luo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.18284.pdf",
      "abstract": "Person-generic audio-driven face generation is a challenging task in computer\nvision. Previous methods have achieved remarkable progress in audio-visual\nsynchronization, but there is still a significant gap between current results\nand practical applications. The challenges are two-fold: 1) Preserving unique\nindividual traits for achieving high-precision lip synchronization. 2)\nGenerating high-quality facial renderings in real-time performance. In this\npaper, we propose a novel generalized audio-driven framework RealTalk, which\nconsists of an audio-to-expression transformer and a high-fidelity\nexpression-to-face renderer. In the first component, we consider both identity\nand intra-personal variation features related to speaking lip movements. By\nincorporating cross-modal attention on the enriched facial priors, we can\neffectively align lip movements with audio, thus attaining greater precision in\nexpression prediction. In the second component, we design a lightweight facial\nidentity alignment (FIA) module which includes a lip-shape control structure\nand a face texture reference structure. This novel design allows us to generate\nfine details in real-time, without depending on sophisticated and inefficient\nfeature alignment modules. Our experimental results, both quantitative and\nqualitative, on public datasets demonstrate the clear advantages of our method\nin terms of lip-speech synchronization and generation quality. Furthermore, our\nmethod is efficient and requires fewer computational resources, making it\nwell-suited to meet the needs of practical applications.",
      "upvotes": 19
    },
    {
      "title": "E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS",
      "url": "https://huggingface.co/papers/2406.18009",
      "authors": [
        "Manthan Thakker",
        "Canrun Li",
        "Chung-Hsien Tsai",
        "Zhen Xiao",
        "Hemin Yang",
        "Zirun Zhu",
        "Min Tang",
        "Yanqing Liu",
        "Sheng Zhao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.18009.pdf",
      "abstract": "This paper introduces Embarrassingly Easy Text-to-Speech (E2 TTS), a fully\nnon-autoregressive zero-shot text-to-speech system that offers human-level\nnaturalness and state-of-the-art speaker similarity and intelligibility. In the\nE2 TTS framework, the text input is converted into a character sequence with\nfiller tokens. The flow-matching-based mel spectrogram generator is then\ntrained based on the audio infilling task. Unlike many previous works, it does\nnot require additional components (e.g., duration model, grapheme-to-phoneme)\nor complex techniques (e.g., monotonic alignment search). Despite its\nsimplicity, E2 TTS achieves state-of-the-art zero-shot TTS capabilities that\nare comparable to or surpass previous works, including Voicebox and\nNaturalSpeech 3. The simplicity of E2 TTS also allows for flexibility in the\ninput representation. We propose several variants of E2 TTS to improve\nusability during inference. See https://aka.ms/e2tts/ for demo samples.",
      "upvotes": 18
    },
    {
      "title": "MIRAI: Evaluating LLM Agents for Event Forecasting",
      "url": "https://huggingface.co/papers/2407.01231",
      "authors": [
        "Ziniu Hu",
        "Zijie Huang",
        "Wei Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01231.pdf",
      "abstract": "Recent advancements in Large Language Models (LLMs) have empowered LLM agents\nto autonomously collect world information, over which to conduct reasoning to\nsolve complex problems. Given this capability, increasing interests have been\nput into employing LLM agents for predicting international events, which can\ninfluence decision-making and shape policy development on an international\nscale. Despite such a growing interest, there is a lack of a rigorous benchmark\nof LLM agents' forecasting capability and reliability. To address this gap, we\nintroduce MIRAI, a novel benchmark designed to systematically evaluate LLM\nagents as temporal forecasters in the context of international events. Our\nbenchmark features an agentic environment with tools for accessing an extensive\ndatabase of historical, structured events and textual news articles. We refine\nthe GDELT event database with careful cleaning and parsing to curate a series\nof relational prediction tasks with varying forecasting horizons, assessing LLM\nagents' abilities from short-term to long-term forecasting. We further\nimplement APIs to enable LLM agents to utilize different tools via a code-based\ninterface. In summary, MIRAI comprehensively evaluates the agents' capabilities\nin three dimensions: 1) autonomously source and integrate critical information\nfrom large global databases; 2) write codes using domain-specific APIs and\nlibraries for tool-use; and 3) jointly reason over historical knowledge from\ndiverse formats and time to accurately predict future events. Through\ncomprehensive benchmarking, we aim to establish a reliable framework for\nassessing the capabilities of LLM agents in forecasting international events,\nthereby contributing to the development of more accurate and trustworthy models\nfor international relation analysis.",
      "upvotes": 16
    },
    {
      "title": "OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents",
      "url": "https://huggingface.co/papers/2407.00114",
      "authors": [
        "Zihao Wang",
        "Zhancun Mu",
        "Haowei Lin",
        "Ceyao Zhang",
        "Xuejie Liu",
        "Yitao Liang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00114.pdf",
      "abstract": "We present OmniJARVIS, a novel Vision-Language-Action (VLA) model for\nopen-world instruction-following agents in open-world Minecraft. Compared to\nprior works that either emit textual goals to separate controllers or produce\nthe control command directly, OmniJARVIS seeks a different path to ensure both\nstrong reasoning and efficient decision-making capabilities via unified\ntokenization of multimodal interaction data. First, we introduce a\nself-supervised approach to learn a behavior encoder that produces discretized\ntokens for behavior trajectories tau = {o_0, a_0, dots} and an\nimitation learning (IL) policy decoder conditioned on these tokens. These\nadditional behavior tokens will be augmented to the vocabulary of pretrained\nMultimodal Language Models (MLMs). With this encoder, we then pack long-term\nmultimodal interactions involving task instructions, memories, thoughts,\nobservations, textual responses, behavior trajectories, etc. into unified token\nsequences and model them with autoregressive transformers. Thanks to the\nsemantically meaningful behavior tokens, the resulting VLA model, OmniJARVIS,\ncan reason (by producing chain-of-thoughts), plan, answer questions, and act\n(by producing behavior tokens for the IL policy decoder). OmniJARVIS\ndemonstrates excellent performances on a comprehensive collection of atomic,\nprogrammatic, and open-ended tasks in open-world Minecraft. Our analysis\nfurther unveils the crucial design principles in interaction data formation,\nunified tokenization, and its scaling potentials.",
      "upvotes": 12
    },
    {
      "title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs",
      "url": "https://huggingface.co/papers/2407.00653",
      "authors": [
        "Sirui Xia",
        "Yanghua Xiao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00653.pdf",
      "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various\nnatural language processing (NLP) tasks, which involve increasingly complex\nreasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving\nnew knowledge from existing one.While it has been widely studied in the context\nof knowledge graphs (KGs), knowledge reasoning in LLMs remains underexplored.\nIn this paper, we introduce Chain-of-Knowledge, a comprehensive framework for\nknowledge reasoning, including methodologies for both dataset construction and\nmodel learning. For dataset construction, we create KnowReason via rule mining\non KGs. For model learning, we observe rule overfitting induced by naive\ntraining. Hence, we enhance CoK with a trial-and-error mechanism that simulates\nthe human process of internal knowledge exploration. We conduct extensive\nexperiments with KnowReason. Our results show the effectiveness of CoK in\nrefining LLMs in not only knowledge reasoning, but also general reasoning\nbenchmarkms.",
      "upvotes": 11
    },
    {
      "title": "T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge",
      "url": "https://huggingface.co/papers/2407.00088",
      "authors": [
        "Yanyong Zhang",
        "Mao Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00088.pdf",
      "abstract": "The deployment of Large Language Models (LLMs) on edge devices is\nincreasingly important to enhance on-device intelligence. Weight quantization\nis crucial for reducing the memory footprint of LLMs on devices. However,\nlow-bit LLMs necessitate mixed precision matrix multiplication (mpGEMM) of low\nprecision weights and high precision activations during inference. Existing\nsystems, lacking native support for mpGEMM, resort to dequantize weights for\nhigh precision computation. Such an indirect way can lead to a significant\ninference overhead.\n  In this paper, we introduce T-MAC, an innovative lookup table(LUT)-based\nmethod designed for efficient low-bit LLM (i.e., weight-quantized LLM)\ninference on CPUs. T-MAC directly supports mpGEMM without dequantization, while\nsimultaneously eliminating multiplications and reducing additions required.\nSpecifically, T-MAC transforms the traditional data-type-centric multiplication\nto bit-wise table lookup, and enables a unified and scalable mpGEMM solution.\n  Our LUT-based kernels scale linearly to the weight bit-width. Evaluated on\nlow-bit Llama and BitNet models, T-MAC demonstrates up to 4x increase in\nthroughput and 70% reduction in energy consumption compared to llama.cpp. For\nBitNet-b1.58-3B, T-MAC delivers a token generation throughput of 30 tokens/s\nwith a single core and 71 tokens/s with eight cores on M2-Ultra, and 11\ntokens/s on lower-end devices like Raspberry Pi 5, which significantly exceeds\nthe adult average reading speed. T-MAC with LUT-based computing paradigm, paves\nthe way for the practical deployment of low-bit LLMs on resource-constrained\nedge devices without compromising computational efficiency. The system is\nopen-sourced at https://github.com/microsoft/T-MAC.",
      "upvotes": 10
    },
    {
      "title": "Towards Robust Speech Representation Learning for Thousands of Languages",
      "url": "https://huggingface.co/papers/2407.00837",
      "authors": [
        "Karen Livescu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00837.pdf",
      "abstract": "Self-supervised learning (SSL) has helped extend speech technologies to more\nlanguages by reducing the need for labeled data. However, models are still far\nfrom supporting the world's 7000+ languages. We propose XEUS, a Cross-lingual\nEncoder for Universal Speech, trained on over 1 million hours of data across\n4057 languages, extending the language coverage of SSL models 4-fold. We\ncombine 1 million hours of speech from existing publicly accessible corpora\nwith a newly created corpus of 7400+ hours from 4057 languages, which will be\npublicly released. To handle the diverse conditions of multilingual speech\ndata, we augment the typical SSL masked prediction approach with a novel\ndereverberation objective, increasing robustness. We evaluate XEUS on several\nbenchmarks, and show that it consistently outperforms or achieves comparable\nresults to state-of-the-art (SOTA) SSL models across a variety of tasks. XEUS\nsets a new SOTA on the ML-SUPERB benchmark: it outperforms MMS 1B and w2v-BERT\n2.0 v2 by 0.8% and 4.4% respectively, despite having less parameters or\npre-training data. Checkpoints, code, and data are found in\nhttps://www.wavlab.org/activities/2024/xeus/.",
      "upvotes": 10
    },
    {
      "title": "Auto Cherry-Picker: Learning from High-quality Generative Data Driven by Language",
      "url": "https://huggingface.co/papers/2406.20085",
      "authors": [
        "Xiangyu Zhao",
        "Kai Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.20085.pdf",
      "abstract": "Diffusion-based models have shown great potential in generating high-quality\nimages with various layouts, which can benefit downstream perception tasks.\nHowever, a fully automatic layout generation driven only by language and a\nsuitable metric for measuring multiple generated instances has not been well\nexplored. In this work, we present Auto Cherry-Picker (ACP), a novel framework\nthat generates high-quality multi-modal training examples to augment perception\nand multi-modal training. Starting with a simple list of natural language\nconcepts, we prompt large language models (LLMs) to generate a detailed\ndescription and design reasonable layouts. Next, we use an off-the-shelf\ntext-to-image model to generate multiple images. Then, the generated data are\nrefined using a comprehensively designed metric to ensure quality. In\nparticular, we present a new metric, Composite Layout and Image Score (CLIS),\nto evaluate the generated images fairly. Our synthetic high-quality examples\nboost performance in various scenarios by customizing the initial concept list,\nespecially in addressing challenges associated with long-tailed distribution\nand imbalanced datasets. Experiment results on downstream tasks demonstrate\nthat Auto Cherry-Picker can significantly improve the performance of existing\nmodels. In addition, we have thoroughly investigated the correlation between\nCLIS and performance gains in downstream tasks, and we find that a better CLIS\nscore results in better performance. This finding shows the potential for\nevaluation metrics as the role for various visual perception and MLLM tasks.\nCode will be available.",
      "upvotes": 9
    },
    {
      "title": "SVG: 3D Stereoscopic Video Generation via Denoising Frame Matrix",
      "url": "https://huggingface.co/papers/2407.00367",
      "authors": [
        "Peng Dai",
        "Feitong Tan",
        "David Futschik",
        "Ruofei Du",
        "Sean Fanello",
        "Xiaojuan Qi",
        "Yinda Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00367.pdf",
      "abstract": "Video generation models have demonstrated great capabilities of producing\nimpressive monocular videos, however, the generation of 3D stereoscopic video\nremains under-explored. We propose a pose-free and training-free approach for\ngenerating 3D stereoscopic videos using an off-the-shelf monocular video\ngeneration model. Our method warps a generated monocular video into camera\nviews on stereoscopic baseline using estimated video depth, and employs a novel\nframe matrix video inpainting framework. The framework leverages the video\ngeneration model to inpaint frames observed from different timestamps and\nviews. This effective approach generates consistent and semantically coherent\nstereoscopic videos without scene optimization or model fine-tuning. Moreover,\nwe develop a disocclusion boundary re-injection scheme that further improves\nthe quality of video inpainting by alleviating the negative effects propagated\nfrom disoccluded areas in the latent space. We validate the efficacy of our\nproposed method by conducting experiments on videos from various generative\nmodels, including Sora [4 ], Lumiere [2], WALT [8 ], and Zeroscope [ 42]. The\nexperiments demonstrate that our method has a significant improvement over\nprevious methods. The code will be released at\nhttps://daipengwa.github.io/SVG_ProjectPage.",
      "upvotes": 9
    },
    {
      "title": "Show Less, Instruct More: Enriching Prompts with Definitions and Guidelines for Zero-Shot NER",
      "url": "https://huggingface.co/papers/2407.01272",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2407.01272.pdf",
      "abstract": "Recently, several specialized instruction-tuned Large Language Models (LLMs)\nfor Named Entity Recognition (NER) have emerged. Compared to traditional NER\napproaches, these models have strong generalization capabilities. Existing LLMs\nmainly focus on zero-shot NER in out-of-domain distributions, being fine-tuned\non an extensive number of entity classes that often highly or completely\noverlap with test sets. In this work instead, we propose SLIMER, an approach\ndesigned to tackle never-seen-before named entity tags by instructing the model\non fewer examples, and by leveraging a prompt enriched with definition and\nguidelines. Experiments demonstrate that definition and guidelines yield better\nperformance, faster and more robust learning, particularly when labelling\nunseen Named Entities. Furthermore, SLIMER performs comparably to\nstate-of-the-art approaches in out-of-domain zero-shot NER, while being trained\non a reduced tag set.",
      "upvotes": 8
    },
    {
      "title": "Accurate Prediction of Ligand-Protein Interaction Affinities with Fine-Tuned Small Language Models",
      "url": "https://huggingface.co/papers/2407.00111",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2407.00111.pdf",
      "abstract": "We describe the accurate prediction of ligand-protein interaction (LPI)\naffinities, also known as drug-target interactions (DTI), with instruction\nfine-tuned pretrained generative small language models (SLMs). We achieved\naccurate predictions for a range of affinity values associated with\nligand-protein interactions on out-of-sample data in a zero-shot setting. Only\nthe SMILES string of the ligand and the amino acid sequence of the protein were\nused as the model inputs. Our results demonstrate a clear improvement over\nmachine learning (ML) and free-energy perturbation (FEP+) based methods in\naccurately predicting a range of ligand-protein interaction affinities, which\ncan be leveraged to further accelerate drug discovery campaigns against\nchallenging therapeutic targets.",
      "upvotes": 5
    },
    {
      "title": "UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI",
      "url": "https://huggingface.co/papers/2407.00106",
      "authors": [
        "Ilia Shumailov",
        "Jamie Hayes",
        "Eleni Triantafillou",
        "Guillermo Ortiz-Jimenez",
        "Nicolas Papernot",
        "Heidi Howard"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00106.pdf",
      "abstract": "Exact unlearning was first introduced as a privacy mechanism that allowed a\nuser to retract their data from machine learning models on request. Shortly\nafter, inexact schemes were proposed to mitigate the impractical costs\nassociated with exact unlearning. More recently unlearning is often discussed\nas an approach for removal of impermissible knowledge i.e. knowledge that the\nmodel should not possess such as unlicensed copyrighted, inaccurate, or\nmalicious information. The promise is that if the model does not have a certain\nmalicious capability, then it cannot be used for the associated malicious\npurpose. In this paper we revisit the paradigm in which unlearning is used for\nin Large Language Models (LLMs) and highlight an underlying inconsistency\narising from in-context learning. Unlearning can be an effective control\nmechanism for the training phase, yet it does not prevent the model from\nperforming an impermissible act during inference. We introduce a concept of\nununlearning, where unlearned knowledge gets reintroduced in-context,\neffectively rendering the model capable of behaving as if it knows the\nforgotten knowledge. As a result, we argue that content filtering for\nimpermissible knowledge will be required and even exact unlearning schemes are\nnot enough for effective content regulation. We discuss feasibility of\nununlearning for modern LLMs and examine broader implications.",
      "upvotes": 5
    },
    {
      "title": "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging",
      "url": "https://huggingface.co/papers/2407.01470",
      "authors": [
        "Yun-Nung Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01470.pdf",
      "abstract": "Reinforcement learning from human feedback (RLHF) is a popular strategy for\naligning large language models (LLMs) with desired behaviors. Reward modeling\nis a crucial step in RLHF. However, collecting paired preference data for\ntraining reward models is often costly and time-consuming, especially for\ndomain-specific preferences requiring expert annotation. To address this\nchallenge, we propose the Domain knowledge merged\nReward Model (DogeRM), a novel framework that integrates\ndomain-specific knowledge into a general reward model by model merging. The\nexperiments demonstrate that DogeRM enhances performance across different\nbenchmarks and provide a detailed analysis showcasing the effects of model\nmerging, showing the great potential of facilitating model alignment.",
      "upvotes": 5
    },
    {
      "title": "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs",
      "url": "https://huggingface.co/papers/2406.20086",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.20086.pdf",
      "abstract": "LLMs process text as sequences of tokens that roughly correspond to words,\nwhere less common words are represented by multiple tokens. However, individual\ntokens are often semantically unrelated to the meanings of the words/concepts\nthey comprise. For example, Llama-2-7b's tokenizer splits the word\n\"northeastern\" into the tokens ['_n', 'ort', 'he', 'astern'], none of which\ncorrespond to semantically meaningful units like \"north\" or \"east.\" Similarly,\nthe overall meanings of named entities like \"Neil Young\" and multi-word\nexpressions like \"break a leg\" cannot be directly inferred from their\nconstituent tokens. Mechanistically, how do LLMs convert such arbitrary groups\nof tokens into useful higher-level representations? In this work, we find that\nlast token representations of named entities and multi-token words exhibit a\npronounced \"erasure\" effect, where information about previous and current\ntokens is rapidly forgotten in early layers. Using this observation, we propose\na method to \"read out\" the implicit vocabulary of an autoregressive LLM by\nexamining differences in token representations across layers, and present\nresults of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is\nthe first attempt to probe the implicit vocabulary of an LLM.",
      "upvotes": 4
    },
    {
      "title": "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models",
      "url": "https://huggingface.co/papers/2406.19999",
      "authors": [
        "Xinyi Chen",
        "Christof Monz",
        "Maarten de Rijke"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.19999.pdf",
      "abstract": "Following multiple instructions is a crucial ability for large language\nmodels (LLMs). Evaluating this ability comes with significant challenges: (i)\nlimited coherence between multiple instructions, (ii) positional bias where the\norder of instructions affects model performance, and (iii) a lack of\nobjectively verifiable tasks. To address these issues, we introduce a benchmark\ndesigned to evaluate models' abilities to follow multiple instructions through\nsequential instruction following (SIFo) tasks. In SIFo, the successful\ncompletion of multiple instructions is verifiable by examining only the final\ninstruction. Our benchmark evaluates instruction following using four tasks\n(text modification, question answering, mathematics, and security rule\nfollowing), each assessing different aspects of sequential instruction\nfollowing. Our evaluation of popular LLMs, both closed-source and open-source,\nshows that more recent and larger models significantly outperform their older\nand smaller counterparts on the SIFo tasks, validating the benchmark's\neffectiveness. All models struggle with following sequences of instructions,\nhinting at an important lack of robustness of today's language models.",
      "upvotes": 3
    },
    {
      "title": "ProgressGym: Alignment with a Millennium of Moral Progress",
      "url": "https://huggingface.co/papers/2406.20087",
      "authors": [
        "Yang Zhang",
        "Xuchuan Huang",
        "Jasmine Xinze Li",
        "Yaodong Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.20087.pdf",
      "abstract": "Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.",
      "upvotes": 3
    }
  ]
}