{
  "date": "2024-01-27",
  "papers": [
    {
      "title": "Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent Diffusion Models for Virtual Try-All",
      "url": "https://huggingface.co/papers/2401.13795",
      "authors": [
        "Karim Bouyarmane",
        "Ismail B. Tutar"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.13795.pdf",
      "abstract": "As online shopping is growing, the ability for buyers to virtually visualize\nproducts in their settings-a phenomenon we define as \"Virtual Try-All\"-has\nbecome crucial. Recent diffusion models inherently contain a world model,\nrendering them suitable for this task within an inpainting context. However,\ntraditional image-conditioned diffusion models often fail to capture the\nfine-grained details of products. In contrast, personalization-driven models\nsuch as DreamPaint are good at preserving the item's details but they are not\noptimized for real-time applications. We present \"Diffuse to Choose,\" a novel\ndiffusion-based image-conditioned inpainting model that efficiently balances\nfast inference with the retention of high-fidelity details in a given reference\nitem while ensuring accurate semantic manipulations in the given scene content.\nOur approach is based on incorporating fine-grained features from the reference\nimage directly into the latent feature maps of the main diffusion model,\nalongside with a perceptual loss to further preserve the reference item's\ndetails. We conduct extensive testing on both in-house and publicly available\ndatasets, and show that Diffuse to Choose is superior to existing zero-shot\ndiffusion inpainting methods as well as few-shot diffusion personalization\nalgorithms like DreamPaint.",
      "upvotes": 66
    },
    {
      "title": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence",
      "url": "https://huggingface.co/papers/2401.14196",
      "authors": [
        "Kai Dong",
        "Wentao Zhang",
        "Guanting Chen",
        "Xiao Bi",
        "Y. K. Li",
        "Yingfei Xiong",
        "Wenfeng Liang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14196.pdf",
      "abstract": "The rapid development of large language models has revolutionized code\nintelligence in software development. However, the predominance of\nclosed-source models has restricted extensive research and development. To\naddress this, we introduce the DeepSeek-Coder series, a range of open-source\ncode models with sizes from 1.3B to 33B, trained from scratch on 2 trillion\ntokens. These models are pre-trained on a high-quality project-level code\ncorpus and employ a fill-in-the-blank task with a 16K window to enhance code\ngeneration and infilling. Our extensive evaluations demonstrate that\nDeepSeek-Coder not only achieves state-of-the-art performance among open-source\ncode models across multiple benchmarks but also surpasses existing\nclosed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models\nare under a permissive license that allows for both research and unrestricted\ncommercial use.",
      "upvotes": 47
    },
    {
      "title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models",
      "url": "https://huggingface.co/papers/2401.13919",
      "authors": [
        "Yong Dai",
        "Hongming Zhang",
        "Dong Yu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.13919.pdf",
      "abstract": "The advancement of large language models (LLMs) leads to a new era marked by\nthe development of autonomous applications in the real world, which drives\ninnovation in the creation of advanced web-based agents. Existing web agents\ntypically only handle one input modality and are evaluated only in simplified\nweb simulators or static web snapshots, greatly limiting their applicability in\nreal-world scenarios. To bridge this gap, we introduce WebVoyager, an\ninnovative Large Multimodal Model (LMM) powered web agent that can complete\nuser instructions end-to-end by interacting with real-world websites. Moreover,\nwe propose a new evaluation protocol for web agents to address the challenges\nof automatic evaluation of open-ended web agent tasks, leveraging the robust\nmultimodal comprehension capabilities of GPT-4V. We create a new benchmark by\ngathering real-world tasks from 15 widely used websites to evaluate our agents.\nWe show that WebVoyager achieves a 55.7% task success rate, significantly\nsurpassing the performance of both GPT-4 (All Tools) and the WebVoyager\n(text-only) setups, underscoring the exceptional capability of WebVoyager in\npractical applications. We found that our proposed automatic evaluation\nachieves 85.3% agreement with human judgment, paving the way for further\ndevelopment of web agents in a real-world setting.",
      "upvotes": 26
    },
    {
      "title": "Rethinking Patch Dependence for Masked Autoencoders",
      "url": "https://huggingface.co/papers/2401.14391",
      "authors": [
        "Renhao Wang",
        "Alexei A. Efros",
        "Ken Goldberg"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14391.pdf",
      "abstract": "In this work, we re-examine inter-patch dependencies in the decoding\nmechanism of masked autoencoders (MAE). We decompose this decoding mechanism\nfor masked patch reconstruction in MAE into self-attention and cross-attention.\nOur investigations suggest that self-attention between mask patches is not\nessential for learning good representations. To this end, we propose a novel\npretraining framework: Cross-Attention Masked Autoencoders (CrossMAE).\nCrossMAE's decoder leverages only cross-attention between masked and visible\ntokens, with no degradation in downstream performance. This design also enables\ndecoding only a small subset of mask tokens, boosting efficiency. Furthermore,\neach decoder block can now leverage different encoder features, resulting in\nimproved representation learning. CrossMAE matches MAE in performance with 2.5\nto 3.7times less decoding compute. It also surpasses MAE on ImageNet\nclassification and COCO instance segmentation under the same compute. Code and\nmodels: https://crossmae.github.io",
      "upvotes": 23
    },
    {
      "title": "Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI",
      "url": "https://huggingface.co/papers/2401.14019",
      "authors": [
        "Roni Friedman-Melamed",
        "Michal Shmueli-Scheuer",
        "Yoav Katz"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14019.pdf",
      "abstract": "In the dynamic landscape of generative NLP, traditional text processing\npipelines limit research flexibility and reproducibility, as they are tailored\nto specific dataset, task, and model combinations. The escalating complexity,\ninvolving system prompts, model-specific formats, instructions, and more, calls\nfor a shift to a structured, modular, and customizable solution. Addressing\nthis need, we present Unitxt, an innovative library for customizable textual\ndata preparation and evaluation tailored to generative language models. Unitxt\nnatively integrates with common libraries like HuggingFace and LM-eval-harness\nand deconstructs processing flows into modular components, enabling easy\ncustomization and sharing between practitioners. These components encompass\nmodel-specific formats, task prompts, and many other comprehensive dataset\nprocessing definitions. The Unitxt-Catalog centralizes these components,\nfostering collaboration and exploration in modern textual data workflows.\nBeyond being a tool, Unitxt is a community-driven platform, empowering users to\nbuild, share, and advance their pipelines collaboratively. Join the Unitxt\ncommunity at https://github.com/IBM/unitxt!",
      "upvotes": 20
    },
    {
      "title": "FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design",
      "url": "https://huggingface.co/papers/2401.14112",
      "authors": [
        "Xiaoxia Wu",
        "Stephen Youn",
        "Yuxiong He"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14112.pdf",
      "abstract": "Six-bit quantization (FP6) can effectively reduce the size of large language\nmodels (LLMs) and preserve the model quality consistently across varied\napplications. However, existing systems do not provide Tensor Core support for\nFP6 quantization and struggle to achieve practical performance improvements\nduring LLM inference. It is challenging to support FP6 quantization on GPUs due\nto (1) unfriendly memory access of model weights with irregular bit-width and\n(2) high runtime overhead of weight de-quantization. To address these problems,\nwe propose TC-FPx, the first full-stack GPU kernel design scheme with unified\nTensor Core support of float-point weights for various quantization bit-width.\nWe integrate TC-FPx kernel into an existing inference system, providing new\nend-to-end support (called FP6-LLM) for quantized LLM inference, where better\ntrade-offs between inference cost and model quality are achieved. Experiments\nshow that FP6-LLM enables the inference of LLaMA-70b using only a single GPU,\nachieving 1.69x-2.65x higher normalized inference throughput than the FP16\nbaseline. The source code will be publicly available soon.",
      "upvotes": 18
    },
    {
      "title": "Deconstructing Denoising Diffusion Models for Self-Supervised Learning",
      "url": "https://huggingface.co/papers/2401.14404",
      "authors": [
        "Kaiming He"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14404.pdf",
      "abstract": "In this study, we examine the representation learning abilities of Denoising\nDiffusion Models (DDM) that were originally purposed for image generation. Our\nphilosophy is to deconstruct a DDM, gradually transforming it into a classical\nDenoising Autoencoder (DAE). This deconstructive procedure allows us to explore\nhow various components of modern DDMs influence self-supervised representation\nlearning. We observe that only a very few modern components are critical for\nlearning good representations, while many others are nonessential. Our study\nultimately arrives at an approach that is highly simplified and to a large\nextent resembles a classical DAE. We hope our study will rekindle interest in a\nfamily of classical methods within the realm of modern self-supervised\nlearning.",
      "upvotes": 17
    },
    {
      "title": "BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models",
      "url": "https://huggingface.co/papers/2401.13974",
      "authors": [
        "Shafiq Joty"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.13974.pdf",
      "abstract": "Recent text-to-image generation models have demonstrated incredible success\nin generating images that faithfully follow input prompts. However, the\nrequirement of using words to describe a desired concept provides limited\ncontrol over the appearance of the generated concepts. In this work, we address\nthis shortcoming by proposing an approach to enable personalization\ncapabilities in existing text-to-image diffusion models. We propose a novel\narchitecture (BootPIG) that allows a user to provide reference images of an\nobject in order to guide the appearance of a concept in the generated images.\n  The proposed BootPIG architecture makes minimal modifications to a pretrained\ntext-to-image diffusion model and utilizes a separate UNet model to steer the\ngenerations toward the desired appearance. We introduce a training procedure\nthat allows us to bootstrap personalization capabilities in the BootPIG\narchitecture using data generated from pretrained text-to-image models, LLM\nchat agents, and image segmentation models. In contrast to existing methods\nthat require several days of pretraining, the BootPIG architecture can be\ntrained in approximately 1 hour. Experiments on the DreamBooth dataset\ndemonstrate that BootPIG outperforms existing zero-shot methods while being\ncomparable with test-time finetuning approaches. Through a user study, we\nvalidate the preference for BootPIG generations over existing methods both in\nmaintaining fidelity to the reference object's appearance and aligning with\ntextual prompts.",
      "upvotes": 12
    },
    {
      "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities",
      "url": "https://huggingface.co/papers/2401.14405",
      "authors": [
        "Xiangyu Yue"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14405.pdf",
      "abstract": "We propose to improve transformers of a specific modality with irrelevant\ndata from other modalities, e.g., improve an ImageNet model with audio or point\ncloud datasets. We would like to highlight that the data samples of the target\nmodality are irrelevant to the other modalities, which distinguishes our method\nfrom other works utilizing paired (e.g., CLIP) or interleaved data of different\nmodalities. We propose a methodology named Multimodal Pathway - given a target\nmodality and a transformer designed for it, we use an auxiliary transformer\ntrained with data of another modality and construct pathways to connect\ncomponents of the two models so that data of the target modality can be\nprocessed by both models. In this way, we utilize the universal\nsequence-to-sequence modeling abilities of transformers obtained from two\nmodalities. As a concrete implementation, we use a modality-specific tokenizer\nand task-specific head as usual but utilize the transformer blocks of the\nauxiliary model via a proposed method named Cross-Modal Re-parameterization,\nwhich exploits the auxiliary weights without any inference costs. On the image,\npoint cloud, video, and audio recognition tasks, we observe significant and\nconsistent performance improvements with irrelevant data from other modalities.\nThe code and models are available at https://github.com/AILab-CVC/M2PT.",
      "upvotes": 11
    },
    {
      "title": "Adaptive Mobile Manipulation for Articulated Objects In the Open World",
      "url": "https://huggingface.co/papers/2401.14403",
      "authors": [
        "Russell Mendonca",
        "Kenneth Shaw",
        "Deepak Pathak"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14403.pdf",
      "abstract": "Deploying robots in open-ended unstructured environments such as homes has\nbeen a long-standing research problem. However, robots are often studied only\nin closed-off lab settings, and prior mobile manipulation work is restricted to\npick-move-place, which is arguably just the tip of the iceberg in this area. In\nthis paper, we introduce Open-World Mobile Manipulation System, a full-stack\napproach to tackle realistic articulated object operation, e.g. real-world\ndoors, cabinets, drawers, and refrigerators in open-ended unstructured\nenvironments. The robot utilizes an adaptive learning framework to initially\nlearns from a small set of data through behavior cloning, followed by learning\nfrom online practice on novel objects that fall outside the training\ndistribution. We also develop a low-cost mobile manipulation hardware platform\ncapable of safe and autonomous online adaptation in unstructured environments\nwith a cost of around 20,000 USD. In our experiments we utilize 20 articulate\nobjects across 4 buildings in the CMU campus. With less than an hour of online\nlearning for each object, the system is able to increase success rate from 50%\nof BC pre-training to 95% using online adaptation. Video results at\nhttps://open-world-mobilemanip.github.io/",
      "upvotes": 10
    },
    {
      "title": "Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation",
      "url": "https://huggingface.co/papers/2401.14257",
      "authors": [
        "Longguang Wang",
        "Yukun Wang",
        "Zhe Sheng",
        "Yisheng He",
        "Yulan Guo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14257.pdf",
      "abstract": "Recently, text-to-3D approaches have achieved high-fidelity 3D content\ngeneration using text description. However, the generated objects are\nstochastic and lack fine-grained control. Sketches provide a cheap approach to\nintroduce such fine-grained control. Nevertheless, it is challenging to achieve\nflexible control from these sketches due to their abstraction and ambiguity. In\nthis paper, we present a multi-view sketch-guided text-to-3D generation\nframework (namely, Sketch2NeRF) to add sketch control to 3D generation.\nSpecifically, our method leverages pretrained 2D diffusion models (e.g., Stable\nDiffusion and ControlNet) to supervise the optimization of a 3D scene\nrepresented by a neural radiance field (NeRF). We propose a novel synchronized\ngeneration and reconstruction method to effectively optimize the NeRF. In the\nexperiments, we collected two kinds of multi-view sketch datasets to evaluate\nthe proposed method. We demonstrate that our method can synthesize 3D\nconsistent contents with fine-grained sketch control while being high-fidelity\nto text prompts. Extensive results show that our method achieves\nstate-of-the-art performance in terms of sketch similarity and text alignment.",
      "upvotes": 10
    },
    {
      "title": "pix2gestalt: Amodal Segmentation by Synthesizing Wholes",
      "url": "https://huggingface.co/papers/2401.14398",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2401.14398.pdf",
      "abstract": "We introduce pix2gestalt, a framework for zero-shot amodal segmentation,\nwhich learns to estimate the shape and appearance of whole objects that are\nonly partially visible behind occlusions. By capitalizing on large-scale\ndiffusion models and transferring their representations to this task, we learn\na conditional diffusion model for reconstructing whole objects in challenging\nzero-shot cases, including examples that break natural and physical priors,\nsuch as art. As training data, we use a synthetically curated dataset\ncontaining occluded objects paired with their whole counterparts. Experiments\nshow that our approach outperforms supervised baselines on established\nbenchmarks. Our model can furthermore be used to significantly improve the\nperformance of existing object recognition and 3D reconstruction methods in the\npresence of occlusions.",
      "upvotes": 9
    },
    {
      "title": "CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion",
      "url": "https://huggingface.co/papers/2401.14066",
      "authors": [
        "Nisha Huang",
        "Weiming Dong",
        "Yuxin Zhang",
        "Fan Tang",
        "Ronghui Li",
        "Chongyang Ma",
        "Xiu Li",
        "Changsheng Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14066.pdf",
      "abstract": "Large-scale text-to-image generative models have made impressive strides,\nshowcasing their ability to synthesize a vast array of high-quality images.\nHowever, adapting these models for artistic image editing presents two\nsignificant challenges. Firstly, users struggle to craft textual prompts that\nmeticulously detail visual elements of the input image. Secondly, prevalent\nmodels, when effecting modifications in specific zones, frequently disrupt the\noverall artistic style, complicating the attainment of cohesive and\naesthetically unified artworks. To surmount these obstacles, we build the\ninnovative unified framework CreativeSynth, which is based on a diffusion model\nwith the ability to coordinate multimodal inputs and multitask in the field of\nartistic image generation. By integrating multimodal features with customized\nattention mechanisms, CreativeSynth facilitates the importation of real-world\nsemantic content into the domain of art through inversion and real-time style\ntransfer. This allows for the precise manipulation of image style and content\nwhile maintaining the integrity of the original model parameters. Rigorous\nqualitative and quantitative evaluations underscore that CreativeSynth excels\nin enhancing artistic images' fidelity and preserves their innate aesthetic\nessence. By bridging the gap between generative models and artistic finesse,\nCreativeSynth becomes a custom digital palette.",
      "upvotes": 8
    },
    {
      "title": "Genie: Achieving Human Parity in Content-Grounded Datasets Generation",
      "url": "https://huggingface.co/papers/2401.14367",
      "authors": [
        "Boaz Carmeli",
        "Eyal Shnarch"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14367.pdf",
      "abstract": "The lack of high-quality data for content-grounded generation tasks has been\nidentified as a major obstacle to advancing these tasks. To address this gap,\nwe propose Genie, a novel method for automatically generating high-quality\ncontent-grounded data. It consists of three stages: (a) Content Preparation,\n(b) Generation: creating task-specific examples from the content (e.g.,\nquestion-answer pairs or summaries). (c) Filtering mechanism aiming to ensure\nthe quality and faithfulness of the generated data. We showcase this\nmethodology by generating three large-scale synthetic data, making wishes, for\nLong-Form Question-Answering (LFQA), summarization, and information extraction.\nIn a human evaluation, our generated data was found to be natural and of high\nquality. Furthermore, we compare models trained on our data with models trained\non human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for\nSummarization. We show that our models are on par with or outperforming models\ntrained on human-generated data and consistently outperforming them in\nfaithfulness. Finally, we applied our method to create LFQA data within the\nmedical domain and compared a model trained on it with models trained on other\ndomains.",
      "upvotes": 7
    }
  ]
}