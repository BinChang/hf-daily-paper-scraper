{
  "date": "2024-08-15",
  "papers": [
    {
      "title": "Seeing and Understanding: Bridging Vision with Chemical Knowledge Via ChemVLM",
      "url": "https://huggingface.co/papers/2408.07246",
      "authors": [
        "Xunzhi Wang",
        "Zeying Hao",
        "Jingdi Lei",
        "Qian Tan",
        "Cai Zhou",
        "Wei Liu",
        "Weiyun Wang",
        "Zhe Chen",
        "Wenhai Wang",
        "Wei Li",
        "Shufei Zhang",
        "Mao Su",
        "Wanli Ouyang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07246.pdf",
      "abstract": "In this technical report, we propose ChemVLM, the first open-source\nmultimodal large language model dedicated to the fields of chemistry, designed\nto address the incompatibility between chemical image understanding and text\nanalysis. Built upon the VIT-MLP-LLM architecture, we leverage ChemLLM-20B as\nthe foundational large model, endowing our model with robust capabilities in\nunderstanding and utilizing chemical text knowledge. Additionally, we employ\nInternVIT-6B as a powerful image encoder. We have curated high-quality data\nfrom the chemical domain, including molecules, reaction formulas, and chemistry\nexamination data, and compiled these into a bilingual multimodal\nquestion-answering dataset. We test the performance of our model on multiple\nopen-source benchmarks and three custom evaluation sets. Experimental results\ndemonstrate that our model achieves excellent performance, securing\nstate-of-the-art results in five out of six involved tasks. Our model can be\nfound at https://huggingface.co/AI4Chem/ChemVLM-26B.",
      "upvotes": 19
    },
    {
      "title": "Generative Photomontage",
      "url": "https://huggingface.co/papers/2408.07116",
      "authors": [
        "Sean J. Liu",
        "Nupur Kumari",
        "Ariel Shamir",
        "Jun-Yan Zhu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07116.pdf",
      "abstract": "Text-to-image models are powerful tools for image creation. However, the\ngeneration process is akin to a dice roll and makes it difficult to achieve a\nsingle image that captures everything a user wants. In this paper, we propose a\nframework for creating the desired image by compositing it from various parts\nof generated images, in essence forming a Generative Photomontage. Given a\nstack of images generated by ControlNet using the same input condition and\ndifferent seeds, we let users select desired parts from the generated results\nusing a brush stroke interface. We introduce a novel technique that takes in\nthe user's brush strokes, segments the generated images using a graph-based\noptimization in diffusion feature space, and then composites the segmented\nregions via a new feature-space blending method. Our method faithfully\npreserves the user-selected regions while compositing them harmoniously. We\ndemonstrate that our flexible framework can be used for many applications,\nincluding generating new appearance combinations, fixing incorrect shapes and\nartifacts, and improving prompt alignment. We show compelling results for each\napplication and demonstrate that our method outperforms existing image blending\nmethods and various baselines.",
      "upvotes": 19
    },
    {
      "title": "Aquila2 Technical Report",
      "url": "https://huggingface.co/papers/2408.07410",
      "authors": [
        "Jijie Li",
        "Xinya Wu",
        "Zhengduo Zhang",
        "Boyan Gao",
        "Yulong Ao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07410.pdf",
      "abstract": "This paper introduces the Aquila2 series, which comprises a wide range of\nbilingual models with parameter sizes of 7, 34, and 70 billion. These models\nare trained based on an innovative framework named HeuriMentor (HM), which\noffers real-time insights into model convergence and enhances the training\nprocess and data management. The HM System, comprising the Adaptive Training\nEngine (ATE), Training State Monitor (TSM), and Data Management Unit (DMU),\nallows for precise monitoring of the model's training progress and enables\nefficient optimization of data distribution, thereby enhancing training\neffectiveness. Extensive evaluations show that the Aquila2 model series\nperforms comparably well on both English and Chinese benchmarks. Specifically,\nAquila2-34B demonstrates only a slight decrease in performance when quantized\nto Int4. Furthermore, we have made our training code\n(https://github.com/FlagOpen/FlagScale) and model weights\n(https://github.com/FlagAI-Open/Aquila2) publicly available to support ongoing\nresearch and the development of applications.",
      "upvotes": 13
    },
    {
      "title": "InfinityMATH: A Scalable Instruction Tuning Dataset in Programmatic Mathematical Reasoning",
      "url": "https://huggingface.co/papers/2408.07089",
      "authors": [
        "Yan Yan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07089.pdf",
      "abstract": "Recent advancements in Chain-of-Thoughts (CoT) and Program-of-Thoughts (PoT)\nmethods have greatly enhanced language models' mathematical reasoning\ncapabilities, facilitating their integration into instruction tuning datasets\nwith LLMs. However, existing methods for large-scale dataset creation require\nsubstantial seed data and high computational costs for data synthesis, posing\nsignificant challenges for scalability. We introduce InfinityMATH, a scalable\ninstruction tuning dataset for programmatic mathematical reasoning. The\nconstruction pipeline emphasizes decoupling numbers from mathematical problems\nto synthesize number-independent programs, enabling efficient and flexible\nscaling while minimizing dependency on specific numerical values. Fine-tuning\nexperiments with open-source language and code models, such as Llama2 and\nCodeLlama, demonstrate the practical benefits of InfinityMATH. These fine-tuned\nmodels, showed significant relative improvements on both in-domain and\nout-of-domain benchmarks, ranging from 184.7% to 514.3% on average.\nAdditionally, these models exhibited high robustness on the GSM8K+ and MATH+\nbenchmarks, which are enhanced version of test sets with simply the number\nvariations. InfinityMATH ensures that models are more versatile and effective\nacross a broader range of mathematical problems. The data is available at\nhttps://huggingface.co/datasets/flagopen/InfinityMATH.",
      "upvotes": 13
    },
    {
      "title": "DeepSpeak Dataset v1.0",
      "url": "https://huggingface.co/papers/2408.05366",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2408.05366.pdf",
      "abstract": "We describe a large-scale dataset--{\\em DeepSpeak}--of real and deepfake\nfootage of people talking and gesturing in front of their webcams. The real\nvideos in this first version of the dataset consist of 9 hours of footage\nfrom 220 diverse individuals. Constituting more than 25 hours of footage, the\nfake videos consist of a range of different state-of-the-art face-swap and\nlip-sync deepfakes with natural and AI-generated voices. We expect to release\nfuture versions of this dataset with different and updated deepfake\ntechnologies. This dataset is made freely available for research and\nnon-commercial uses; requests for commercial use will be considered.",
      "upvotes": 10
    },
    {
      "title": "3D Gaussian Editing with A Single Image",
      "url": "https://huggingface.co/papers/2408.07540",
      "authors": [
        "Guan Luo",
        "Tian-Xing Xu",
        "Ying-Tian Liu",
        "Xiao-Xiong Fan",
        "Fang-Lue Zhang",
        "Song-Hai Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07540.pdf",
      "abstract": "The modeling and manipulation of 3D scenes captured from the real world are\npivotal in various applications, attracting growing research interest. While\nprevious works on editing have achieved interesting results through\nmanipulating 3D meshes, they often require accurately reconstructed meshes to\nperform editing, which limits their application in 3D content generation. To\naddress this gap, we introduce a novel single-image-driven 3D scene editing\napproach based on 3D Gaussian Splatting, enabling intuitive manipulation via\ndirectly editing the content on a 2D image plane. Our method learns to optimize\nthe 3D Gaussians to align with an edited version of the image rendered from a\nuser-specified viewpoint of the original scene. To capture long-range object\ndeformation, we introduce positional loss into the optimization process of 3D\nGaussian Splatting and enable gradient propagation through reparameterization.\nTo handle occluded 3D Gaussians when rendering from the specified viewpoint, we\nbuild an anchor-based structure and employ a coarse-to-fine optimization\nstrategy capable of handling long-range deformation while maintaining\nstructural stability. Furthermore, we design a novel masking strategy to\nadaptively identify non-rigid deformation regions for fine-scale modeling.\nExtensive experiments show the effectiveness of our method in handling\ngeometric details, long-range, and non-rigid deformation, demonstrating\nsuperior editing flexibility and quality compared to previous approaches.",
      "upvotes": 10
    },
    {
      "title": "PeriodWave: Multi-Period Flow Matching for High-Fidelity Waveform Generation",
      "url": "https://huggingface.co/papers/2408.07547",
      "authors": [
        "Seong-Whan Lee"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07547.pdf",
      "abstract": "Recently, universal waveform generation tasks have been investigated\nconditioned on various out-of-distribution scenarios. Although GAN-based\nmethods have shown their strength in fast waveform generation, they are\nvulnerable to train-inference mismatch scenarios such as two-stage\ntext-to-speech. Meanwhile, diffusion-based models have shown their powerful\ngenerative performance in other domains; however, they stay out of the\nlimelight due to slow inference speed in waveform generation tasks. Above all,\nthere is no generator architecture that can explicitly disentangle the natural\nperiodic features of high-resolution waveform signals. In this paper, we\npropose PeriodWave, a novel universal waveform generation model. First, we\nintroduce a period-aware flow matching estimator that can capture the periodic\nfeatures of the waveform signal when estimating the vector fields.\nAdditionally, we utilize a multi-period estimator that avoids overlaps to\ncapture different periodic features of waveform signals. Although increasing\nthe number of periods can improve the performance significantly, this requires\nmore computational costs. To reduce this issue, we also propose a single\nperiod-conditional universal estimator that can feed-forward parallel by\nperiod-wise batch inference. Additionally, we utilize discrete wavelet\ntransform to losslessly disentangle the frequency information of waveform\nsignals for high-frequency modeling, and introduce FreeU to reduce the\nhigh-frequency noise for waveform generation. The experimental results\ndemonstrated that our model outperforms the previous models both in\nMel-spectrogram reconstruction and text-to-speech tasks. All source code will\nbe available at https://github.com/sh-lee-prml/PeriodWave.",
      "upvotes": 7
    },
    {
      "title": "Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space",
      "url": "https://huggingface.co/papers/2408.07416",
      "authors": [
        "Hyunjee Lee",
        "Youngsik Yun",
        "Jeongmin Bae",
        "Seoha Kim",
        "Youngjung Uh"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07416.pdf",
      "abstract": "Understanding the 3D semantics of a scene is a fundamental problem for\nvarious scenarios such as embodied agents. While NeRFs and 3DGS excel at\nnovel-view synthesis, previous methods for understanding their semantics have\nbeen limited to incomplete 3D understanding: their segmentation results are 2D\nmasks and their supervision is anchored at 2D pixels. This paper revisits the\nproblem set to pursue a better 3D understanding of a scene modeled by NeRFs and\n3DGS as follows. 1) We directly supervise the 3D points to train the language\nembedding field. It achieves state-of-the-art accuracy without relying on\nmulti-scale language embeddings. 2) We transfer the pre-trained language field\nto 3DGS, achieving the first real-time rendering speed without sacrificing\ntraining time or accuracy. 3) We introduce a 3D querying and evaluation\nprotocol for assessing the reconstructed geometry and semantics together. Code,\ncheckpoints, and annotations will be available online. Project page:\nhttps://hyunji12.github.io/Open3DRF",
      "upvotes": 5
    }
  ]
}