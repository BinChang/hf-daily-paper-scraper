{
  "date": "2024-09-11",
  "papers": [
    {
      "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models",
      "url": "https://huggingface.co/papers/2409.06666",
      "authors": [
        "Yang Feng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06666.pdf",
      "abstract": "Models like GPT-4o enable real-time interaction with large language models\n(LLMs) through speech, significantly enhancing user experience compared to\ntraditional text-based interaction. However, there is still a lack of\nexploration on how to build speech interaction models based on open-source\nLLMs. To address this, we propose LLaMA-Omni, a novel model architecture\ndesigned for low-latency and high-quality speech interaction with LLMs.\nLLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,\nand a streaming speech decoder. It eliminates the need for speech\ntranscription, and can simultaneously generate text and speech responses\ndirectly from speech instructions with extremely low latency. We build our\nmodel based on the latest Llama-3.1-8B-Instruct model. To align the model with\nspeech interaction scenarios, we construct a dataset named InstructS2S-200K,\nwhich includes 200K speech instructions and corresponding speech responses.\nExperimental results show that compared to previous speech-language models,\nLLaMA-Omni provides better responses in both content and style, with a response\nlatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3\ndays on just 4 GPUs, paving the way for the efficient development of\nspeech-language models in the future.",
      "upvotes": 55
    },
    {
      "title": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering",
      "url": "https://huggingface.co/papers/2409.06595",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2409.06595.pdf",
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations.",
      "upvotes": 37
    },
    {
      "title": "INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding",
      "url": "https://huggingface.co/papers/2409.06210",
      "authors": [
        "Se Young Chun"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06210.pdf",
      "abstract": "Affordance denotes the potential interactions inherent in objects. The\nperception of affordance can enable intelligent agents to navigate and interact\nwith new environments efficiently. Weakly supervised affordance grounding\nteaches agents the concept of affordance without costly pixel-level\nannotations, but with exocentric images. Although recent advances in weakly\nsupervised affordance grounding yielded promising results, there remain\nchallenges including the requirement for paired exocentric and egocentric image\ndataset, and the complexity in grounding diverse affordances for a single\nobject. To address them, we propose INTeraction Relationship-aware weakly\nsupervised Affordance grounding (INTRA). Unlike prior arts, INTRA recasts this\nproblem as representation learning to identify unique features of interactions\nthrough contrastive learning with exocentric images only, eliminating the need\nfor paired datasets. Moreover, we leverage vision-language model embeddings for\nperforming affordance grounding flexibly with any text, designing\ntext-conditioned affordance map generation to reflect interaction relationship\nfor contrastive learning and enhancing robustness with our text synonym\naugmentation. Our method outperformed prior arts on diverse datasets such as\nAGD20K, IIT-AFF, CAD and UMD. Additionally, experimental results demonstrate\nthat our method has remarkable domain scalability for synthesized images /\nillustrations and is capable of performing affordance grounding for novel\ninteractions and objects.",
      "upvotes": 24
    },
    {
      "title": "SongCreator: Lyrics-based Universal Song Generation",
      "url": "https://huggingface.co/papers/2409.06029",
      "authors": [
        "Shun Lei",
        "Yixuan Zhou",
        "Boshi Tang",
        "Feng Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06029.pdf",
      "abstract": "Music is an integral part of human culture, embodying human intelligence and\ncreativity, of which songs compose an essential part. While various aspects of\nsong generation have been explored by previous works, such as singing voice,\nvocal composition and instrumental arrangement, etc., generating songs with\nboth vocals and accompaniment given lyrics remains a significant challenge,\nhindering the application of music generation models in the real world. In this\nlight, we propose SongCreator, a song-generation system designed to tackle this\nchallenge. The model features two novel designs: a meticulously designed\ndual-sequence language model (DSLM) to capture the information of vocals and\naccompaniment for song generation, and an additional attention mask strategy\nfor DSLM, which allows our model to understand, generate and edit songs, making\nit suitable for various song-related generation tasks. Extensive experiments\ndemonstrate the effectiveness of SongCreator by achieving state-of-the-art or\ncompetitive performances on all eight tasks. Notably, it surpasses previous\nworks by a large margin in lyrics-to-song and lyrics-to-vocals. Additionally,\nit is able to independently control the acoustic conditions of the vocals and\naccompaniment in the generated song through different prompts, exhibiting its\npotential applicability. Our samples are available at\nhttps://songcreator.github.io/.",
      "upvotes": 20
    },
    {
      "title": "Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis",
      "url": "https://huggingface.co/papers/2409.06135",
      "authors": [
        "Binjie Mao",
        "Xing Nie",
        "Pengfei Gao",
        "Ying Guo",
        "Cheng Zhen",
        "Pengfei Yan",
        "Shiming Xiang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06135.pdf",
      "abstract": "Foley is a term commonly used in filmmaking, referring to the addition of\ndaily sound effects to silent films or videos to enhance the auditory\nexperience. Video-to-Audio (V2A), as a particular type of automatic foley task,\npresents inherent challenges related to audio-visual synchronization. These\nchallenges encompass maintaining the content consistency between the input\nvideo and the generated audio, as well as the alignment of temporal and\nloudness properties within the video. To address these issues, we construct a\ncontrollable video-to-audio synthesis model, termed Draw an Audio, which\nsupports multiple input instructions through drawn masks and loudness signals.\nTo ensure content consistency between the synthesized audio and target video,\nwe introduce the Mask-Attention Module (MAM), which employs masked video\ninstruction to enable the model to focus on regions of interest. Additionally,\nwe implement the Time-Loudness Module (TLM), which uses an auxiliary loudness\nsignal to ensure the synthesis of sound that aligns with the video in both\nloudness and temporal dimensions. Furthermore, we have extended a large-scale\nV2A dataset, named VGGSound-Caption, by annotating caption prompts. Extensive\nexperiments on challenging benchmarks across two large-scale V2A datasets\nverify Draw an Audio achieves the state-of-the-art. Project page:\nhttps://yannqi.github.io/Draw-an-Audio/.",
      "upvotes": 14
    },
    {
      "title": "SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation",
      "url": "https://huggingface.co/papers/2409.06633",
      "authors": [
        "Teng Hu",
        "Jiangning Zhang",
        "Ran Yi",
        "Yabiao Wang",
        "Lizhuang Ma"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06633.pdf",
      "abstract": "In recent years, the development of diffusion models has led to significant\nprogress in image and video generation tasks, with pre-trained models like the\nStable Diffusion series playing a crucial role. Inspired by model pruning which\nlightens large pre-trained models by removing unimportant parameters, we\npropose a novel model fine-tuning method to make full use of these ineffective\nparameters and enable the pre-trained model with new task-specified\ncapabilities. In this work, we first investigate the importance of parameters\nin pre-trained diffusion models, and discover that the smallest 10% to 20% of\nparameters by absolute values do not contribute to the generation process.\nBased on this observation, we propose a method termed SaRA that re-utilizes\nthese temporarily ineffective parameters, equating to optimizing a sparse\nweight matrix to learn the task-specific knowledge. To mitigate overfitting, we\npropose a nuclear-norm-based low-rank sparse training scheme for efficient\nfine-tuning. Furthermore, we design a new progressive parameter adjustment\nstrategy to make full use of the re-trained/finetuned parameters. Finally, we\npropose a novel unstructural backpropagation strategy, which significantly\nreduces memory costs during fine-tuning. Our method enhances the generative\ncapabilities of pre-trained models in downstream applications and outperforms\ntraditional fine-tuning methods like LoRA in maintaining model's generalization\nability. We validate our approach through fine-tuning experiments on SD models,\ndemonstrating significant improvements. SaRA also offers a practical advantage\nthat requires only a single line of code modification for efficient\nimplementation and is seamlessly compatible with existing methods.",
      "upvotes": 14
    },
    {
      "title": "LEIA: Latent View-invariant Embeddings for Implicit 3D Articulation",
      "url": "https://huggingface.co/papers/2409.06703",
      "authors": [
        "Anubhav Gupta",
        "Kamal Gupta",
        "Shishira R. Maiya",
        "Vatsal Agarwal"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06703.pdf",
      "abstract": "Neural Radiance Fields (NeRFs) have revolutionized the reconstruction of\nstatic scenes and objects in 3D, offering unprecedented quality. However,\nextending NeRFs to model dynamic objects or object articulations remains a\nchallenging problem. Previous works have tackled this issue by focusing on\npart-level reconstruction and motion estimation for objects, but they often\nrely on heuristics regarding the number of moving parts or object categories,\nwhich can limit their practical use. In this work, we introduce LEIA, a novel\napproach for representing dynamic 3D objects. Our method involves observing the\nobject at distinct time steps or \"states\" and conditioning a hypernetwork on\nthe current state, using this to parameterize our NeRF. This approach allows us\nto learn a view-invariant latent representation for each state. We further\ndemonstrate that by interpolating between these states, we can generate novel\narticulation configurations in 3D space that were previously unseen. Our\nexperimental results highlight the effectiveness of our method in articulating\nobjects in a manner that is independent of the viewing angle and joint\nconfiguration. Notably, our approach outperforms previous methods that rely on\nmotion information for articulation registration.",
      "upvotes": 2
    }
  ]
}