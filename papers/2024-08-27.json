{
  "date": "2024-08-27",
  "papers": [
    {
      "title": "SwiftBrush v2: Make Your One-step Diffusion Model Better Than Its Teacher",
      "url": "https://huggingface.co/papers/2408.14176",
      "authors": [
        "Cuong Pham"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.14176.pdf",
      "abstract": "In this paper, we aim to enhance the performance of SwiftBrush, a prominent\none-step text-to-image diffusion model, to be competitive with its multi-step\nStable Diffusion counterpart. Initially, we explore the quality-diversity\ntrade-off between SwiftBrush and SD Turbo: the former excels in image\ndiversity, while the latter excels in image quality. This observation motivates\nour proposed modifications in the training methodology, including better weight\ninitialization and efficient LoRA training. Moreover, our introduction of a\nnovel clamped CLIP loss enhances image-text alignment and results in improved\nimage quality. Remarkably, by combining the weights of models trained with\nefficient LoRA and full training, we achieve a new state-of-the-art one-step\ndiffusion model, achieving an FID of 8.14 and surpassing all GAN-based and\nmulti-step Stable Diffusion models. The evaluation code is available at:\nhttps://github.com/vinairesearch/swiftbrushv2.",
      "upvotes": 59
    },
    {
      "title": "Foundation Models for Music: A Survey",
      "url": "https://huggingface.co/papers/2408.14340",
      "authors": [
        "Anton Ragni",
        "Bleiz MacSen Del Sette",
        "Charalampos Saitis",
        "Gy√∂rgy Fazekas",
        "Gus Xia",
        "Jiawen Huang",
        "Julien Guinot",
        "Luca Marinelli",
        "Max W. Y. Lam"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.14340.pdf",
      "abstract": "In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.",
      "upvotes": 40
    },
    {
      "title": "SWE-bench-java: A GitHub Issue Resolving Benchmark for Java",
      "url": "https://huggingface.co/papers/2408.14354",
      "authors": [
        "Wei Liu",
        "Dong Chen",
        "Hao Yu",
        "Muhan Zeng",
        "Bo Shen",
        "Pan Bian",
        "Guangtai Liang",
        "Bei Guan",
        "Pengjie Huang",
        "Tao Xie"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.14354.pdf",
      "abstract": "GitHub issue resolving is a critical task in software engineering, recently\ngaining significant attention in both industry and academia. Within this task,\nSWE-bench has been released to evaluate issue resolving capabilities of large\nlanguage models (LLMs), but has so far only focused on Python version. However,\nsupporting more programming languages is also important, as there is a strong\ndemand in industry. As a first step toward multilingual support, we have\ndeveloped a Java version of SWE-bench, called SWE-bench-java. We have publicly\nreleased the dataset, along with the corresponding Docker-based evaluation\nenvironment and leaderboard, which will be continuously maintained and updated\nin the coming months. To verify the reliability of SWE-bench-java, we implement\na classic method SWE-agent and test several powerful LLMs on it. As is well\nknown, developing a high-quality multi-lingual benchmark is time-consuming and\nlabor-intensive, so we welcome contributions through pull requests or\ncollaboration to accelerate its iteration and refinement, paving the way for\nfully automated programming.",
      "upvotes": 40
    },
    {
      "title": "K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences",
      "url": "https://huggingface.co/papers/2408.14468",
      "authors": [
        "Zhikai Li",
        "Dongrong Fu",
        "Jianquan Li",
        "Qingyi Gu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.14468.pdf",
      "abstract": "The rapid advancement of visual generative models necessitates efficient and\nreliable evaluation methods. Arena platform, which gathers user votes on model\ncomparisons, can rank models with human preferences. However, traditional Arena\nmethods, while established, require an excessive number of comparisons for\nranking to converge and are vulnerable to preference noise in voting,\nsuggesting the need for better approaches tailored to contemporary evaluation\nchallenges. In this paper, we introduce K-Sort Arena, an efficient and reliable\nplatform based on a key insight: images and videos possess higher perceptual\nintuitiveness than texts, enabling rapid evaluation of multiple samples\nsimultaneously. Consequently, K-Sort Arena employs K-wise comparisons, allowing\nK models to engage in free-for-all competitions, which yield much richer\ninformation than pairwise comparisons. To enhance the robustness of the system,\nwe leverage probabilistic modeling and Bayesian updating techniques. We propose\nan exploration-exploitation-based matchmaking strategy to facilitate more\ninformative comparisons. In our experiments, K-Sort Arena exhibits 16.3x faster\nconvergence compared to the widely used ELO algorithm. To further validate the\nsuperiority and obtain a comprehensive leaderboard, we collect human feedback\nvia crowdsourced evaluations of numerous cutting-edge text-to-image and\ntext-to-video models. Thanks to its high efficiency, K-Sort Arena can\ncontinuously incorporate emerging models and update the leaderboard with\nminimal votes. Our project has undergone several months of internal testing and\nis now available at https://huggingface.co/spaces/ksort/K-Sort-Arena",
      "upvotes": 33
    },
    {
      "title": "LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs",
      "url": "https://huggingface.co/papers/2408.13467",
      "authors": [
        "Fan Wang",
        "Jing Tang",
        "Sunghun Kim"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.13467.pdf",
      "abstract": "The widespread adoption of cloud-based proprietary large language models\n(LLMs) has introduced significant challenges, including operational\ndependencies, privacy concerns, and the necessity of continuous internet\nconnectivity. In this work, we introduce an LLMOps pipeline, \"LlamaDuo\", for\nthe seamless migration of knowledge and abilities from service-oriented LLMs to\nsmaller, locally manageable models. This pipeline is crucial for ensuring\nservice continuity in the presence of operational failures, strict privacy\npolicies, or offline requirements. Our LlamaDuo involves fine-tuning a small\nlanguage model against the service LLM using a synthetic dataset generated by\nthe latter. If the performance of the fine-tuned model falls short of\nexpectations, it is enhanced by further fine-tuning with additional similar\ndata created by the service LLM. This iterative process guarantees that the\nsmaller model can eventually match or even surpass the service LLM's\ncapabilities in specific downstream tasks, offering a practical and scalable\nsolution for managing AI deployments in constrained environments. Extensive\nexperiments with leading edge LLMs are conducted to demonstrate the\neffectiveness, adaptability, and affordability of LlamaDuo across various\ndownstream tasks. Our pipeline implementation is available at\nhttps://github.com/deep-diver/llamaduo.",
      "upvotes": 23
    },
    {
      "title": "Learning to Move Like Professional Counter-Strike Players",
      "url": "https://huggingface.co/papers/2408.13934",
      "authors": [
        "David Durst",
        "Feng Xie",
        "Iuri Frosio",
        "Chen Tessler",
        "Joohwan Kim",
        "Carly Taylor",
        "Gilbert Bernstein",
        "Sanjiban Choudhury",
        "Pat Hanrahan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.13934.pdf",
      "abstract": "In multiplayer, first-person shooter games like Counter-Strike: Global\nOffensive (CS:GO), coordinated movement is a critical component of high-level\nstrategic play. However, the complexity of team coordination and the variety of\nconditions present in popular game maps make it impractical to author\nhand-crafted movement policies for every scenario. We show that it is possible\nto take a data-driven approach to creating human-like movement controllers for\nCS:GO. We curate a team movement dataset comprising 123 hours of professional\ngame play traces, and use this dataset to train a transformer-based movement\nmodel that generates human-like team movement for all players in a \"Retakes\"\nround of the game. Importantly, the movement prediction model is efficient.\nPerforming inference for all players takes less than 0.5 ms per game step\n(amortized cost) on a single CPU core, making it plausible for use in\ncommercial games today. Human evaluators assess that our model behaves more\nlike humans than both commercially-available bots and procedural movement\ncontrollers scripted by experts (16% to 59% higher by TrueSkill rating of\n\"human-like\"). Using experiments involving in-game bot vs. bot self-play, we\ndemonstrate that our model performs simple forms of teamwork, makes fewer\ncommon movement mistakes, and yields movement distributions, player lifetimes,\nand kill locations similar to those observed in professional CS:GO match play.",
      "upvotes": 21
    },
    {
      "title": "Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler",
      "url": "https://huggingface.co/papers/2408.13359",
      "authors": [
        "Shawn Tan",
        "Aditya Prasad",
        "David D. Cox"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.13359.pdf",
      "abstract": "Finding the optimal learning rate for language model pretraining is a\nchallenging task. This is not only because there is a complicated correlation\nbetween learning rate, batch size, number of training tokens, model size, and\nother hyperparameters but also because it is prohibitively expensive to perform\na hyperparameter search for large language models with Billions or Trillions of\nparameters. Recent studies propose using small proxy models and small corpus to\nperform hyperparameter searches and transposing the optimal parameters to large\nmodels and large corpus. While the zero-shot transferability is theoretically\nand empirically proven for model size related hyperparameters, like depth and\nwidth, the zero-shot transfer from small corpus to large corpus is\nunderexplored. In this paper, we study the correlation between optimal learning\nrate, batch size, and number of training tokens for the recently proposed WSD\nscheduler. After thousands of small experiments, we found a power-law\nrelationship between variables and demonstrated its transferability across\nmodel sizes. Based on the observation, we propose a new learning rate\nscheduler, Power scheduler, that is agnostic about the number of training\ntokens and batch size. The experiment shows that combining the Power scheduler\nwith Maximum Update Parameterization (muP) can consistently achieve impressive\nperformance with one set of hyperparameters regardless of the number of\ntraining tokens, batch size, model size, and even model architecture. Our 3B\ndense and MoE models trained with the Power scheduler achieve comparable\nperformance as state-of-the-art small language models. We open-source these\npretrained models at https://ibm.biz/BdKhLa.",
      "upvotes": 21
    },
    {
      "title": "Training-free Long Video Generation with Chain of Diffusion Model Experts",
      "url": "https://huggingface.co/papers/2408.13423",
      "authors": [
        "Wenhao Li",
        "Xie Su",
        "Xi Lin",
        "Shan You",
        "Yi Chen",
        "Chang Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.13423.pdf",
      "abstract": "Video generation models hold substantial potential in areas such as\nfilmmaking. However, current video diffusion models need high computational\ncosts and produce suboptimal results due to high complexity of video generation\ntask. In this paper, we propose ConFiner, an efficient high-quality\nvideo generation framework that decouples video generation into easier\nsubtasks: structure control and spatial-temporal refinement.\nIt can generate high-quality videos with chain of off-the-shelf diffusion model\nexperts, each expert responsible for a decoupled subtask. During the\nrefinement, we introduce coordinated denoising, which can merge multiple\ndiffusion experts' capabilities into a single sampling. Furthermore, we design\nConFiner-Long framework, which can generate long coherent video with three\nconstraint strategies on ConFiner. Experimental results indicate that with only\n10\\% of the inference cost, our ConFiner surpasses representative models like\nLavie and Modelscope across all objective and subjective metrics. And\nConFiner-Long can generate high-quality and coherent videos with up to 600\nframes.",
      "upvotes": 20
    },
    {
      "title": "LLaVaOLMoBitnet1B: Ternary LLM goes Multimodal!",
      "url": "https://huggingface.co/papers/2408.13402",
      "authors": [
        "Ravishankar Iyer"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.13402.pdf",
      "abstract": "Multimodal Large Language Models (MM-LLMs) have seen significant advancements\nin the last year, demonstrating impressive performance across tasks. However,\nto truly democratize AI, models must exhibit strong capabilities and be able to\nrun efficiently on small compute footprints accessible by most. Part of this\nquest, we introduce LLaVaOLMoBitnet1B - the first Ternary Multimodal LLM\ncapable of accepting Image(s)+Text inputs to produce coherent textual\nresponses. The model is fully open-sourced along with training scripts to\nencourage further research in this space. This accompanying technical report\nhighlights the training process, evaluation details, challenges associated with\nternary models and future opportunities. Link to the model:\nhttps://huggingface.co/IntelLabs/LlavaOLMoBitnet1B",
      "upvotes": 17
    },
    {
      "title": "NanoFlow: Towards Optimal Large Language Model Serving Throughput",
      "url": "https://huggingface.co/papers/2408.12757",
      "authors": [
        "Kan Zhu",
        "Liangyu Zhao",
        "Gefei Zuo",
        "Yile Gu",
        "Yufei Gao",
        "Qinyu Xu",
        "Zihao Ye",
        "Stephanie Wang",
        "Arvind Krishnamurthy",
        "Baris Kasikci"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.12757.pdf",
      "abstract": "The increasing usage of Large Language Models (LLMs) has resulted in a\nsurging demand for planet-scale serving systems, where tens of thousands of\nGPUs continuously serve hundreds of millions of users. Consequently, throughput\n(under reasonable latency constraints) has emerged as a key metric that\ndetermines serving systems' performance. To boost throughput, various methods\nof inter-device parallelism (e.g., data, tensor, pipeline) have been explored.\nHowever, existing methods do not consider overlapping the utilization of\ndifferent resources within a single device, leading to underutilization and\nsub-optimal performance.\n  We propose NanoFlow, a novel serving framework that exploits intra-device\nparallelism, which overlaps the usage of resources including compute, memory,\nand network within a single device through operation co-scheduling. To exploit\nintra-device parallelism, NanoFlow introduces two key innovations: First,\nNanoFlow splits requests into nano-batches at the granularity of operations,\nwhich breaks the dependency of sequential operations in LLM inference and\nenables overlapping; then, to get benefit from overlapping, NanoFlow uses an\noperation-level pipeline with execution unit scheduling, which partitions the\ndevice's functional units and simultaneously executes different operations in\neach unit. NanoFlow automates the pipeline setup using a parameter search\nalgorithm, which enables easily porting NanoFlow to different models. We\nimplement NanoFlow on NVIDIA GPUs and evaluate end-to-end serving throughput on\nseveral popular models such as LLaMA-2-70B, Mixtral 8x7B, LLaMA-3-8B, etc..\nWith practical workloads, NanoFlow provides 1.91x throughput boost compared to\nstate-of-the-art serving systems achieving 59% to 72% of optimal throughput\nacross ported models.",
      "upvotes": 15
    },
    {
      "title": "MobileQuant: Mobile-friendly Quantization for On-device Language Models",
      "url": "https://huggingface.co/papers/2408.13933",
      "authors": [
        "Fuwen Tan",
        "Sourav Bhattacharya",
        "Timothy Hospedales",
        "Brais Martinez"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.13933.pdf",
      "abstract": "Large language models (LLMs) have revolutionized language processing,\ndelivering outstanding results across multiple applications. However, deploying\nLLMs on edge devices poses several challenges with respect to memory, energy,\nand compute costs, limiting their widespread use in devices such as mobile\nphones. A promising solution is to reduce the number of bits used to represent\nweights and activations. While existing works have found partial success at\nquantizing LLMs to lower bitwidths, e.g. 4-bit weights, quantizing activations\nbeyond 16 bits often leads to large computational overheads due to poor\non-device quantization support, or a considerable accuracy drop. Yet, 8-bit\nactivations are very attractive for on-device deployment as they would enable\nLLMs to fully exploit mobile-friendly hardware, e.g. Neural Processing Units\n(NPUs). In this work, we make a first attempt to facilitate the on-device\ndeployment of LLMs using integer-only quantization. We first investigate the\nlimitations of existing quantization methods for on-device deployment, with a\nspecial focus on activation quantization. We then address these limitations by\nintroducing a simple post-training quantization method, named MobileQuant, that\nextends previous weight equivalent transformation works by jointly optimizing\nthe weight transformation and activation range parameters in an end-to-end\nmanner. MobileQuant demonstrates superior capabilities over existing methods by\n1) achieving near-lossless quantization on a wide range of LLM benchmarks, 2)\nreducing latency and energy consumption by 20\\%-50\\% compared to current\non-device quantization strategies, 3) requiring limited compute budget, 4)\nbeing compatible with mobile-friendly compute units, e.g. NPU.",
      "upvotes": 13
    },
    {
      "title": "TVG: A Training-free Transition Video Generation Method with Diffusion Models",
      "url": "https://huggingface.co/papers/2408.13413",
      "authors": [
        "Rui Zhang",
        "Wei Wang",
        "Xuming Wen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.13413.pdf",
      "abstract": "Transition videos play a crucial role in media production, enhancing the flow\nand coherence of visual narratives. Traditional methods like morphing often\nlack artistic appeal and require specialized skills, limiting their\neffectiveness. Recent advances in diffusion model-based video generation offer\nnew possibilities for creating transitions but face challenges such as poor\ninter-frame relationship modeling and abrupt content changes. We propose a\nnovel training-free Transition Video Generation (TVG) approach using\nvideo-level diffusion models that addresses these limitations without\nadditional training. Our method leverages Gaussian Process Regression\n(GPR) to model latent representations, ensuring smooth and dynamic\ntransitions between frames. Additionally, we introduce interpolation-based\nconditional controls and a Frequency-aware Bidirectional Fusion (FBiF)\narchitecture to enhance temporal control and transition reliability.\nEvaluations of benchmark datasets and custom image pairs demonstrate the\neffectiveness of our approach in generating high-quality smooth transition\nvideos. The code are provided in https://sobeymil.github.io/tvg.com.",
      "upvotes": 13
    },
    {
      "title": "Efficient Detection of Toxic Prompts in Large Language Models",
      "url": "https://huggingface.co/papers/2408.11727",
      "authors": [
        "Huijia Sun",
        "Ling Shi",
        "Yang Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.11727.pdf",
      "abstract": "Large language models (LLMs) like ChatGPT and Gemini have significantly\nadvanced natural language processing, enabling various applications such as\nchatbots and automated content generation. However, these models can be\nexploited by malicious individuals who craft toxic prompts to elicit harmful or\nunethical responses. These individuals often employ jailbreaking techniques to\nbypass safety mechanisms, highlighting the need for robust toxic prompt\ndetection methods. Existing detection techniques, both blackbox and whitebox,\nface challenges related to the diversity of toxic prompts, scalability, and\ncomputational efficiency. In response, we propose ToxicDetector, a lightweight\ngreybox method designed to efficiently detect toxic prompts in LLMs.\nToxicDetector leverages LLMs to create toxic concept prompts, uses embedding\nvectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)\nclassifier for prompt classification. Our evaluation on various versions of the\nLLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector\nachieves a high accuracy of 96.39\\% and a low false positive rate of 2.00\\%,\noutperforming state-of-the-art methods. Additionally, ToxicDetector's\nprocessing time of 0.0780 seconds per prompt makes it highly suitable for\nreal-time applications. ToxicDetector achieves high accuracy, efficiency, and\nscalability, making it a practical method for toxic prompt detection in LLMs.",
      "upvotes": 11
    },
    {
      "title": "MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware Diffusion and Iterative Refinement",
      "url": "https://huggingface.co/papers/2408.14211",
      "authors": [
        "Xu He",
        "Di Kang",
        "Jiangnan Ye",
        "Chaopeng Zhang",
        "Liyang Chen",
        "Xiangjun Gao",
        "Han Zhang",
        "Haolin Zhuang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.14211.pdf",
      "abstract": "Existing works in single-image human reconstruction suffer from weak\ngeneralizability due to insufficient training data or 3D inconsistencies for a\nlack of comprehensive multi-view knowledge. In this paper, we introduce\nMagicMan, a human-specific multi-view diffusion model designed to generate\nhigh-quality novel view images from a single reference image. As its core, we\nleverage a pre-trained 2D diffusion model as the generative prior for\ngeneralizability, with the parametric SMPL-X model as the 3D body prior to\npromote 3D awareness. To tackle the critical challenge of maintaining\nconsistency while achieving dense multi-view generation for improved 3D human\nreconstruction, we first introduce hybrid multi-view attention to facilitate\nboth efficient and thorough information interchange across different views.\nAdditionally, we present a geometry-aware dual branch to perform concurrent\ngeneration in both RGB and normal domains, further enhancing consistency via\ngeometry cues. Last but not least, to address ill-shaped issues arising from\ninaccurate SMPL-X estimation that conflicts with the reference image, we\npropose a novel iterative refinement strategy, which progressively optimizes\nSMPL-X accuracy while enhancing the quality and consistency of the generated\nmulti-views. Extensive experimental results demonstrate that our method\nsignificantly outperforms existing approaches in both novel view synthesis and\nsubsequent 3D human reconstruction tasks.",
      "upvotes": 9
    }
  ]
}