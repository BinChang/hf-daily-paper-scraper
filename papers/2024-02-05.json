{
  "date": "2024-02-05",
  "papers": [
    {
      "title": "Specialized Language Models with Cheap Inference from Limited Domain Data",
      "url": "https://huggingface.co/papers/2402.01093",
      "authors": [
        "David Grangier",
        "Angelos Katharopoulos",
        "Pierre Ablin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01093.pdf",
      "abstract": "Large language models have emerged as a versatile tool but are challenging to\napply to tasks lacking large inference budgets and large in-domain training\nsets. This work formalizes these constraints and distinguishes four important\nvariables: the pretraining budget (for training before the target domain is\nknown), the specialization budget (for training after the target domain is\nknown), the inference budget, and the in-domain training set size. Across these\nsettings, we compare different approaches from the machine learning literature.\nLimited by inference cost, we find better alternatives to the standard practice\nof training very large vanilla transformer models. In particular, we show that\nhyper-networks and mixture of experts have better perplexity for large\npretraining budgets, while small models trained on importance sampled datasets\nare attractive for large specialization budgets.",
      "upvotes": 45
    },
    {
      "title": "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback",
      "url": "https://huggingface.co/papers/2402.01391",
      "authors": [
        "Yan Liu",
        "Limao Xiong",
        "Junjie Shan",
        "Caishuang Huang",
        "Tao Ji",
        "Rui Zheng",
        "Qi Zhang",
        "Xuanjing Huang",
        "Tao Gui"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01391.pdf",
      "abstract": "The advancement of large language models (LLMs) has significantly propelled\nthe field of code generation. Previous work integrated reinforcement learning\n(RL) with compiler feedback for exploring the output space of LLMs to enhance\ncode generation quality. However, the lengthy code generated by LLMs in\nresponse to complex human requirements makes RL exploration a challenge. Also,\nsince the unit tests may not cover the complicated code, optimizing LLMs by\nusing these unexecuted code snippets is ineffective. To tackle these\nchallenges, we introduce StepCoder, a novel RL framework for code generation,\nconsisting of two main components: CCCS addresses the exploration challenge by\nbreaking the long sequences code generation task into a Curriculum of Code\nCompletion Subtasks, while FGO only optimizes the model by masking the\nunexecuted code segments to provide Fine-Grained Optimization. In addition, we\nfurthermore construct the APPS+ dataset for RL training, which is manually\nverified to ensure the correctness of unit tests. Experimental results show\nthat our method improves the ability to explore the output space and\noutperforms state-of-the-art approaches in corresponding benchmarks.",
      "upvotes": 41
    },
    {
      "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
      "url": "https://huggingface.co/papers/2402.01622",
      "authors": [
        "Yanghua Xiao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01622.pdf",
      "abstract": "Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.",
      "upvotes": 33
    },
    {
      "title": "PokéLLMon: A Human-Parity Agent for Pokémon Battles with Large Language Models",
      "url": "https://huggingface.co/papers/2402.01118",
      "authors": [
        "Ling Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01118.pdf",
      "abstract": "We introduce Pok\\'eLLMon, the first LLM-embodied agent that achieves\nhuman-parity performance in tactical battle games, as demonstrated in Pok\\'emon\nbattles. The design of Pok\\'eLLMon incorporates three key strategies:\n(i) In-context reinforcement learning that instantly consumes text-based\nfeedback derived from battles to iteratively refine the policy; (ii)\nKnowledge-augmented generation that retrieves external knowledge to counteract\nhallucination and enables the agent to act timely and properly; (iii)\nConsistent action generation to mitigate the panic switching\nphenomenon when the agent faces a powerful opponent and wants to elude the\nbattle. We show that online battles against human demonstrates\nPok\\'eLLMon's human-like battle strategies and just-in-time decision\nmaking, achieving 49\\% of win rate in the Ladder competitions and 56\\% of win\nrate in the invited battles. Our implementation and playable battle logs are\navailable at: https://github.com/git-disl/PokeLLMon.",
      "upvotes": 29
    },
    {
      "title": "Boximator: Generating Rich and Controllable Motions for Video Synthesis",
      "url": "https://huggingface.co/papers/2402.01566",
      "authors": [
        "Jiawei Wang",
        "Yuchen Zhang",
        "Liping Yuan",
        "Hang Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01566.pdf",
      "abstract": "Generating rich and controllable motion is a pivotal challenge in video\nsynthesis. We propose Boximator, a new approach for fine-grained motion\ncontrol. Boximator introduces two constraint types: hard box and soft box.\nUsers select objects in the conditional frame using hard boxes and then use\neither type of boxes to roughly or rigorously define the object's position,\nshape, or motion path in future frames. Boximator functions as a plug-in for\nexisting video diffusion models. Its training process preserves the base\nmodel's knowledge by freezing the original weights and training only the\ncontrol module. To address training challenges, we introduce a novel\nself-tracking technique that greatly simplifies the learning of box-object\ncorrelations. Empirically, Boximator achieves state-of-the-art video quality\n(FVD) scores, improving on two base models, and further enhanced after\nincorporating box constraints. Its robust motion controllability is validated\nby drastic increases in the bounding box alignment metric. Human evaluation\nalso shows that users favor Boximator generation results over the base model.",
      "upvotes": 26
    },
    {
      "title": "Repeat After Me: Transformers are Better than State Space Models at Copying",
      "url": "https://huggingface.co/papers/2402.01032",
      "authors": [
        "Sham M. Kakade"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01032.pdf",
      "abstract": "Transformers are the dominant architecture for sequence modeling, but there\nis growing interest in models that use a fixed-size latent state that does not\ndepend on the sequence length, which we refer to as \"generalized state space\nmodels\" (GSSMs). In this paper we show that while GSSMs are promising in terms\nof inference-time efficiency, they are limited compared to transformer models\non tasks that require copying from the input context. We start with a\ntheoretical analysis of the simple task of string copying and prove that a two\nlayer transformer can copy strings of exponential length while GSSMs are\nfundamentally limited by their fixed-size latent state. Empirically, we find\nthat transformers outperform GSSMs in terms of efficiency and generalization on\nsynthetic tasks that require copying the context. Finally, we evaluate\npretrained large language models and find that transformer models dramatically\noutperform state space models at copying and retrieving information from\ncontext. Taken together, these results suggest a fundamental gap between\ntransformers and GSSMs on tasks of practical interest.",
      "upvotes": 22
    },
    {
      "title": "K-Level Reasoning with Large Language Models",
      "url": "https://huggingface.co/papers/2402.01521",
      "authors": [
        "Yadong Zhang",
        "Xun Wang",
        "Yan Xia",
        "Man Lan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01521.pdf",
      "abstract": "While Large Language Models (LLMs) have demonstrated their proficiency in\ncomplex reasoning tasks, their performance in dynamic, interactive, and\ncompetitive scenarios - such as business strategy and stock market analysis -\nremains underexplored. To bridge this gap, we formally explore the dynamic\nreasoning capabilities of LLMs for decision-making in rapidly evolving\nenvironments. We introduce two game theory-based pilot challenges that mirror\nthe complexities of real-world dynamic decision-making. These challenges are\nwell-defined, enabling clear, controllable, and precise evaluation of LLMs'\ndynamic reasoning abilities. Through extensive experiments, we find that\nexisting reasoning methods tend to falter in dynamic settings that require\nk-level thinking - a key concept not tackled by previous works. To address\nthis, we propose a novel reasoning approach for LLMs, named \"K-Level\nReasoning\". This approach adopts the perspective of rivals to recursively\nemploy k-level thinking based on available historical information, which\nsignificantly improves the prediction accuracy of rivals' subsequent moves and\ninforms more strategic decision-making. This research not only sets a robust\nquantitative benchmark for the assessment of dynamic reasoning but also\nmarkedly enhances the proficiency of LLMs in dynamic contexts.",
      "upvotes": 17
    },
    {
      "title": "Nomic Embed: Training a Reproducible Long Context Text Embedder",
      "url": "https://huggingface.co/papers/2402.01613",
      "authors": [
        "John X. Morris"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01613.pdf",
      "abstract": "This technical report describes the training of nomic-embed-text-v1, the\nfirst fully reproducible, open-source, open-weights, open-data, 8192 context\nlength English text embedding model that outperforms both OpenAI Ada-002 and\nOpenAI text-embedding-3-small on short and long-context tasks. We release the\ntraining code and model weights under an Apache 2 license. In contrast with\nother open-source models, we release a training data loader with 235 million\ncurated text pairs that allows for the full replication of nomic-embed-text-v1.\nYou can find code and data to replicate the model at\nhttps://github.com/nomic-ai/contrastors",
      "upvotes": 14
    },
    {
      "title": "EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks",
      "url": "https://huggingface.co/papers/2402.00892",
      "authors": [
        "Shiyi Lan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.00892.pdf",
      "abstract": "The advent of Large Models marks a new era in machine learning, significantly\noutperforming smaller models by leveraging vast datasets to capture and\nsynthesize complex patterns. Despite these advancements, the exploration into\nscaling, especially in the audio generation domain, remains limited, with\nprevious efforts didn't extend into the high-fidelity (HiFi) 44.1kHz domain and\nsuffering from both spectral discontinuities and blurriness in the\nhigh-frequency domain, alongside a lack of robustness against out-of-domain\ndata. These limitations restrict the applicability of models to diverse use\ncases, including music and singing generation. Our work introduces Enhanced\nVarious Audio Generation via Scalable Generative Adversarial Networks\n(EVA-GAN), yields significant improvements over previous state-of-the-art in\nspectral and high-frequency reconstruction and robustness in out-of-domain data\nperformance, enabling the generation of HiFi audios by employing an extensive\ndataset of 36,000 hours of 44.1kHz audio, a context-aware module, a\nHuman-In-The-Loop artifact measurement toolkit, and expands the model to\napproximately 200 million parameters. Demonstrations of our work are available\nat https://double-blind-eva-gan.cc.",
      "upvotes": 13
    }
  ]
}