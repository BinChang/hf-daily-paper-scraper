{
  "date": "2024-06-12",
  "papers": [
    {
      "title": "An Image is Worth 32 Tokens for Reconstruction and Generation",
      "url": "https://huggingface.co/papers/2406.07550",
      "authors": [
        "Daniel Cremers",
        "Liang-Chieh Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07550.pdf",
      "abstract": "Recent advancements in generative models have highlighted the crucial role of\nimage tokenization in the efficient synthesis of high-resolution images.\nTokenization, which transforms images into latent representations, reduces\ncomputational demands compared to directly processing pixels and enhances the\neffectiveness and efficiency of the generation process. Prior methods, such as\nVQGAN, typically utilize 2D latent grids with fixed downsampling factors.\nHowever, these 2D tokenizations face challenges in managing the inherent\nredundancies present in images, where adjacent regions frequently display\nsimilarities. To overcome this issue, we introduce Transformer-based\n1-Dimensional Tokenizer (TiTok), an innovative approach that tokenizes images\ninto 1D latent sequences. TiTok provides a more compact latent representation,\nyielding substantially more efficient and effective representations than\nconventional techniques. For example, a 256 x 256 x 3 image can be reduced to\njust 32 discrete tokens, a significant reduction from the 256 or 1024 tokens\nobtained by prior methods. Despite its compact nature, TiTok achieves\ncompetitive performance to state-of-the-art approaches. Specifically, using the\nsame generator framework, TiTok attains 1.97 gFID, outperforming MaskGIT\nbaseline significantly by 4.21 at ImageNet 256 x 256 benchmark. The advantages\nof TiTok become even more significant when it comes to higher resolution. At\nImageNet 512 x 512 benchmark, TiTok not only outperforms state-of-the-art\ndiffusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the image\ntokens by 64x, leading to 410x faster generation process. Our best-performing\nvariant can significantly surpasses DiT-XL/2 (gFID 2.13 vs. 3.04) while still\ngenerating high-quality samples 74x faster.",
      "upvotes": 55
    },
    {
      "title": "The Prompt Report: A Systematic Survey of Prompting Techniques",
      "url": "https://huggingface.co/papers/2406.06608",
      "authors": [
        "Aayush Gupta",
        "Chau Pham",
        "Feileen Li",
        "Hevander Da Costa",
        "Saloni Gupta"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06608.pdf",
      "abstract": "Generative Artificial Intelligence (GenAI) systems are being increasingly\ndeployed across all parts of industry and research settings. Developers and end\nusers interact with these systems through the use of prompting or prompt\nengineering. While prompting is a widespread and highly researched concept,\nthere exists conflicting terminology and a poor ontological understanding of\nwhat constitutes a prompt due to the area's nascency. This paper establishes a\nstructured understanding of prompts, by assembling a taxonomy of prompting\ntechniques and analyzing their use. We present a comprehensive vocabulary of 33\nvocabulary terms, a taxonomy of 58 text-only prompting techniques, and 40\ntechniques for other modalities. We further present a meta-analysis of the\nentire literature on natural language prefix-prompting.",
      "upvotes": 53
    },
    {
      "title": "McEval: Massively Multilingual Code Evaluation",
      "url": "https://huggingface.co/papers/2406.07436",
      "authors": [
        "Ke Jin",
        "Tao Sun",
        "Changyu Ren",
        "Boyang Wang",
        "Xianjie Wu",
        "Bing Wang",
        "Liqun Yang",
        "Sufeng Duan",
        "Zhoujun Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07436.pdf",
      "abstract": "Code large language models (LLMs) have shown remarkable advances in code\nunderstanding, completion, and generation tasks. Programming benchmarks,\ncomprised of a selection of code challenges and corresponding test cases, serve\nas a standard to evaluate the capability of different LLMs in such tasks.\nHowever, most existing benchmarks primarily focus on Python and are still\nrestricted to a limited number of languages, where other languages are\ntranslated from the Python samples (e.g. MultiPL-E) degrading the data\ndiversity. To further facilitate the research of code LLMs, we propose a\nmassively multilingual code benchmark covering 40 programming languages\n(McEval) with 16K test samples, which substantially pushes the limits of code\nLLMs in multilingual scenarios. The benchmark contains challenging code\ncompletion, understanding, and generation evaluation tasks with finely curated\nmassively multilingual instruction corpora McEval-Instruct. In addition, we\nintroduce an effective multilingual coder mCoder trained on McEval-Instruct to\nsupport multilingual programming language generation. Extensive experimental\nresults on McEval show that there is still a difficult journey between\nopen-source models and closed-source LLMs (e.g. GPT-series models) in numerous\nlanguages. The instruction corpora, evaluation benchmark, and leaderboard are\navailable at https://mceval.github.io/.",
      "upvotes": 39
    },
    {
      "title": "Zero-shot Image Editing with Reference Imitation",
      "url": "https://huggingface.co/papers/2406.07547",
      "authors": [
        "Yujun Shen",
        "Hengshuang Zhao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07547.pdf",
      "abstract": "Image editing serves as a practical yet challenging task considering the\ndiverse demands from users, where one of the hardest parts is to precisely\ndescribe how the edited image should look like. In this work, we present a new\nform of editing, termed imitative editing, to help users exercise their\ncreativity more conveniently. Concretely, to edit an image region of interest,\nusers are free to directly draw inspiration from some in-the-wild references\n(e.g., some relative pictures come across online), without having to cope with\nthe fit between the reference and the source. Such a design requires the system\nto automatically figure out what to expect from the reference to perform the\nediting. For this purpose, we propose a generative training framework, dubbed\nMimicBrush, which randomly selects two frames from a video clip, masks some\nregions of one frame, and learns to recover the masked regions using the\ninformation from the other frame. That way, our model, developed from a\ndiffusion prior, is able to capture the semantic correspondence between\nseparate images in a self-supervised manner. We experimentally show the\neffectiveness of our method under various test cases as well as its superiority\nover existing alternatives. We also construct a benchmark to facilitate further\nresearch.",
      "upvotes": 30
    },
    {
      "title": "TextGrad: Automatic \"Differentiation\" via Text",
      "url": "https://huggingface.co/papers/2406.07496",
      "authors": [
        "Joseph Boen",
        "Carlos Guestrin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07496.pdf",
      "abstract": "AI is undergoing a paradigm shift, with breakthroughs achieved by systems\norchestrating multiple large language models (LLMs) and other complex\ncomponents. As a result, developing principled and automated optimization\nmethods for compound AI systems is one of the most important new challenges.\nNeural networks faced a similar challenge in its early days until\nbackpropagation and automatic differentiation transformed the field by making\noptimization turn-key. Inspired by this, we introduce TextGrad, a powerful\nframework performing automatic ``differentiation'' via text. TextGrad\nbackpropagates textual feedback provided by LLMs to improve individual\ncomponents of a compound AI system. In our framework, LLMs provide rich,\ngeneral, natural language suggestions to optimize variables in computation\ngraphs, ranging from code snippets to molecular structures. TextGrad follows\nPyTorch's syntax and abstraction and is flexible and easy-to-use. It works\nout-of-the-box for a variety of tasks, where the users only provide the\nobjective function without tuning components or prompts of the framework. We\nshowcase TextGrad's effectiveness and generality across a diverse range of\napplications, from question answering and molecule optimization to radiotherapy\ntreatment planning. Without modifying the framework, TextGrad improves the\nzero-shot accuracy of GPT-4o in Google-Proof Question Answering from 51% to\n55%, yields 20% relative performance gain in optimizing LeetCode-Hard\ncoding problem solutions, improves prompts for reasoning, designs new druglike\nsmall molecules with desirable in silico binding, and designs radiation\noncology treatment plans with high specificity. TextGrad lays a foundation to\naccelerate the development of the next-generation of AI systems.",
      "upvotes": 26
    },
    {
      "title": "Improve Mathematical Reasoning in Language Models by Automated Process Supervision",
      "url": "https://huggingface.co/papers/2406.06592",
      "authors": [
        "Rosanne Liu",
        "Lei Shu",
        "Yun Zhu",
        "Lei Meng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06592.pdf",
      "abstract": "Complex multi-step reasoning tasks, such as solving mathematical problems or\ngenerating code, remain a significant hurdle for even the most advanced large\nlanguage models (LLMs). Verifying LLM outputs with an Outcome Reward Model\n(ORM) is a standard inference-time technique aimed at enhancing the reasoning\nperformance of LLMs. However, this still proves insufficient for reasoning\ntasks with a lengthy or multi-hop reasoning chain, where the intermediate\noutcomes are neither properly rewarded nor penalized. Process supervision\naddresses this limitation by assigning intermediate rewards during the\nreasoning process. To date, the methods used to collect process supervision\ndata have relied on either human annotation or per-step Monte Carlo estimation,\nboth prohibitively expensive to scale, thus hindering the broad application of\nthis technique. In response to this challenge, we propose a novel\ndivide-and-conquer style Monte Carlo Tree Search (MCTS) algorithm named\nOmegaPRM for the efficient collection of high-quality process\nsupervision data. This algorithm swiftly identifies the first error in the\nChain of Thought (CoT) with binary search and balances the positive and\nnegative examples, thereby ensuring both efficiency and quality. As a result,\nwe are able to collect over 1.5 million process supervision annotations to\ntrain a Process Reward Model (PRM). Utilizing this fully automated process\nsupervision alongside the weighted self-consistency algorithm, we have enhanced\nthe instruction tuned Gemini Pro model's math reasoning performance, achieving\na 69.4\\% success rate on the MATH benchmark, a 36\\% relative improvement from\nthe 51\\% base model performance. Additionally, the entire process operates\nwithout any human intervention, making our method both financially and\ncomputationally cost-effective compared to existing methods.",
      "upvotes": 24
    },
    {
      "title": "Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B",
      "url": "https://huggingface.co/papers/2406.07394",
      "authors": [
        "Jiatong Li",
        "Xiaoshui Huang",
        "Wanli Ouyang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07394.pdf",
      "abstract": "This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative\nintegration of Large Language Models (LLMs) with Monte Carlo Tree Search\n(MCTS), designed to enhance performance in complex mathematical reasoning\ntasks. Addressing the challenges of accuracy and reliability in LLMs,\nparticularly in strategic and mathematical reasoning, MCTSr leverages\nsystematic exploration and heuristic self-refine mechanisms to improve\ndecision-making frameworks within LLMs. The algorithm constructs a Monte Carlo\nsearch tree through iterative processes of Selection, self-refine,\nself-evaluation, and Backpropagation, utilizing an improved Upper Confidence\nBound (UCB) formula to optimize the exploration-exploitation balance. Extensive\nexperiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical\nproblems, significantly improving success rates across multiple datasets,\nincluding GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math\nOdyssey, AIME, and OlympiadBench. The study advances the application of LLMs in\ncomplex reasoning tasks and sets a foundation for future AI integration,\nenhancing decision-making accuracy and reliability in LLM-driven applications.",
      "upvotes": 22
    },
    {
      "title": "Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models",
      "url": "https://huggingface.co/papers/2406.06563",
      "authors": [
        "Bo Zhu",
        "Biye Li",
        "Weiwei LÃ¼",
        "Yutuan Ma",
        "Rui Hu",
        "Shuicheng Yan",
        "Han Fang",
        "Yahui Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06563.pdf",
      "abstract": "In this technical report, we introduce the training methodologies implemented\nin the development of Skywork-MoE, a high-performance mixture-of-experts (MoE)\nlarge language model (LLM) with 146 billion parameters and 16 experts. It is\ninitialized from the pre-existing dense checkpoints of our Skywork-13B model.\nWe explore the comparative effectiveness of upcycling versus training from\nscratch initializations. Our findings suggest that the choice between these two\napproaches should consider both the performance of the existing dense\ncheckpoints and the MoE training budget. We highlight two innovative\ntechniques: gating logit normalization, which improves expert diversification,\nand adaptive auxiliary loss coefficients, allowing for layer-specific\nadjustment of auxiliary loss coefficients. Our experimental results validate\nthe effectiveness of these methods. Leveraging these techniques and insights,\nwe trained our upcycled Skywork-MoE on a condensed subset of our SkyPile\ncorpus. The evaluation results demonstrate that our model delivers strong\nperformance across a wide range of benchmarks.",
      "upvotes": 17
    },
    {
      "title": "SEE-2-SOUND: Zero-Shot Spatial Environment-to-Spatial Sound",
      "url": "https://huggingface.co/papers/2406.06612",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.06612.pdf",
      "abstract": "Generating combined visual and auditory sensory experiences is critical for\nthe consumption of immersive content. Recent advances in neural generative\nmodels have enabled the creation of high-resolution content across multiple\nmodalities such as images, text, speech, and videos. Despite these successes,\nthere remains a significant gap in the generation of high-quality spatial audio\nthat complements generated visual content. Furthermore, current audio\ngeneration models excel in either generating natural audio or speech or music\nbut fall short in integrating spatial audio cues necessary for immersive\nexperiences. In this work, we introduce SEE-2-SOUND, a zero-shot approach that\ndecomposes the task into (1) identifying visual regions of interest; (2)\nlocating these elements in 3D space; (3) generating mono-audio for each; and\n(4) integrating them into spatial audio. Using our framework, we demonstrate\ncompelling results for generating spatial audio for high-quality videos,\nimages, and dynamic images from the internet, as well as media generated by\nlearned approaches.",
      "upvotes": 14
    },
    {
      "title": "AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising",
      "url": "https://huggingface.co/papers/2406.06911",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.06911.pdf",
      "abstract": "Diffusion models have garnered significant interest from the community for\ntheir great generative ability across various applications. However, their\ntypical multi-step sequential-denoising nature gives rise to high cumulative\nlatency, thereby precluding the possibilities of parallel computation. To\naddress this, we introduce AsyncDiff, a universal and plug-and-play\nacceleration scheme that enables model parallelism across multiple devices. Our\napproach divides the cumbersome noise prediction model into multiple\ncomponents, assigning each to a different device. To break the dependency chain\nbetween these components, it transforms the conventional sequential denoising\ninto an asynchronous process by exploiting the high similarity between hidden\nstates in consecutive diffusion steps. Consequently, each component is\nfacilitated to compute in parallel on separate devices. The proposed strategy\nsignificantly reduces inference latency while minimally impacting the\ngenerative quality. Specifically, for the Stable Diffusion v2.1, AsyncDiff\nachieves a 2.7x speedup with negligible degradation and a 4.0x speedup with\nonly a slight reduction of 0.38 in CLIP Score, on four NVIDIA A5000 GPUs. Our\nexperiments also demonstrate that AsyncDiff can be readily applied to video\ndiffusion models with encouraging performances. The code is available at\nhttps://github.com/czg1225/AsyncDiff.",
      "upvotes": 10
    },
    {
      "title": "4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models",
      "url": "https://huggingface.co/papers/2406.07472",
      "authors": [
        "Heng Yu",
        "Sergey Tulyakov"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07472.pdf",
      "abstract": "Existing dynamic scene generation methods mostly rely on distilling knowledge\nfrom pre-trained 3D generative models, which are typically fine-tuned on\nsynthetic object datasets. As a result, the generated scenes are often\nobject-centric and lack photorealism. To address these limitations, we\nintroduce a novel pipeline designed for photorealistic text-to-4D scene\ngeneration, discarding the dependency on multi-view generative models and\ninstead fully utilizing video generative models trained on diverse real-world\ndatasets. Our method begins by generating a reference video using the video\ngeneration model. We then learn the canonical 3D representation of the video\nusing a freeze-time video, delicately generated from the reference video. To\nhandle inconsistencies in the freeze-time video, we jointly learn a per-frame\ndeformation to model these imperfections. We then learn the temporal\ndeformation based on the canonical representation to capture dynamic\ninteractions in the reference video. The pipeline facilitates the generation of\ndynamic scenes with enhanced photorealism and structural integrity, viewable\nfrom multiple perspectives, thereby setting a new standard in 4D scene\ngeneration.",
      "upvotes": 10
    },
    {
      "title": "MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering",
      "url": "https://huggingface.co/papers/2406.06573",
      "authors": [
        "Junaid Bajwa",
        "Carey E. Priebe"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06573.pdf",
      "abstract": "Large language models (LLM) have achieved impressive performance on medical\nquestion-answering benchmarks. However, high benchmark accuracy does not imply\nthat the performance generalizes to real-world clinical settings. Medical\nquestion-answering benchmarks rely on assumptions consistent with quantifying\nLLM performance but that may not hold in the open world of the clinic. Yet LLMs\nlearn broad knowledge that can help the LLM generalize to practical conditions\nregardless of unrealistic assumptions in celebrated benchmarks. We seek to\nquantify how well LLM medical question-answering benchmark performance\ngeneralizes when benchmark assumptions are violated. Specifically, we present\nan adversarial method that we call MedFuzz (for medical fuzzing). MedFuzz\nattempts to modify benchmark questions in ways aimed at confounding the LLM. We\ndemonstrate the approach by targeting strong assumptions about patient\ncharacteristics presented in the MedQA benchmark. Successful \"attacks\" modify a\nbenchmark item in ways that would be unlikely to fool a medical expert but\nnonetheless \"trick\" the LLM into changing from a correct to an incorrect\nanswer. Further, we present a permutation test technique that can ensure a\nsuccessful attack is statistically significant. We show how to use performance\non a \"MedFuzzed\" benchmark, as well as individual successful attacks. The\nmethods show promise at providing insights into the ability of an LLM to\noperate robustly in more realistic settings.",
      "upvotes": 8
    },
    {
      "title": "Separating the \"Chirp\" from the \"Chat\": Self-supervised Visual Grounding of Sound and Language",
      "url": "https://huggingface.co/papers/2406.05629",
      "authors": [
        "Andrew Zisserman",
        "John R. Hershey",
        "William T. Freeman"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05629.pdf",
      "abstract": "We present DenseAV, a novel dual encoder grounding architecture that learns\nhigh-resolution, semantically meaningful, and audio-visually aligned features\nsolely through watching videos. We show that DenseAV can discover the\n``meaning'' of words and the ``location'' of sounds without explicit\nlocalization supervision. Furthermore, it automatically discovers and\ndistinguishes between these two types of associations without supervision. We\nshow that DenseAV's localization abilities arise from a new multi-head feature\naggregation operator that directly compares dense image and audio\nrepresentations for contrastive learning. In contrast, many other systems that\nlearn ``global'' audio and video representations cannot localize words and\nsound. Finally, we contribute two new datasets to improve the evaluation of AV\nrepresentations through speech and sound prompted semantic segmentation. On\nthese and other datasets we show DenseAV dramatically outperforms the prior art\non speech and sound prompted semantic segmentation. DenseAV outperforms the\nprevious state-of-the-art, ImageBind, on cross-modal retrieval using fewer than\nhalf of the parameters. Project Page:\nhttps://aka.ms/denseav{https://aka.ms/denseav}",
      "upvotes": 7
    },
    {
      "title": "Simple and Effective Masked Diffusion Language Models",
      "url": "https://huggingface.co/papers/2406.07524",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.07524.pdf",
      "abstract": "While diffusion models excel at generating high-quality images, prior work\nreports a significant performance gap between diffusion and autoregressive (AR)\nmethods in language modeling. In this work, we show that simple masked discrete\ndiffusion is more performant than previously thought. We apply an effective\ntraining recipe that improves the performance of masked diffusion models and\nderive a simplified, Rao-Blackwellized objective that results in additional\nimprovements. Our objective has a simple form -- it is a mixture of classical\nmasked language modeling losses -- and can be used to train encoder-only\nlanguage models that admit efficient samplers, including ones that can generate\narbitrary lengths of text semi-autoregressively like a traditional language\nmodel. On language modeling benchmarks, a range of masked diffusion models\ntrained with modern engineering practices achieves a new state-of-the-art among\ndiffusion models, and approaches AR perplexity. We release our code at:\nhttps://github.com/kuleshov-group/mdlm",
      "upvotes": 7
    },
    {
      "title": "Neural Gaffer: Relighting Any Object via Diffusion",
      "url": "https://huggingface.co/papers/2406.07520",
      "authors": [
        "Yuan Li",
        "Kai Zhang",
        "Jin Sun"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07520.pdf",
      "abstract": "Single-image relighting is a challenging task that involves reasoning about\nthe complex interplay between geometry, materials, and lighting. Many prior\nmethods either support only specific categories of images, such as portraits,\nor require special capture conditions, like using a flashlight. Alternatively,\nsome methods explicitly decompose a scene into intrinsic components, such as\nnormals and BRDFs, which can be inaccurate or under-expressive. In this work,\nwe propose a novel end-to-end 2D relighting diffusion model, called Neural\nGaffer, that takes a single image of any object and can synthesize an accurate,\nhigh-quality relit image under any novel environmental lighting condition,\nsimply by conditioning an image generator on a target environment map, without\nan explicit scene decomposition. Our method builds on a pre-trained diffusion\nmodel, and fine-tunes it on a synthetic relighting dataset, revealing and\nharnessing the inherent understanding of lighting present in the diffusion\nmodel. We evaluate our model on both synthetic and in-the-wild Internet imagery\nand demonstrate its advantages in terms of generalization and accuracy.\nMoreover, by combining with other generative methods, our model enables many\ndownstream 2D tasks, such as text-based relighting and object insertion. Our\nmodel can also operate as a strong relighting prior for 3D tasks, such as\nrelighting a radiance field.",
      "upvotes": 4
    },
    {
      "title": "Merging Improves Self-Critique Against Jailbreak Attacks",
      "url": "https://huggingface.co/papers/2406.07188",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.07188.pdf",
      "abstract": "The robustness of large language models (LLMs) against adversarial\nmanipulations, such as jailbreak attacks, remains a significant challenge. In\nthis work, we propose an approach that enhances the self-critique capability of\nthe LLM and further fine-tunes it over sanitized synthetic data. This is done\nwith the addition of an external critic model that can be merged with the\noriginal, thus bolstering self-critique capabilities and improving the\nrobustness of the LLMs response to adversarial prompts. Our results demonstrate\nthat the combination of merging and self-critique can reduce the attack success\nrate of adversaries significantly, thus offering a promising defense mechanism\nagainst jailbreak attacks. Code, data and models released at\nhttps://github.com/vicgalle/merging-self-critique-jailbreaks .",
      "upvotes": 3
    }
  ]
}