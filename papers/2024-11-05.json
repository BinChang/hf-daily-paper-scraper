{
  "date": "2024-11-05",
  "papers": [
    {
      "title": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents",
      "url": "https://huggingface.co/papers/2410.24024",
      "authors": [
        "Yifan Xu",
        "Hao Yu",
        "Dan Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.24024.pdf",
      "abstract": "Autonomous agents have become increasingly important for interacting with the\nreal world. Android agents, in particular, have been recently a\nfrequently-mentioned interaction method. However, existing studies for training\nand evaluating Android agents lack systematic research on both open-source and\nclosed-source models. In this work, we propose AndroidLab as a systematic\nAndroid agent framework. It includes an operation environment with different\nmodalities, action space, and a reproducible benchmark. It supports both large\nlanguage models (LLMs) and multimodal models (LMMs) in the same action space.\nAndroidLab benchmark includes predefined Android virtual devices and 138 tasks\nacross nine apps built on these devices. By using the AndroidLab environment,\nwe develop an Android Instruction dataset and train six open-source LLMs and\nLMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from\n1.93% to 13.28% for LMMs. AndroidLab is open-sourced and publicly available at\nhttps://github.com/THUDM/Android-Lab.",
      "upvotes": 45
    },
    {
      "title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM Quantization",
      "url": "https://huggingface.co/papers/2411.02355",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2411.02355.pdf",
      "abstract": "Despite the popularity of large language model (LLM) quantization for\ninference acceleration, significant uncertainty remains regarding the\naccuracy-performance trade-offs associated with various quantization formats.\nWe present a comprehensive empirical study of quantized accuracy, evaluating\npopular quantization formats (FP8, INT8, INT4) across academic benchmarks and\nreal-world tasks, on the entire Llama-3.1 model family. Additionally, our study\nexamines the difference in text generated by quantized models versus their\nuncompressed counterparts. Beyond benchmarks, we also present a couple of\nquantization improvements which allowed us to obtain state-of-the-art accuracy\nrecovery results. Our investigation, encompassing over 500,000 individual\nevaluations, yields several key findings: (1) FP8 weight and activation\nquantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and\nactivation quantization (W8A8-INT), when properly tuned, incurs surprisingly\nlow 1-3% accuracy degradation, and (3) INT4 weight-only quantization\n(W4A16-INT) is competitive with 8-bit integer weight and activation\nquantization. To address the question of the \"best\" format for a given\ndeployment environment, we conduct inference performance analysis using the\npopular open-source vLLM framework on various GPU architectures. We find that\nW4A16 offers the best cost-efficiency for synchronous deployments, and for\nasynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel\nin asynchronous \"continuous batching\" deployment of mid- and large-size models\non high-end GPUs. Our results provide a set of practical guidelines for\ndeploying quantized LLMs across scales and performance requirements.",
      "upvotes": 44
    },
    {
      "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning",
      "url": "https://huggingface.co/papers/2411.02337",
      "authors": [
        "Iat Long Iong",
        "Xinyue Yang",
        "Wei Xu",
        "Jie Tang",
        "Yuxiao Dong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02337.pdf",
      "abstract": "Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems.",
      "upvotes": 32
    },
    {
      "title": "How Far is Video Generation from World Model: A Physical Law Perspective",
      "url": "https://huggingface.co/papers/2411.02385",
      "authors": [
        "Rui Lu",
        "Zhijie Lin",
        "Yang Zhao",
        "Kaixin Wang",
        "Gao Huang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02385.pdf",
      "abstract": "OpenAI's Sora highlights the potential of video generation for developing\nworld models that adhere to fundamental physical laws. However, the ability of\nvideo generation models to discover such laws purely from visual data without\nhuman priors can be questioned. A world model learning the true law should give\npredictions robust to nuances and correctly extrapolate on unseen scenarios. In\nthis work, we evaluate across three key scenarios: in-distribution,\nout-of-distribution, and combinatorial generalization. We developed a 2D\nsimulation testbed for object movement and collisions to generate videos\ndeterministically governed by one or more classical mechanics laws. This\nprovides an unlimited supply of data for large-scale experimentation and\nenables quantitative evaluation of whether the generated videos adhere to\nphysical laws. We trained diffusion-based video generation models to predict\nobject movements based on initial frames. Our scaling experiments show perfect\ngeneralization within the distribution, measurable scaling behavior for\ncombinatorial generalization, but failure in out-of-distribution scenarios.\nFurther experiments reveal two key insights about the generalization mechanisms\nof these models: (1) the models fail to abstract general physical rules and\ninstead exhibit \"case-based\" generalization behavior, i.e., mimicking the\nclosest training example; (2) when generalizing to new cases, models are\nobserved to prioritize different factors when referencing training data: color\n> size > velocity > shape. Our study suggests that scaling alone is\ninsufficient for video generation models to uncover fundamental physical laws,\ndespite its role in Sora's broader success. See our project page at\nhttps://phyworld.github.io",
      "upvotes": 29
    },
    {
      "title": "MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D",
      "url": "https://huggingface.co/papers/2411.02336",
      "authors": [
        "Juncheng Mu",
        "Xianfang Zeng",
        "Xin Chen",
        "Anqi Pang",
        "Chi Zhang",
        "Zhibin Wang",
        "Bin Fu",
        "Gang Yu",
        "Ziwei Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02336.pdf",
      "abstract": "Texturing is a crucial step in the 3D asset production workflow, which\nenhances the visual appeal and diversity of 3D assets. Despite recent\nadvancements in Text-to-Texture (T2T) generation, existing methods often yield\nsubpar results, primarily due to local discontinuities, inconsistencies across\nmultiple views, and their heavy dependence on UV unwrapping outcomes. To tackle\nthese challenges, we propose a novel generation-refinement 3D texturing\nframework called MVPaint, which can generate high-resolution, seamless textures\nwhile emphasizing multi-view consistency. MVPaint mainly consists of three key\nmodules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model,\nMVPaint first simultaneously generates multi-view images by employing an SMG\nmodel, which leads to coarse texturing results with unpainted parts due to\nmissing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete\n3D texturing, we introduce the S3I method, specifically designed to effectively\ntexture previously unobserved areas. 3) UV Refinement (UVR). Furthermore,\nMVPaint employs a UVR module to improve the texture quality in the UV space,\nwhich first performs a UV-space Super-Resolution, followed by a Spatial-aware\nSeam-Smoothing algorithm for revising spatial texturing discontinuities caused\nby UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the\nObjaverse T2T benchmark and the GSO T2T benchmark, based on selected\nhigh-quality 3D meshes from the Objaverse dataset and the entire GSO dataset,\nrespectively. Extensive experimental results demonstrate that MVPaint surpasses\nexisting state-of-the-art methods. Notably, MVPaint could generate\nhigh-fidelity textures with minimal Janus issues and highly enhanced cross-view\nconsistency.",
      "upvotes": 23
    },
    {
      "title": "Survey of Cultural Awareness in Language Models: Text and Beyond",
      "url": "https://huggingface.co/papers/2411.00860",
      "authors": [
        "Junyeong Park",
        "Arnav Arora"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00860.pdf",
      "abstract": "Large-scale deployment of large language models (LLMs) in various\napplications, such as chatbots and virtual assistants, requires LLMs to be\nculturally sensitive to the user to ensure inclusivity. Culture has been widely\nstudied in psychology and anthropology, and there has been a recent surge in\nresearch on making LLMs more culturally inclusive in LLMs that goes beyond\nmultilinguality and builds on findings from psychology and anthropology. In\nthis paper, we survey efforts towards incorporating cultural awareness into\ntext-based and multimodal LLMs. We start by defining cultural awareness in\nLLMs, taking the definitions of culture from anthropology and psychology as a\npoint of departure. We then examine methodologies adopted for creating\ncross-cultural datasets, strategies for cultural inclusion in downstream tasks,\nand methodologies that have been used for benchmarking cultural awareness in\nLLMs. Further, we discuss the ethical implications of cultural alignment, the\nrole of Human-Computer Interaction in driving cultural inclusion in LLMs, and\nthe role of cultural alignment in driving social science research. We finally\nprovide pointers to future research based on our findings about gaps in the\nliterature.",
      "upvotes": 23
    },
    {
      "title": "Training-free Regional Prompting for Diffusion Transformers",
      "url": "https://huggingface.co/papers/2411.02395",
      "authors": [
        "Wenzhao Zheng",
        "Renrui Zhang",
        "Shanghang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02395.pdf",
      "abstract": "Diffusion models have demonstrated excellent capabilities in text-to-image\ngeneration. Their semantic understanding (i.e., prompt following) ability has\nalso been greatly improved with large language models (e.g., T5, Llama).\nHowever, existing models cannot perfectly handle long and complex text prompts,\nespecially when the text prompts contain various objects with numerous\nattributes and interrelated spatial relationships. While many regional\nprompting methods have been proposed for UNet-based models (SD1.5, SDXL), but\nthere are still no implementations based on the recent Diffusion Transformer\n(DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and\nimplement regional prompting for FLUX.1 based on attention manipulation, which\nenables DiT with fined-grained compositional text-to-image generation\ncapability in a training-free manner. Code is available at\nhttps://github.com/antonioo-c/Regional-Prompting-FLUX.",
      "upvotes": 22
    },
    {
      "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent",
      "url": "https://huggingface.co/papers/2411.02265",
      "authors": [
        "Xingwu Sun",
        "Jiaqi Zhu",
        "Kai Zhang",
        "Shuaipeng Li",
        "Zhen Yang",
        "Jonny Han",
        "Xiaobo Shu",
        "Jiahao Bu",
        "Zhongzhi Chen",
        "Xuemeng Huang",
        "Fengzong Lian",
        "Saiyong Yang",
        "Jianfeng Yan",
        "Yuyuan Zeng",
        "Xiaoqin Ren",
        "Chao Yu",
        "Lulu Wu",
        "Yue Mao",
        "Tao Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02265.pdf",
      "abstract": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
      "upvotes": 22
    },
    {
      "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
      "url": "https://huggingface.co/papers/2411.02397",
      "authors": [
        "Haozhe Liu",
        "Sen He",
        "Ding Liu",
        "Menglin Jia",
        "Michael S. Ryoo",
        "Tian Xie"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02397.pdf",
      "abstract": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
      "upvotes": 18
    },
    {
      "title": "GenXD: Generating Any 3D and 4D Scenes",
      "url": "https://huggingface.co/papers/2411.02319",
      "authors": [
        "Yuyang Zhao",
        "Chung-Ching Lin",
        "Lijuan Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02319.pdf",
      "abstract": "Recent developments in 2D visual generation have been remarkably successful.\nHowever, 3D and 4D generation remain challenging in real-world applications due\nto the lack of large-scale 4D data and effective model design. In this paper,\nwe propose to jointly investigate general 3D and 4D generation by leveraging\ncamera and object movements commonly observed in daily life. Due to the lack of\nreal-world 4D data in the community, we first propose a data curation pipeline\nto obtain camera poses and object motion strength from videos. Based on this\npipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K.\nBy leveraging all the 3D and 4D data, we develop our framework, GenXD, which\nallows us to produce any 3D or 4D scene. We propose multiview-temporal modules,\nwhich disentangle camera and object movements, to seamlessly learn from both 3D\nand 4D data. Additionally, GenXD employs masked latent conditions to support a\nvariety of conditioning views. GenXD can generate videos that follow the camera\ntrajectory as well as consistent 3D views that can be lifted into 3D\nrepresentations. We perform extensive evaluations across various real-world and\nsynthetic datasets, demonstrating GenXD's effectiveness and versatility\ncompared to previous methods in 3D and 4D generation.",
      "upvotes": 16
    },
    {
      "title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models",
      "url": "https://huggingface.co/papers/2411.00836",
      "authors": [
        "Xingang Guo",
        "Bin Hu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00836.pdf",
      "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great\npotential in tackling mathematical reasoning tasks that involve visual context.\nUnlike humans who can reliably apply solution steps to similar problems with\nminor modifications, we found that SOTA VLMs like GPT-4o can consistently fail\nin these scenarios, revealing limitations in their mathematical reasoning\ncapabilities. In this paper, we investigate the mathematical reasoning\nrobustness in VLMs and evaluate how well these models perform under different\nvariants of the same question, such as changes in visual numerical values or\nfunction graphs. While several vision-based math benchmarks have been developed\nto assess VLMs' problem-solving capabilities, these benchmarks contain only\nstatic sets of problems and cannot easily evaluate mathematical reasoning\nrobustness. To fill this gap, we introduce DynaMath, a dynamic visual math\nbenchmark designed for in-depth assessment of VLMs. DynaMath includes 501\nhigh-quality, multi-topic seed questions, each represented as a Python program.\nThose programs are carefully designed and annotated to enable the automatic\ngeneration of a much larger set of concrete questions, including many different\ntypes of visual and textual variations. DynaMath allows us to evaluate the\ngeneralization ability of VLMs, by assessing their performance under varying\ninput conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010\ngenerated concrete questions. Our results show that the worst-case model\naccuracy, defined as the percentage of correctly answered seed questions in all\n10 variants, is significantly lower than the average-case accuracy. Our\nanalysis emphasizes the need to study the robustness of VLMs' reasoning\nabilities, and DynaMath provides valuable insights to guide the development of\nmore reliable models for mathematical reasoning.",
      "upvotes": 15
    },
    {
      "title": "AutoVFX: Physically Realistic Video Editing from Natural Language Instructions",
      "url": "https://huggingface.co/papers/2411.02394",
      "authors": [
        "Zhi-Hao Lin",
        "Albert Zhai",
        "Hongchi Xia",
        "Shenlong Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02394.pdf",
      "abstract": "Modern visual effects (VFX) software has made it possible for skilled artists\nto create imagery of virtually anything. However, the creation process remains\nlaborious, complex, and largely inaccessible to everyday users. In this work,\nwe present AutoVFX, a framework that automatically creates realistic and\ndynamic VFX videos from a single video and natural language instructions. By\ncarefully integrating neural scene modeling, LLM-based code generation, and\nphysical simulation, AutoVFX is able to provide physically-grounded,\nphotorealistic editing effects that can be controlled directly using natural\nlanguage instructions. We conduct extensive experiments to validate AutoVFX's\nefficacy across a diverse spectrum of videos and instructions. Quantitative and\nqualitative results suggest that AutoVFX outperforms all competing methods by a\nlarge margin in generative quality, instruction alignment, editing versatility,\nand physical plausibility.",
      "upvotes": 14
    },
    {
      "title": "DynaSaur: Large Language Agents Beyond Predefined Actions",
      "url": "https://huggingface.co/papers/2411.01747",
      "authors": [
        "Seunghyun Yoon",
        "Ryan A. Rossi",
        "Handong Zhao",
        "Puneet Mathur",
        "Nedim Lipka",
        "Yu Wang",
        "Trung Bui",
        "Tianyi Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01747.pdf",
      "abstract": "Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly-scoped environments, we argue that it presents two major challenges\nwhen deploying LLM agents in real-world scenarios: (1) selecting from a fixed\nset of actions significantly restricts the planning and acting capabilities of\nLLM agents, and (2) this approach requires substantial human effort to\nenumerate and implement all possible actions, which becomes impractical in\ncomplex environments with a vast number of potential actions. In this work, we\npropose an LLM agent framework that enables the dynamic creation and\ncomposition of actions in an online manner. In this framework, the agent\ninteracts with the environment by generating and executing programs written in\na general-purpose programming language at each step. Furthermore, generated\nactions are accumulated over time for future reuse. Our extensive experiments\non the GAIA benchmark demonstrate that this framework offers significantly\ngreater flexibility and outperforms previous methods. Notably, it allows an LLM\nagent to recover in scenarios where no relevant action exists in the predefined\nset or when existing actions fail due to unforeseen edge cases. At the time of\nwriting, we hold the top position on the GAIA public leaderboard. Our code can\nbe found in\nhttps://github.com/adobe-research/dynasaur{https://github.com/adobe-research/dynasaur}.",
      "upvotes": 13
    },
    {
      "title": "PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance",
      "url": "https://huggingface.co/papers/2411.02327",
      "authors": [
        "Haoran Tang",
        "Haibo Liu",
        "Yixiao Ge",
        "Ying Shan",
        "Chen Li",
        "Jiankun Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02327.pdf",
      "abstract": "The past year has witnessed the significant advancement of video-based large\nlanguage models. However, the challenge of developing a unified model for both\nshort and long video understanding remains unresolved. Most existing video LLMs\ncannot handle hour-long videos, while methods custom for long videos tend to be\nineffective for shorter videos and images. In this paper, we identify the key\nissue as the redundant content in videos. To address this, we propose a novel\npooling strategy that simultaneously achieves token compression and\ninstruction-aware visual feature aggregation. Our model is termed Prompt-guided\nPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three\ncore components: the CLIP-based visual-prompt alignment that extracts visual\ninformation relevant to the user's instructions, the prompt-guided pooling that\ncompresses the visual sequence to arbitrary scales using convolution-style\npooling, and the clip context extension designed for lengthy prompt common in\nvisual dialogue. Moreover, our codebase also integrates the most advanced video\nDirect Preference Optimization (DPO) and visual interleave training. Extensive\nexperiments have validated the performance of our model. With superior\nthroughput and only 1024 visual context, PPLLaVA achieves better results on\nimage benchmarks as a video LLM, while achieving state-of-the-art performance\nacross various video benchmarks, excelling in tasks ranging from caption\ngeneration to multiple-choice questions, and handling video lengths from\nseconds to hours. Codes have been available at\nhttps://github.com/farewellthree/PPLLaVA.",
      "upvotes": 11
    },
    {
      "title": "Sparsing Law: Towards Large Language Models with Greater Activation Sparsity",
      "url": "https://huggingface.co/papers/2411.02335",
      "authors": [
        "Yuqi Luo",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02335.pdf",
      "abstract": "Activation sparsity denotes the existence of substantial weakly-contributed\nelements within activation outputs that can be eliminated, benefiting many\nimportant applications concerned with large language models (LLMs). Although\npromoting greater activation sparsity within LLMs deserves deep studies,\nexisting works lack comprehensive and quantitative research on the correlation\nbetween activation sparsity and potentially influential factors. In this paper,\nwe present a comprehensive study on the quantitative scaling properties and\ninfluential factors of the activation sparsity within decoder-only\nTransformer-based LLMs. Specifically, we propose PPL-p% sparsity, a precise\nand performance-aware activation sparsity metric that is applicable to any\nactivation function. Through extensive experiments, we find several important\nphenomena. Firstly, different activation functions exhibit comparable\nperformance but opposite training-time sparsity trends. The activation ratio\n(i.e., 1-sparsity ratio) evolves as a convergent increasing\npower-law and decreasing logspace power-law with the amount of training data\nfor SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate\nthat ReLU is more efficient as the activation function than SiLU and can\nleverage more training data to improve activation sparsity. Secondly, the\nactivation ratio linearly increases with the width-depth ratio below a certain\nbottleneck point, indicating the potential advantage of a deeper architecture\nat a fixed parameter scale. Finally, at similar width-depth ratios, we\nsurprisingly find that the limit value of activation sparsity varies weakly\nwith the parameter scale, i.e., the activation patterns within LLMs are\ninsensitive to the parameter scale. These empirical laws towards LLMs with\ngreater activation sparsity have important implications for making LLMs more\nefficient and interpretable.",
      "upvotes": 10
    },
    {
      "title": "SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF",
      "url": "https://huggingface.co/papers/2411.01798",
      "authors": [
        "Atoosa Chegini",
        "Hamid Kazemi",
        "Iman Mirzadeh",
        "Dong Yin",
        "Maxwell Horton",
        "Moin Nabi",
        "Mehrdad Farajtabar",
        "Keivan Alizadeh"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01798.pdf",
      "abstract": "In Large Language Model (LLM) development, Reinforcement Learning from Human\nFeedback (RLHF) is crucial for aligning models with human values and\npreferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence\nbetween the current policy and a frozen initial policy as a reference, which is\nadded as a penalty in policy optimization algorithms like Proximal Policy\nOptimization (PPO). While this constraint prevents models from deviating too\nfar from the initial checkpoint, it limits exploration of the reward landscape,\nreducing the model's ability to discover higher-quality solutions. As a result,\npolicy optimization is often trapped in a narrow region of the parameter space,\nleading to suboptimal alignment and performance. This paper presents SALSA\n(Soup-based Alignment Learning for Stronger Adaptation), a novel approach\ndesigned to overcome these limitations by creating a more flexible and better\nlocated reference model through weight-space averaging of two independent\nsupervised fine-tuned (SFT) models. This model soup allows for larger deviation\nin KL divergence and exploring a promising region of the solution space without\nsacrificing stability. By leveraging this more robust reference model, SALSA\nfosters better exploration, achieving higher rewards and improving model\nrobustness, out-of-distribution generalization, and performance. We validate\nthe effectiveness of SALSA through extensive experiments on popular open models\n(Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench,\nArena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering\ndeeper exploration and achieving superior alignment in LLMs.",
      "upvotes": 8
    },
    {
      "title": "IGOR: Image-GOal Representations are the Atomic Control Units for Foundation Models in Embodied AI",
      "url": "https://huggingface.co/papers/2411.00785",
      "authors": [
        "Xiaoyu Chen",
        "Junliang Guo",
        "Tianyu He",
        "Chuheng Zhang",
        "Derek Cathera Yang",
        "Li Zhao",
        "Jiang Bian"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00785.pdf",
      "abstract": "We introduce Image-GOal Representations (IGOR), aiming to learn a unified,\nsemantically consistent action space across human and various robots. Through\nthis unified latent action space, IGOR enables knowledge transfer among\nlarge-scale robot and human activity data. We achieve this by compressing\nvisual changes between an initial image and its goal state into latent actions.\nIGOR allows us to generate latent action labels for internet-scale video data.\nThis unified latent action space enables the training of foundation policy and\nworld models across a wide variety of tasks performed by both robots and\nhumans. We demonstrate that: (1) IGOR learns a semantically consistent action\nspace for both human and robots, characterizing various possible motions of\nobjects representing the physical interaction knowledge; (2) IGOR can \"migrate\"\nthe movements of the object in the one video to other videos, even across human\nand robots, by jointly using the latent action model and world model; (3) IGOR\ncan learn to align latent actions with natural language through the foundation\npolicy model, and integrate latent actions with a low-level policy model to\nachieve effective robot control. We believe IGOR opens new possibilities for\nhuman-to-robot knowledge transfer and control.",
      "upvotes": 8
    },
    {
      "title": "LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models",
      "url": "https://huggingface.co/papers/2411.00918",
      "authors": [
        "Luong Tran",
        "Van Nguyen",
        "Quang Pham"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00918.pdf",
      "abstract": "Mixture of Experts (MoEs) plays an important role in the development of more\nefficient and effective large language models (LLMs). Due to the enormous\nresource requirements, studying large scale MoE algorithms remain in-accessible\nto many researchers. This work develops LibMoE, a comprehensive and\nmodular framework to streamline the research, training, and evaluation of MoE\nalgorithms. Built upon three core principles: (i) modular design, (ii)\nefficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs\nmore accessible to a wide range of researchers by standardizing the training\nand evaluation pipelines. Using LibMoE, we extensively benchmarked five\nstate-of-the-art MoE algorithms over three different LLMs and 11 datasets under\nthe zero-shot setting. The results show that despite the unique\ncharacteristics, all MoE algorithms perform roughly similar when averaged\nacross a wide range of tasks. With the modular design and extensive evaluation,\nwe believe LibMoE will be invaluable for researchers to make meaningful\nprogress towards the next generation of MoE and LLMs. Project page:\nhttps://fsoft-aic.github.io/fsoft-LibMoE.github.io.",
      "upvotes": 8
    },
    {
      "title": "Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models",
      "url": "https://huggingface.co/papers/2411.00743",
      "authors": [
        "Mona Diab",
        "Virginia Smith"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00743.pdf",
      "abstract": "Understanding and mitigating the potential risks associated with foundation\nmodels (FMs) hinges on developing effective interpretability methods. Sparse\nAutoencoders (SAEs) have emerged as a promising tool for disentangling FM\nrepresentations, but they struggle to capture rare, yet crucial concepts in the\ndata. We introduce Specialized Sparse Autoencoders (SSAEs), designed to\nilluminate these elusive dark matter features by focusing on specific\nsubdomains. We present a practical recipe for training SSAEs, demonstrating the\nefficacy of dense retrieval for data selection and the benefits of Tilted\nEmpirical Risk Minimization as a training objective to improve concept recall.\nOur evaluation of SSAEs on standard metrics, such as downstream perplexity and\nL_0 sparsity, show that they effectively capture subdomain tail concepts,\nexceeding the capabilities of general-purpose SAEs. We showcase the practical\nutility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs\nachieve a 12.5\\% increase in worst-group classification accuracy when applied\nto remove spurious gender information. SSAEs provide a powerful new lens for\npeering into the inner workings of FMs in subdomains.",
      "upvotes": 6
    },
    {
      "title": "Constrained Diffusion Implicit Models",
      "url": "https://huggingface.co/papers/2411.00359",
      "authors": [
        "Ira Kemelmacher-Shlizerman",
        "Steven M. Seitz",
        "John Thickstun"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00359.pdf",
      "abstract": "This paper describes an efficient algorithm for solving noisy linear inverse\nproblems using pretrained diffusion models. Extending the paradigm of denoising\ndiffusion implicit models (DDIM), we propose constrained diffusion implicit\nmodels (CDIM) that modify the diffusion updates to enforce a constraint upon\nthe final output. For noiseless inverse problems, CDIM exactly satisfies the\nconstraints; in the noisy case, we generalize CDIM to satisfy an exact\nconstraint on the residual distribution of the noise. Experiments across a\nvariety of tasks and metrics show strong performance of CDIM, with analogous\ninference acceleration to unconstrained DDIM: 10 to 50 times faster than\nprevious conditional diffusion methods. We demonstrate the versatility of our\napproach on many problems including super-resolution, denoising, inpainting,\ndeblurring, and 3D point cloud reconstruction.",
      "upvotes": 5
    },
    {
      "title": "Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models",
      "url": "https://huggingface.co/papers/2411.00492",
      "authors": [
        "Kenji Kawaguchi",
        "Nancy F. Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00492.pdf",
      "abstract": "We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu\net al., 2023), designed to improve the large language model (LLM) generation.\nSpecifically, it guides an LLM to fulfill an input instruction by simulating\nmultiple experts, aggregating their responses, and selecting the best among\nindividual and aggregated responses. This process is performed in a single\nchain of thoughts through our seven carefully designed subtasks derived from\nthe Nominal Group Technique (Ven and Delbecq, 1974), a well-established\ndecision-making framework. Our evaluations demonstrate that Multi-expert\nPrompting significantly outperforms ExpertPrompting and comparable baselines in\nenhancing the truthfulness, factuality, informativeness, and usefulness of\nresponses while reducing toxicity and hurtfulness. It further achieves\nstate-of-the-art truthfulness by outperforming the best baseline by 8.69% with\nChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable\nto diverse scenarios, eliminating the need for manual prompt construction.",
      "upvotes": 5
    },
    {
      "title": "LoRA-Contextualizing Adaptation of Large Multimodal Models for Long Document Understanding",
      "url": "https://huggingface.co/papers/2411.01106",
      "authors": [
        "Jian Chen",
        "Yufan Zhou",
        "Tong Yu",
        "Jiuxiang Gu",
        "Ryan A. Rossi",
        "Changyou Chen",
        "Tong Sun"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01106.pdf",
      "abstract": "Large multimodal models (LMMs) have recently shown great progress in\ntext-rich image understanding, yet they still struggle with complex,\nmulti-page, visually-rich documents. Traditional methods using document parsers\nfor retrieval-augmented generation suffer from performance and efficiency\nlimitations, while directly presenting all pages to LMMs leads to\ninefficiencies, especially with lengthy documents. In this work, we present a\nnovel framework named LoRA-Contextualizing Adaptation of Large multimodal\nmodels (LoCAL), which broadens the capabilities of any LMM to support\nlong-document understanding. We demonstrate that LMMs can effectively serve as\nmultimodal retrievers, fetching relevant pages to answer user questions based\non these pages. LoCAL is implemented with two specific LMM adapters: one for\nevidence page retrieval and another for question answering. Empirical results\nshow state-of-the-art performance on public benchmarks, demonstrating the\neffectiveness of LoCAL.",
      "upvotes": 4
    },
    {
      "title": "Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks",
      "url": "https://huggingface.co/papers/2411.01192",
      "authors": [
        "El Moatez Billah Nagoudi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01192.pdf",
      "abstract": "We introduce Swan, a family of embedding models centred around the Arabic\nlanguage, addressing both small-scale and large-scale use cases. Swan includes\ntwo variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on\nArMistral, a pretrained Arabic large language model. To evaluate these models,\nwe propose ArabicMTEB, a comprehensive benchmark suite that assesses\ncross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text\nembedding performance, covering eight diverse tasks and spanning 94 datasets.\nSwan-Large achieves state-of-the-art results, outperforming\nMultilingual-E5-large in most Arabic tasks, while the Swan-Small consistently\nsurpasses Multilingual-E5 base. Our extensive evaluations demonstrate that Swan\nmodels are both dialectally and culturally aware, excelling across various\nArabic domains while offering significant monetary efficiency. This work\nsignificantly advances the field of Arabic language modelling and provides\nvaluable resources for future research and applications in Arabic natural\nlanguage processing. Our models and benchmark will be made publicly accessible\nfor research.",
      "upvotes": 3
    }
  ]
}