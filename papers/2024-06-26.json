{
  "date": "2024-06-26",
  "papers": [
    {
      "title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale",
      "url": "https://huggingface.co/papers/2406.17557",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.17557.pdf",
      "abstract": "The performance of a large language model (LLM) depends heavily on the\nquality and size of its pretraining dataset. However, the pretraining datasets\nfor state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly\navailable and very little is known about how they were created. In this work,\nwe introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl\nsnapshots that produces better-performing LLMs than other open pretraining\ndatasets. To advance the understanding of how best to curate high-quality\npretraining datasets, we carefully document and ablate all of the design\nchoices used in FineWeb, including in-depth investigations of deduplication and\nfiltering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion\ntoken collection of educational text filtered from FineWeb. LLMs pretrained on\nFineWeb-Edu exhibit dramatically better performance on knowledge- and\nreasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we\npublicly release our data curation codebase and all of the models trained\nduring our ablation experiments.",
      "upvotes": 86
    },
    {
      "title": "YouDream: Generating Anatomically Controllable Consistent Text-to-3D Animals",
      "url": "https://huggingface.co/papers/2406.16273",
      "authors": [
        "Alan C. Bovik"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.16273.pdf",
      "abstract": "3D generation guided by text-to-image diffusion models enables the creation\nof visually compelling assets. However previous methods explore generation\nbased on image or text. The boundaries of creativity are limited by what can be\nexpressed through words or the images that can be sourced. We present YouDream,\na method to generate high-quality anatomically controllable animals. YouDream\nis guided using a text-to-image diffusion model controlled by 2D views of a 3D\npose prior. Our method generates 3D animals that are not possible to create\nusing previous text-to-3D generative methods. Additionally, our method is\ncapable of preserving anatomic consistency in the generated animals, an area\nwhere prior text-to-3D approaches often struggle. Moreover, we design a fully\nautomated pipeline for generating commonly found animals. To circumvent the\nneed for human intervention to create a 3D pose, we propose a multi-agent LLM\nthat adapts poses from a limited library of animal 3D poses to represent the\ndesired animal. A user study conducted on the outcomes of YouDream demonstrates\nthe preference of the animal models generated by our method over others.\nTurntable results and code are released at https://youdream3d.github.io/",
      "upvotes": 40
    },
    {
      "title": "Unlocking Continual Learning Abilities in Language Models",
      "url": "https://huggingface.co/papers/2406.17245",
      "authors": [
        "Ka Chun Cheung",
        "Reynold Cheng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17245.pdf",
      "abstract": "Language models (LMs) exhibit impressive performance and generalization\ncapabilities. However, LMs struggle with the persistent challenge of\ncatastrophic forgetting, which undermines their long-term sustainability in\ncontinual learning (CL). Existing approaches usually address the issue by\nincorporating old task data or task-wise inductive bias into LMs. However, old\ndata and accurate task information are often unavailable or costly to collect,\nhindering the availability of current CL approaches for LMs. To address this\nlimitation, we introduce MIGU (MagnItude-based\nGradient Updating for continual learning), a\nrehearsal-free and task-label-free method that only updates the model\nparameters with large magnitudes of output in LMs' linear layers. MIGU is based\non our observation that the L1-normalized magnitude distribution of the output\nin LMs' linear layers is different when the LM models deal with different task\ndata. By imposing this simple constraint on the gradient update process, we can\nleverage the inherent behaviors of LMs, thereby unlocking their innate CL\nabilities. Our experiments demonstrate that MIGU is universally applicable to\nall three LM architectures (T5, RoBERTa, and Llama2), delivering\nstate-of-the-art or on-par performance across continual finetuning and\ncontinual pre-training settings on four CL benchmarks. For example, MIGU brings\na 15.2% average accuracy improvement over conventional parameter-efficient\nfinetuning baselines in a 15-task CL benchmark. MIGU can also seamlessly\nintegrate with all three existing CL types to further enhance performance. Code\nis available at https://github.com/wenyudu/MIGU{this https URL}.",
      "upvotes": 28
    },
    {
      "title": "Aligning Diffusion Models with Noise-Conditioned Perception",
      "url": "https://huggingface.co/papers/2406.17636",
      "authors": [
        "Ilya Makarov"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17636.pdf",
      "abstract": "Recent advancements in human preference optimization, initially developed for\nLanguage Models (LMs), have shown promise for text-to-image Diffusion Models,\nenhancing prompt alignment, visual appeal, and user preference. Unlike LMs,\nDiffusion Models typically optimize in pixel or VAE space, which does not align\nwell with human perception, leading to slower and less efficient training\nduring the preference alignment stage. We propose using a perceptual objective\nin the U-Net embedding space of the diffusion model to address these issues.\nOur approach involves fine-tuning Stable Diffusion 1.5 and XL using Direct\nPreference Optimization (DPO), Contrastive Preference Optimization (CPO), and\nsupervised fine-tuning (SFT) within this embedding space. This method\nsignificantly outperforms standard latent-space implementations across various\nmetrics, including quality and computational cost. For SDXL, our approach\nprovides 60.8\\% general preference, 62.2\\% visual appeal, and 52.1\\% prompt\nfollowing against original open-sourced SDXL-DPO on the PartiPrompts dataset,\nwhile significantly reducing compute. Our approach not only improves the\nefficiency and quality of human preference alignment for diffusion models but\nis also easily integrable with other optimization techniques. The training code\nand LoRA weights will be available here:\nhttps://huggingface.co/alexgambashidze/SDXL\\_NCP-DPO\\_v0.1",
      "upvotes": 26
    },
    {
      "title": "APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets",
      "url": "https://huggingface.co/papers/2406.18518",
      "authors": [
        "Thai Hoang",
        "Tian Lan",
        "Shirley Kokane",
        "Juntao Tan",
        "Zhiwei Liu",
        "Yihao Feng",
        "Liangwei Yang",
        "Silvio Savarese",
        "Huan Wang",
        "Shelby Heinecke",
        "Caiming Xiong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.18518.pdf",
      "abstract": "The advancement of function-calling agent models requires diverse, reliable,\nand high-quality datasets. This paper presents APIGen, an automated data\ngeneration pipeline designed to synthesize verifiable high-quality datasets for\nfunction-calling applications. We leverage APIGen and collect 3,673 executable\nAPIs across 21 different categories to generate diverse function-calling\ndatasets in a scalable and structured manner. Each data in our dataset is\nverified through three hierarchical stages: format checking, actual function\nexecutions, and semantic verification, ensuring its reliability and\ncorrectness. We demonstrate that models trained with our curated datasets, even\nwith only 7B parameters, can achieve state-of-the-art performance on the\nBerkeley Function-Calling Benchmark, outperforming multiple GPT-4 models.\nMoreover, our 1B model achieves exceptional performance, surpassing\nGPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60,000\nhigh-quality entries, aiming to advance the field of function-calling agent\ndomains. The dataset is available on Huggingface:\nhttps://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and the\nproject homepage: https://apigen-pipeline.github.io/",
      "upvotes": 23
    },
    {
      "title": "DiffusionPDE: Generative PDE-Solving Under Partial Observation",
      "url": "https://huggingface.co/papers/2406.17763",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.17763.pdf",
      "abstract": "We introduce a general framework for solving partial differential equations\n(PDEs) using generative diffusion models. In particular, we focus on the\nscenarios where we do not have the full knowledge of the scene necessary to\napply classical solvers. Most existing forward or inverse PDE approaches\nperform poorly when the observations on the data or the underlying coefficients\nare incomplete, which is a common assumption for real-world measurements. In\nthis work, we propose DiffusionPDE that can simultaneously fill in the missing\ninformation and solve a PDE by modeling the joint distribution of the solution\nand coefficient spaces. We show that the learned generative priors lead to a\nversatile framework for accurately solving a wide range of PDEs under partial\nobservation, significantly outperforming the state-of-the-art methods for both\nforward and inverse directions.",
      "upvotes": 23
    },
    {
      "title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs",
      "url": "https://huggingface.co/papers/2406.17588",
      "authors": [
        "Shawn Gavin",
        "Tuney Zheng",
        "Noah Wang",
        "Chenchen Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17588.pdf",
      "abstract": "The long-context capabilities of large language models (LLMs) have been a hot\ntopic in recent years. To evaluate the performance of LLMs in different\nscenarios, various assessment benchmarks have emerged. However, as most of\nthese benchmarks focus on identifying key information to answer questions,\nwhich mainly requires the retrieval ability of LLMs, these benchmarks can\npartially represent the reasoning performance of LLMs from large amounts of\ninformation. Meanwhile, although LLMs often claim to have context windows of\n32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual\nsupported length of these LLMs. To address these issues, we propose the LongIns\nbenchmark dataset, a challenging long-context instruction-based exam for LLMs,\nwhich is built based on the existing instruction datasets. Specifically, in our\nLongIns, we introduce three evaluation settings: Global Instruction & Single\nTask (GIST), Local Instruction & Single Task (LIST), and Local Instruction &\nMultiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations\non existing LLMs and have the following important findings: (1). The\ntop-performing GPT-4 with 128k context length performs poorly on the evaluation\ncontext window of 16k in our LongIns. (2). For the multi-hop reasoning ability\nof many existing LLMs, significant efforts are still needed under short context\nwindows (less than 4k).",
      "upvotes": 20
    },
    {
      "title": "MotionBooth: Motion-Aware Customized Text-to-Video Generation",
      "url": "https://huggingface.co/papers/2406.17758",
      "authors": [
        "Jiangning Zhang",
        "Qianyu Zhou",
        "Yunhai Tong",
        "Kai Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17758.pdf",
      "abstract": "In this work, we present MotionBooth, an innovative framework designed for\nanimating customized subjects with precise control over both object and camera\nmovements. By leveraging a few images of a specific object, we efficiently\nfine-tune a text-to-video model to capture the object's shape and attributes\naccurately. Our approach presents subject region loss and video preservation\nloss to enhance the subject's learning performance, along with a subject token\ncross-attention loss to integrate the customized subject with motion control\nsignals. Additionally, we propose training-free techniques for managing subject\nand camera motions during inference. In particular, we utilize cross-attention\nmap manipulation to govern subject motion and introduce a novel latent shift\nmodule for camera movement control as well. MotionBooth excels in preserving\nthe appearance of subjects while simultaneously controlling the motions in\ngenerated videos. Extensive quantitative and qualitative evaluations\ndemonstrate the superiority and effectiveness of our method. Our project page\nis at https://jianzongwu.github.io/projects/motionbooth",
      "upvotes": 18
    },
    {
      "title": "MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning",
      "url": "https://huggingface.co/papers/2406.17770",
      "authors": [
        "Haian Huang",
        "Yining Li",
        "Kai Chen",
        "Hua Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17770.pdf",
      "abstract": "Multi-modal large language models (MLLMs) have made significant strides in\nvarious visual understanding tasks. However, the majority of these models are\nconstrained to process low-resolution images, which limits their effectiveness\nin perception tasks that necessitate detailed visual information. In our study,\nwe present MG-LLaVA, an innovative MLLM that enhances the model's visual\nprocessing capabilities by incorporating a multi-granularity vision flow, which\nincludes low-resolution, high-resolution, and object-centric features. We\npropose the integration of an additional high-resolution visual encoder to\ncapture fine-grained details, which are then fused with base visual features\nthrough a Conv-Gate fusion network. To further refine the model's object\nrecognition abilities, we incorporate object-level features derived from\nbounding boxes identified by offline detectors. Being trained solely on\npublicly available multimodal data through instruction tuning, MG-LLaVA\ndemonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide\nvariety of language encoders, ranging from 3.8B to 34B, to evaluate the model's\nperformance comprehensively. Extensive evaluations across multiple benchmarks\ndemonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter\nsizes, showcasing its remarkable efficacy. The code will be available at\nhttps://github.com/PhoenixZ810/MG-LLaVA.",
      "upvotes": 18
    },
    {
      "title": "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA",
      "url": "https://huggingface.co/papers/2406.17419",
      "authors": [
        "Run Luo",
        "Min Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17419.pdf",
      "abstract": "Long-context modeling capabilities have garnered widespread attention,\nleading to the emergence of Large Language Models (LLMs) with ultra-context\nwindows. Meanwhile, benchmarks for evaluating long-context LLMs are gradually\ncatching up. However, existing benchmarks employ irrelevant noise texts to\nartificially extend the length of test cases, diverging from the real-world\nscenarios of long-context applications. To bridge this gap, we propose a novel\nlong-context benchmark, Loong, aligning with realistic scenarios through\nextended multi-document question answering (QA). Unlike typical document QA, in\nLoong's test cases, each document is relevant to the final answer, ignoring any\ndocument will lead to the failure of the answer. Furthermore, Loong introduces\nfour types of tasks with a range of context lengths: Spotlight Locating,\nComparison, Clustering, and Chain of Reasoning, to facilitate a more realistic\nand comprehensive evaluation of long-context understanding. Extensive\nexperiments indicate that existing long-context language models still exhibit\nconsiderable potential for enhancement. Retrieval augmented generation (RAG)\nachieves poor performance, demonstrating that Loong can reliably assess the\nmodel's long-context modeling capabilities.",
      "upvotes": 16
    },
    {
      "title": "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation",
      "url": "https://huggingface.co/papers/2406.16678",
      "authors": [
        "Markus Schedl"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.16678.pdf",
      "abstract": "Segmenting text into sentences plays an early and crucial role in many NLP\nsystems. This is commonly achieved by using rule-based or statistical methods\nrelying on lexical features such as punctuation. Although some recent works no\nlonger exclusively rely on punctuation, we find that no prior method achieves\nall of (i) robustness to missing punctuation, (ii) effective adaptability to\nnew domains, and (iii) high efficiency. We introduce a new model - Segment any\nText (SaT) - to solve this problem. To enhance robustness, we propose a new\npretraining scheme that ensures less reliance on punctuation. To address\nadaptability, we introduce an extra stage of parameter-efficient fine-tuning,\nestablishing state-of-the-art performance in distinct domains such as verses\nfrom lyrics and legal documents. Along the way, we introduce architectural\nmodifications that result in a threefold gain in speed over the previous state\nof the art and solve spurious reliance on context far in the future. Finally,\nwe introduce a variant of our model with fine-tuning on a diverse, multilingual\nmixture of sentence-segmented data, acting as a drop-in replacement and\nenhancement for existing segmentation tools. Overall, our contributions provide\na universal approach for segmenting any text. Our method outperforms all\nbaselines - including strong LLMs - across 8 corpora spanning diverse domains\nand languages, especially in practically relevant situations where text is\npoorly formatted. Our models and code, including documentation, are available\nat https://huggingface.co/segment-any-text under the MIT license.",
      "upvotes": 14
    },
    {
      "title": "DialSim: A Real-Time Simulator for Evaluating Long-Term Dialogue Understanding of Conversational Agents",
      "url": "https://huggingface.co/papers/2406.13144",
      "authors": [
        "Hyunseung Chung",
        "Eunbyeol Cho",
        "Yohan Jo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13144.pdf",
      "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced the capabilities of conversational agents, making them applicable to\nvarious fields (e.g., education). Despite their progress, the evaluation of the\nagents often overlooks the complexities of real-world conversations, such as\nreal-time interactions, multi-party dialogues, and extended contextual\ndependencies. To bridge this gap, we introduce DialSim, a real-time dialogue\nsimulator. In this simulator, an agent is assigned the role of a character from\npopular TV shows, requiring it to respond to spontaneous questions using past\ndialogue information and to distinguish between known and unknown information.\nKey features of DialSim include evaluating the agent's ability to respond\nwithin a reasonable time limit, handling long-term multi-party dialogues, and\nmanaging adversarial settings (e.g., swap character names) to challenge the\nagent's reliance on pre-trained knowledge. We utilized this simulator to\nevaluate the latest conversational agents and analyze their limitations. Our\nexperiments highlight both the strengths and weaknesses of these agents,\nproviding valuable insights for future improvements in the field of\nconversational AI. DialSim is available at\nhttps://github.com/jiho283/Simulator.",
      "upvotes": 11
    },
    {
      "title": "On the Transformations across Reward Model, Parameter Update, and In-Context Prompt",
      "url": "https://huggingface.co/papers/2406.16377",
      "authors": [
        "Bowen Cao",
        "Zhisong Zhang",
        "Yan Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.16377.pdf",
      "abstract": "Despite the general capabilities of pre-trained large language models (LLMs),\nthey still need further adaptation to better serve practical applications. In\nthis paper, we demonstrate the interchangeability of three popular and distinct\nadaptation tools: parameter updating, reward modeling, and in-context\nprompting. This interchangeability establishes a triangular framework with six\ntransformation directions, each of which facilitates a variety of applications.\nOur work offers a holistic view that unifies numerous existing studies and\nsuggests potential research directions. We envision our work as a useful\nroadmap for future research on LLMs.",
      "upvotes": 11
    },
    {
      "title": "FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models",
      "url": "https://huggingface.co/papers/2406.16863",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.16863.pdf",
      "abstract": "Diffusion model has demonstrated remarkable capability in video generation,\nwhich further sparks interest in introducing trajectory control into the\ngeneration process. While existing works mainly focus on training-based methods\n(e.g., conditional adapter), we argue that diffusion model itself allows decent\ncontrol over the generated content without requiring any training. In this\nstudy, we introduce a tuning-free framework to achieve trajectory-controllable\nvideo generation, by imposing guidance on both noise construction and attention\ncomputation. Specifically, 1) we first show several instructive phenomenons and\nanalyze how initial noises influence the motion trajectory of generated\ncontent. 2) Subsequently, we propose FreeTraj, a tuning-free approach that\nenables trajectory control by modifying noise sampling and attention\nmechanisms. 3) Furthermore, we extend FreeTraj to facilitate longer and larger\nvideo generation with controllable trajectories. Equipped with these designs,\nusers have the flexibility to provide trajectories manually or opt for\ntrajectories automatically generated by the LLM trajectory planner. Extensive\nexperiments validate the efficacy of our approach in enhancing the trajectory\ncontrollability of video diffusion models.",
      "upvotes": 10
    },
    {
      "title": "Image Conductor: Precision Control for Interactive Video Synthesis",
      "url": "https://huggingface.co/papers/2406.15339",
      "authors": [
        "Ziyang Yuan",
        "Yuexian Zou",
        "Ying Shan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15339.pdf",
      "abstract": "Filmmaking and animation production often require sophisticated techniques\nfor coordinating camera transitions and object movements, typically involving\nlabor-intensive real-world capturing. Despite advancements in generative AI for\nvideo creation, achieving precise control over motion for interactive video\nasset generation remains challenging. To this end, we propose Image Conductor,\na method for precise control of camera transitions and object movements to\ngenerate video assets from a single image. An well-cultivated training strategy\nis proposed to separate distinct camera and object motion by camera LoRA\nweights and object LoRA weights. To further address cinematographic variations\nfrom ill-posed trajectories, we introduce a camera-free guidance technique\nduring inference, enhancing object movements while eliminating camera\ntransitions. Additionally, we develop a trajectory-oriented video motion data\ncuration pipeline for training. Quantitative and qualitative experiments\ndemonstrate our method's precision and fine-grained control in generating\nmotion-controllable videos from images, advancing the practical application of\ninteractive video synthesis. Project webpage available at\nhttps://liyaowei-stu.github.io/project/ImageConductor/",
      "upvotes": 8
    },
    {
      "title": "Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients",
      "url": "https://huggingface.co/papers/2406.17660",
      "authors": [
        "Oscar Li",
        "Mona Diab",
        "Virginia Smith"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17660.pdf",
      "abstract": "Large language model (LLM) training and finetuning are often bottlenecked by\nlimited GPU memory. While existing projection-based optimization methods\naddress this by projecting gradients into a lower-dimensional subspace to\nreduce optimizer state memory, they typically rely on dense projection\nmatrices, which can introduce computational and memory overheads. In this work,\nwe propose Grass (GRAdient Stuctured Sparsification), a novel approach that\nleverages sparse projections to transform gradients into structured sparse\nupdates. This design not only significantly reduces memory usage for optimizer\nstates but also minimizes gradient memory footprint, computation, and\ncommunication costs, leading to substantial throughput improvements. Extensive\nexperiments on pretraining and finetuning tasks demonstrate that Grass achieves\ncompetitive performance to full-rank training and existing projection-based\nmethods. Notably, Grass enables half-precision pretraining of a 13B parameter\nLLaMA model on a single 40GB A100 GPU--a feat infeasible for previous\nmethods--and yields up to a 2times throughput improvement on an 8-GPU\nsystem. Code can be found at https://github.com/aashiqmuhamed/GRASS .",
      "upvotes": 5
    },
    {
      "title": "Large Language Models Assume People are More Rational than We Really are",
      "url": "https://huggingface.co/papers/2406.17055",
      "authors": [
        "Ryan Liu",
        "Joshua C. Peterson",
        "Ilia Sucholutsky",
        "Thomas L. Griffiths"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17055.pdf",
      "abstract": "In order for AI systems to communicate effectively with people, they must\nunderstand how we make decisions. However, people's decisions are not always\nrational, so the implicit internal models of human decision-making in Large\nLanguage Models (LLMs) must account for this. Previous empirical evidence seems\nto suggest that these implicit models are accurate -- LLMs offer believable\nproxies of human behavior, acting how we expect humans would in everyday\ninteractions. However, by comparing LLM behavior and predictions to a large\ndataset of human decisions, we find that this is actually not the case: when\nboth simulating and predicting people's choices, a suite of cutting-edge LLMs\n(GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more\nrational than we really are. Specifically, these models deviate from human\nbehavior and align more closely with a classic model of rational choice --\nexpected value theory. Interestingly, people also tend to assume that other\npeople are rational when interpreting their behavior. As a consequence, when we\ncompare the inferences that LLMs and people draw from the decisions of others\nusing another psychological dataset, we find that these inferences are highly\ncorrelated. Thus, the implicit decision-making models of LLMs appear to be\naligned with the human expectation that other people will act rationally,\nrather than with how people actually act.",
      "upvotes": 4
    },
    {
      "title": "Multi-property Steering of Large Language Models with Dynamic Activation Composition",
      "url": "https://huggingface.co/papers/2406.17563",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.17563.pdf",
      "abstract": "Activation steering methods were shown to be effective in conditioning\nlanguage model generation by additively intervening over models' intermediate\nrepresentations. However, the evaluation of these techniques has so far been\nlimited to single conditioning properties and synthetic settings. In this work,\nwe conduct a comprehensive evaluation of various activation steering\nstrategies, highlighting the property-dependent nature of optimal parameters to\nensure a robust effect throughout generation. To address this issue, we propose\nDynamic Activation Composition, an information-theoretic approach to modulate\nthe steering intensity of one or more properties throughout generation. Our\nexperiments on multi-property steering show that our method successfully\nmaintains high conditioning while minimizing the impact of conditioning on\ngeneration fluency.",
      "upvotes": 4
    },
    {
      "title": "Cross-Modality Safety Alignment",
      "url": "https://huggingface.co/papers/2406.15279",
      "authors": [
        "Xingsong Ye",
        "Qinyuan Cheng",
        "Junwen Duan",
        "Shimin Li",
        "Jinlan Fu",
        "Xipeng Qiu",
        "Xuanjing Huang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15279.pdf",
      "abstract": "As Artificial General Intelligence (AGI) becomes increasingly integrated into\nvarious facets of human life, ensuring the safety and ethical alignment of such\nsystems is paramount. Previous studies primarily focus on single-modality\nthreats, which may not suffice given the integrated and complex nature of\ncross-modality interactions. We introduce a novel safety alignment challenge\ncalled Safe Inputs but Unsafe Output (SIUO) to evaluate cross-modality safety\nalignment. Specifically, it considers cases where single modalities are safe\nindependently but could potentially lead to unsafe or unethical outputs when\ncombined. To empirically investigate this problem, we developed the SIUO, a\ncross-modality benchmark encompassing 9 critical safety domains, such as\nself-harm, illegal activities, and privacy violations. Our findings reveal\nsubstantial safety vulnerabilities in both closed- and open-source LVLMs, such\nas GPT-4V and LLaVA, underscoring the inadequacy of current models to reliably\ninterpret and respond to complex, real-world scenarios.",
      "upvotes": 3
    }
  ]
}