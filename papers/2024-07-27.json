{
  "date": "2024-07-27",
  "papers": [
    {
      "title": "Diffree: Text-Guided Shape Free Object Inpainting with Diffusion Model",
      "url": "https://huggingface.co/papers/2407.16982",
      "authors": [
        "Tianshuo Yang",
        "Yuxin Zhang",
        "Yu Qiao",
        "Ping Luo",
        "Rongrong Ji"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.16982.pdf",
      "abstract": "This paper addresses an important problem of object addition for images with\nonly text guidance. It is challenging because the new object must be integrated\nseamlessly into the image with consistent visual context, such as lighting,\ntexture, and spatial location. While existing text-guided image inpainting\nmethods can add objects, they either fail to preserve the background\nconsistency or involve cumbersome human intervention in specifying bounding\nboxes or user-scribbled masks. To tackle this challenge, we introduce Diffree,\na Text-to-Image (T2I) model that facilitates text-guided object addition with\nonly text control. To this end, we curate OABench, an exquisite synthetic\ndataset by removing objects with advanced image inpainting techniques. OABench\ncomprises 74K real-world tuples of an original image, an inpainted image with\nthe object removed, an object mask, and object descriptions. Trained on OABench\nusing the Stable Diffusion model with an additional mask prediction module,\nDiffree uniquely predicts the position of the new object and achieves object\naddition with guidance from only text. Extensive experiments demonstrate that\nDiffree excels in adding new objects with a high success rate while maintaining\nbackground consistency, spatial appropriateness, and object relevance and\nquality.",
      "upvotes": 40
    },
    {
      "title": "LAMBDA: A Large Model Based Data Agent",
      "url": "https://huggingface.co/papers/2407.17535",
      "authors": [
        "Ruijian Han",
        "Binyan Jiang",
        "Houduo Qi",
        "Defeng Sun",
        "Yancheng Yuan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.17535.pdf",
      "abstract": "We introduce ``LAMBDA,\" a novel open-source, code-free multi-agent data\nanalysis system that that harnesses the power of large models. LAMBDA is\ndesigned to address data analysis challenges in complex data-driven\napplications through the use of innovatively designed data agents that operate\niteratively and generatively using natural language. At the core of LAMBDA are\ntwo key agent roles: the programmer and the inspector, which are engineered to\nwork together seamlessly. Specifically, the programmer generates code based on\nthe user's instructions and domain-specific knowledge, enhanced by advanced\nmodels. Meanwhile, the inspector debugs the code when necessary. To ensure\nrobustness and handle adverse scenarios, LAMBDA features a user interface that\nallows direct user intervention in the operational loop. Additionally, LAMBDA\ncan flexibly integrate external models and algorithms through our knowledge\nintegration mechanism, catering to the needs of customized data analysis.\nLAMBDA has demonstrated strong performance on various machine learning\ndatasets. It has the potential to enhance data science practice and analysis\nparadigm by seamlessly integrating human and artificial intelligence, making it\nmore accessible, effective, and efficient for individuals from diverse\nbackgrounds. The strong performance of LAMBDA in solving data science problems\nis demonstrated in several case studies, which are presented at\nhttps://www.polyu.edu.hk/ama/cmfai/lambda.html.",
      "upvotes": 34
    },
    {
      "title": "Very Large-Scale Multi-Agent Simulation in AgentScope",
      "url": "https://huggingface.co/papers/2407.17789",
      "authors": [
        "Yuexiang Xie",
        "Zhewei Wei",
        "Yaliang Li",
        "Bolin Ding"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.17789.pdf",
      "abstract": "Recent advances in large language models (LLMs) have opened new avenues for\napplying multi-agent systems in very large-scale simulations. However, there\nremain several challenges when conducting multi-agent simulations with existing\nplatforms, such as limited scalability and low efficiency, unsatisfied agent\ndiversity, and effort-intensive management processes. To address these\nchallenges, we develop several new features and components for AgentScope, a\nuser-friendly multi-agent platform, enhancing its convenience and flexibility\nfor supporting very large-scale multi-agent simulations. Specifically, we\npropose an actor-based distributed mechanism as the underlying technological\ninfrastructure towards great scalability and high efficiency, and provide\nflexible environment support for simulating various real-world scenarios, which\nenables parallel execution of multiple agents, centralized workflow\norchestration, and both inter-agent and agent-environment interactions among\nagents. Moreover, we integrate an easy-to-use configurable tool and an\nautomatic background generation pipeline in AgentScope, simplifying the process\nof creating agents with diverse yet detailed background settings. Last but not\nleast, we provide a web-based interface for conveniently monitoring and\nmanaging a large number of agents that might deploy across multiple devices. We\nconduct a comprehensive simulation to demonstrate the effectiveness of the\nproposed enhancements in AgentScope, and provide detailed observations and\ndiscussions to highlight the great potential of applying multi-agent systems in\nlarge-scale simulations. The source code is released on GitHub at\nhttps://github.com/modelscope/agentscope to inspire further research and\ndevelopment in large-scale multi-agent simulations.",
      "upvotes": 30
    },
    {
      "title": "AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents",
      "url": "https://huggingface.co/papers/2407.17490",
      "authors": [
        "Yazhe Niu",
        "Han Xiao",
        "Liang Liu",
        "Dingyu Zhang",
        "Peng Gao",
        "Shuai Ren"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.17490.pdf",
      "abstract": "AI agents have drawn increasing attention mostly on their ability to perceive\nenvironments, understand tasks, and autonomously achieve goals. To advance\nresearch on AI agents in mobile scenarios, we introduce the Android\nMulti-annotation EXpo (AMEX), a comprehensive, large-scale dataset designed for\ngeneralist mobile GUI-control agents. Their capabilities of completing complex\ntasks by directly interacting with the graphical user interface (GUI) on mobile\ndevices are trained and evaluated with the proposed dataset. AMEX comprises\nover 104K high-resolution screenshots from 110 popular mobile applications,\nwhich are annotated at multiple levels. Unlike existing mobile device-control\ndatasets, e.g., MoTIF, AitW, etc., AMEX includes three levels of annotations:\nGUI interactive element grounding, GUI screen and element functionality\ndescriptions, and complex natural language instructions, each averaging 13\nsteps with stepwise GUI-action chains. We develop this dataset from a more\ninstructive and detailed perspective, complementing the general settings of\nexisting datasets. Additionally, we develop a baseline model SPHINX Agent and\ncompare its performance across state-of-the-art agents trained on other\ndatasets. To facilitate further research, we open-source our dataset, models,\nand relevant evaluation tools. The project is available at\nhttps://yuxiangchai.github.io/AMEX/",
      "upvotes": 30
    },
    {
      "title": "BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular Depth Estimation",
      "url": "https://huggingface.co/papers/2407.17952",
      "authors": [
        "Markus Gross",
        "Konrad Schindler",
        "Christopher Schroers"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.17952.pdf",
      "abstract": "By training over large-scale datasets, zero-shot monocular depth estimation\n(MDE) methods show robust performance in the wild but often suffer from\ninsufficiently precise details. Although recent diffusion-based MDE approaches\nexhibit appealing detail extraction ability, they still struggle in\ngeometrically challenging scenes due to the difficulty of gaining robust\ngeometric priors from diverse datasets. To leverage the complementary merits of\nboth worlds, we propose BetterDepth to efficiently achieve geometrically\ncorrect affine-invariant MDE performance while capturing fine-grained details.\nSpecifically, BetterDepth is a conditional diffusion-based refiner that takes\nthe prediction from pre-trained MDE models as depth conditioning, in which the\nglobal depth context is well-captured, and iteratively refines details based on\nthe input image. For the training of such a refiner, we propose global\npre-alignment and local patch masking methods to ensure the faithfulness of\nBetterDepth to depth conditioning while learning to capture fine-grained scene\ndetails. By efficient training on small-scale synthetic datasets, BetterDepth\nachieves state-of-the-art zero-shot MDE performance on diverse public datasets\nand in-the-wild scenes. Moreover, BetterDepth can improve the performance of\nother MDE models in a plug-and-play manner without additional re-training.",
      "upvotes": 29
    },
    {
      "title": "Course-Correction: Safety Alignment Using Synthetic Preferences",
      "url": "https://huggingface.co/papers/2407.16637",
      "authors": [
        "Yishuo Cai",
        "Zhenhong Zhou",
        "Yan Liu",
        "Tianwei Zhang",
        "Wei Xu",
        "Han Qiu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.16637.pdf",
      "abstract": "The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of course-correction,\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the C^2-Eval benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\nC^2-Syn, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, Llama2-Chat 7B and\nQwen2 7B, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks.",
      "upvotes": 24
    },
    {
      "title": "Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?",
      "url": "https://huggingface.co/papers/2407.16607",
      "authors": [
        "Sewoong Oh",
        "Noah A. Smith"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.16607.pdf",
      "abstract": "The pretraining data of today's strongest language models is opaque. In\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation -- byte-pair encoding (BPE) tokenizers, used by the vast majority\nof modern language models. Our key insight is that the ordered list of merge\nrules learned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data: the first merge is the most common byte pair,\nthe second is the most common pair after merging the first token, and so on.\nGiven a tokenizer's merge list along with data samples for each category of\ninterest, we formulate a linear program that solves for the proportion of each\ncategory in the tokenizer's training set. Importantly, to the extent to which\ntokenizer training data is representative of the pretraining data, we\nindirectly learn about the pretraining data. In controlled experiments, we show\nthat our attack recovers mixture ratios with high precision for tokenizers\ntrained on known mixtures of natural languages, programming languages, and data\nsources. We then apply our approach to off-the-shelf tokenizers released with\nrecent LMs. We confirm much publicly disclosed information about these models,\nand also make several new inferences: GPT-4o's tokenizer is much more\nmultilingual than its predecessors, training on 39% non-English data; Llama3\nextends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and\nClaude's tokenizers are trained on predominantly code (~60%). We hope our work\nsheds light on current design practices for pretraining data, and inspires\ncontinued research into data mixture inference for LMs.",
      "upvotes": 21
    },
    {
      "title": "Efficient Inference of Vision Instruction-Following Models with Elastic Cache",
      "url": "https://huggingface.co/papers/2407.18121",
      "authors": [
        "Jiahui Wang",
        "Jiwen Lu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.18121.pdf",
      "abstract": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
      "upvotes": 15
    },
    {
      "title": "Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic",
      "url": "https://huggingface.co/papers/2407.18129",
      "authors": [
        "Fakhraddin Alwajih",
        "Muhammad Abdul-Mageed"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.18129.pdf",
      "abstract": "Recent advancements have significantly enhanced the capabilities of\nMultimodal Large Language Models (MLLMs) in generating and understanding\nimage-to-text content. Despite these successes, progress is predominantly\nlimited to English due to the scarcity of high quality multimodal resources in\nother languages. This limitation impedes the development of competitive models\nin languages such as Arabic. To alleviate this situation, we introduce an\nefficient Arabic multimodal assistant, dubbed Dallah, that utilizes an advanced\nlanguage model based on LLaMA-2 to facilitate multimodal interactions. Dallah\ndemonstrates state-of-the-art performance in Arabic MLLMs. Through fine-tuning\nsix Arabic dialects, Dallah showcases its capability to handle complex\ndialectal interactions incorporating both textual and visual elements. The\nmodel excels in two benchmark tests: one evaluating its performance on Modern\nStandard Arabic (MSA) and another specifically designed to assess dialectal\nresponses. Beyond its robust performance in multimodal interaction tasks,\nDallah has the potential to pave the way for further development of\ndialect-aware Arabic MLLMs.",
      "upvotes": 11
    },
    {
      "title": "LKCell: Efficient Cell Nuclei Instance Segmentation with Large Convolution Kernels",
      "url": "https://huggingface.co/papers/2407.18054",
      "authors": [
        "Jingfeng Yao",
        "Juan Yang",
        "Wenyu Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.18054.pdf",
      "abstract": "The segmentation of cell nuclei in tissue images stained with the blood dye\nhematoxylin and eosin (H&E) is essential for various clinical applications\nand analyses. Due to the complex characteristics of cellular morphology, a\nlarge receptive field is considered crucial for generating high-quality\nsegmentation. However, previous methods face challenges in achieving a balance\nbetween the receptive field and computational burden. To address this issue, we\npropose LKCell, a high-accuracy and efficient cell segmentation method. Its\ncore insight lies in unleashing the potential of large convolution kernels to\nachieve computationally efficient large receptive fields. Specifically, (1) We\ntransfer pre-trained large convolution kernel models to the medical domain for\nthe first time, demonstrating their effectiveness in cell segmentation. (2) We\nanalyze the redundancy of previous methods and design a new segmentation\ndecoder based on large convolution kernels. It achieves higher performance\nwhile significantly reducing the number of parameters. We evaluate our method\non the most challenging benchmark and achieve state-of-the-art results (0.5080\nmPQ) in cell nuclei instance segmentation with only 21.6% FLOPs compared with\nthe previous leading method. Our source code and models are available at\nhttps://github.com/hustvl/LKCell.",
      "upvotes": 10
    },
    {
      "title": "Text-Driven Neural Collaborative Filtering Model for Paper Source Tracing",
      "url": "https://huggingface.co/papers/2407.17722",
      "authors": [
        "Bingyu Chang",
        "Qingpeng Liu",
        "Ling Jian"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.17722.pdf",
      "abstract": "Identifying significant references within the complex interrelations of a\ncitation knowledge graph is challenging, which encompasses connections through\ncitations, authorship, keywords, and other relational attributes. The Paper\nSource Tracing (PST) task seeks to automate the identification of pivotal\nreferences for given scholarly articles utilizing advanced data mining\ntechniques. In the KDD CUP 2024, we design a recommendation-based framework\ntailored for the PST task. This framework employs the Neural Collaborative\nFiltering (NCF) model to generate final predictions. To process the textual\nattributes of the papers and extract input features for the model, we utilize\nSciBERT, a pre-trained language model. According to the experimental results,\nour method achieved a score of 0.37814 on the Mean Average Precision (MAP)\nmetric, outperforming baseline models and ranking 11th among all participating\nteams. The source code is publicly available at\nhttps://github.com/MyLove-XAB/KDDCupFinal.",
      "upvotes": 7
    },
    {
      "title": "The FIGNEWS Shared Task on News Media Narratives",
      "url": "https://huggingface.co/papers/2407.18147",
      "authors": [
        "Wajdi Zaghouani",
        "Houda Bouamor",
        "Imed Zitouni",
        "Mona Diab",
        "Samhaa R. El-Beltagy",
        "Muhammed AbuOdeh"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.18147.pdf",
      "abstract": "We present an overview of the FIGNEWS shared task, organized as part of the\nArabicNLP 2024 conference co-located with ACL 2024. The shared task addresses\nbias and propaganda annotation in multilingual news posts. We focus on the\nearly days of the Israel War on Gaza as a case study. The task aims to foster\ncollaboration in developing annotation guidelines for subjective tasks by\ncreating frameworks for analyzing diverse narratives highlighting potential\nbias and propaganda. In a spirit of fostering and encouraging diversity, we\naddress the problem from a multilingual perspective, namely within five\nlanguages: English, French, Arabic, Hebrew, and Hindi. A total of 17 teams\nparticipated in two annotation subtasks: bias (16 teams) and propaganda (6\nteams). The teams competed in four evaluation tracks: guidelines development,\nannotation quality, annotation quantity, and consistency. Collectively, the\nteams produced 129,800 data points. Key findings and implications for the field\nare discussed.",
      "upvotes": 7
    }
  ]
}