{
  "date": "2024-07-17",
  "papers": [
    {
      "title": "Qwen2-Audio Technical Report",
      "url": "https://huggingface.co/papers/2407.10759",
      "authors": [
        "Jin Xu",
        "Qian Yang",
        "Xipin Wei"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.10759.pdf",
      "abstract": "We introduce the latest progress of Qwen-Audio, a large-scale audio-language\nmodel called Qwen2-Audio, which is capable of accepting various audio signal\ninputs and performing audio analysis or direct textual responses with regard to\nspeech instructions. In contrast to complex hierarchical tags, we have\nsimplified the pre-training process by utilizing natural language prompts for\ndifferent data and tasks, and have further expanded the data volume. We have\nboosted the instruction-following capability of Qwen2-Audio and implemented two\ndistinct audio interaction modes for voice chat and audio analysis. In the\nvoice chat mode, users can freely engage in voice interactions with Qwen2-Audio\nwithout text input. In the audio analysis mode, users could provide audio and\ntext instructions for analysis during the interaction. Note that we do not use\nany system prompts to switch between voice chat and audio analysis modes.\nQwen2-Audio is capable of intelligently comprehending the content within audio\nand following voice commands to respond appropriately. For instance, in an\naudio segment that simultaneously contains sounds, multi-speaker conversations,\nand a voice command, Qwen2-Audio can directly understand the command and\nprovide an interpretation and response to the audio. Additionally, DPO has\noptimized the model's performance in terms of factuality and adherence to\ndesired behavior. According to the evaluation results from AIR-Bench,\nQwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests\nfocused on audio-centric instruction-following capabilities. Qwen2-Audio is\nopen-sourced with the aim of fostering the advancement of the multi-modal\nlanguage community.",
      "upvotes": 55
    },
    {
      "title": "NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?",
      "url": "https://huggingface.co/papers/2407.11963",
      "authors": [
        "Yunxin Liu",
        "Kai Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11963.pdf",
      "abstract": "In evaluating the long-context capabilities of large language models (LLMs),\nidentifying content relevant to a user's query from original long documents is\na crucial prerequisite for any LLM to answer questions based on long text. We\npresent NeedleBench, a framework consisting of a series of progressively more\nchallenging tasks for assessing bilingual long-context capabilities, spanning\nmultiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) and\ndifferent depth ranges, allowing the strategic insertion of critical data\npoints in different text depth zones to rigorously test the retrieval and\nreasoning capabilities of models in diverse contexts. We use the NeedleBench\nframework to assess how well the leading open-source models can identify key\ninformation relevant to the question and apply that information to reasoning in\nbilingual long texts. Furthermore, we propose the Ancestral Trace Challenge\n(ATC) to mimic the complexity of logical reasoning challenges that are likely\nto be present in real-world long-context tasks, providing a simple method for\nevaluating LLMs in dealing with complex long-context situations. Our results\nsuggest that current LLMs have significant room for improvement in practical\nlong-context applications, as they struggle with the complexity of logical\nreasoning challenges that are likely to be present in real-world long-context\ntasks. All codes and resources are available at OpenCompass:\nhttps://github.com/open-compass/opencompass.",
      "upvotes": 43
    },
    {
      "title": "Scaling Diffusion Transformers to 16 Billion Parameters",
      "url": "https://huggingface.co/papers/2407.11633",
      "authors": [
        "Zhengcong Fei",
        "Mingyuan Fan",
        "Changqian Yu",
        "Debang Li",
        "Junshi Huang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11633.pdf",
      "abstract": "In this paper, we present DiT-MoE, a sparse version of the diffusion\nTransformer, that is scalable and competitive with dense networks while\nexhibiting highly optimized inference. The DiT-MoE includes two simple designs:\nshared expert routing and expert-level balance loss, thereby capturing common\nknowledge and reducing redundancy among the different routed experts. When\napplied to conditional image generation, a deep analysis of experts\nspecialization gains some interesting observations: (i) Expert selection shows\npreference with spatial position and denoising time step, while insensitive\nwith different class-conditional information; (ii) As the MoE layers go deeper,\nthe selection of experts gradually shifts from specific spacial position to\ndispersion and balance. (iii) Expert specialization tends to be more\nconcentrated at the early time step and then gradually uniform after half. We\nattribute it to the diffusion process that first models the low-frequency\nspatial information and then high-frequency complex information. Based on the\nabove guidance, a series of DiT-MoE experimentally achieves performance on par\nwith dense networks yet requires much less computational load during inference.\nMore encouragingly, we demonstrate the potential of DiT-MoE with synthesized\nimage data, scaling diffusion model at a 16.5B parameter that attains a new\nSoTA FID-50K score of 1.80 in 512times512 resolution settings. The project\npage: https://github.com/feizc/DiT-MoE.",
      "upvotes": 25
    },
    {
      "title": "Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes",
      "url": "https://huggingface.co/papers/2407.10957",
      "authors": [
        "Peiwen Sun",
        "Dongzhan Zhou",
        "Guangyao Li",
        "Honggang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.10957.pdf",
      "abstract": "Traditional reference segmentation tasks have predominantly focused on silent\nvisual scenes, neglecting the integral role of multimodal perception and\ninteraction in human experiences. In this work, we introduce a novel task\ncalled Reference Audio-Visual Segmentation (Ref-AVS), which seeks to segment\nobjects within the visual domain based on expressions containing multimodal\ncues. Such expressions are articulated in natural language forms but are\nenriched with multimodal cues, including audio and visual descriptions. To\nfacilitate this research, we construct the first Ref-AVS benchmark, which\nprovides pixel-level annotations for objects described in corresponding\nmultimodal-cue expressions. To tackle the Ref-AVS task, we propose a new method\nthat adequately utilizes multimodal cues to offer precise segmentation\nguidance. Finally, we conduct quantitative and qualitative experiments on three\ntest subsets to compare our approach with existing methods from related tasks.\nThe results demonstrate the effectiveness of our method, highlighting its\ncapability to precisely segment objects using multimodal-cue expressions.\nDataset is available at\nhttps://gewu-lab.github.io/Ref-AVS{https://gewu-lab.github.io/Ref-AVS}.",
      "upvotes": 23
    },
    {
      "title": "Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning",
      "url": "https://huggingface.co/papers/2407.10718",
      "authors": [
        "Lifeng Liu",
        "Jian Xie"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.10718.pdf",
      "abstract": "Existing agents based on large language models (LLMs) demonstrate robust\nproblem-solving capabilities by integrating LLMs' inherent knowledge, strong\nin-context learning and zero-shot capabilities, and the use of tools combined\nwith intricately designed LLM invocation workflows by humans. However, these\nagents still exhibit shortcomings in long-term reasoning and under-use the\npotential of existing tools, leading to noticeable deficiencies in complex\nreal-world reasoning scenarios. To address these limitations, we introduce\nSibyl, a simple yet powerful LLM-based agent framework designed to tackle\ncomplex reasoning tasks by efficiently leveraging a minimal set of tools.\nDrawing inspiration from Global Workspace Theory, Sibyl incorporates a global\nworkspace to enhance the management and sharing of knowledge and conversation\nhistory throughout the system. Furthermore, guided by Society of Mind Theory,\nSibyl implements a multi-agent debate-based jury to self-refine the final\nanswers, ensuring a comprehensive and balanced approach. This approach aims to\nreduce system complexity while expanding the scope of problems solvable-from\nmatters typically resolved by humans in minutes to those requiring hours or\neven days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl\nhas been designed with a focus on scalability and ease of debugging by\nincorporating the concept of reentrancy from functional programming from its\ninception, with the aim of seamless and low effort integration in other LLM\napplications to improve capabilities. Our experimental results on the GAIA\nbenchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves\nstate-of-the-art performance with an average score of 34.55%, compared to other\nagents based on GPT-4. We hope that Sibyl can inspire more reliable and\nreusable LLM-based agent solutions to address complex real-world reasoning\ntasks.",
      "upvotes": 17
    },
    {
      "title": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models",
      "url": "https://huggingface.co/papers/2407.11691",
      "authors": [
        "Junming Yang",
        "Yuxuan Qiao",
        "Xinyu Fang",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Pan Zhang",
        "Jiaqi Wang",
        "Dahua Lin",
        "Kai Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11691.pdf",
      "abstract": "We present VLMEvalKit: an open-source toolkit for evaluating large\nmulti-modality models based on PyTorch. The toolkit aims to provide a\nuser-friendly and comprehensive framework for researchers and developers to\nevaluate existing multi-modality models and publish reproducible evaluation\nresults. In VLMEvalKit, we implement over 70 different large multi-modality\nmodels, including both proprietary APIs and open-source models, as well as more\nthan 20 different multi-modal benchmarks. By implementing a single interface,\nnew models can be easily added to the toolkit, while the toolkit automatically\nhandles the remaining workloads, including data preparation, distributed\ninference, prediction post-processing, and metric calculation. Although the\ntoolkit is currently mainly used for evaluating large vision-language models,\nits design is compatible with future updates that incorporate additional\nmodalities, such as audio and video. Based on the evaluation results obtained\nwith the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to\ntrack the progress of multi-modality learning research. The toolkit is released\nat https://github.com/open-compass/VLMEvalKit and is actively maintained.",
      "upvotes": 13
    },
    {
      "title": "DreamCatalyst: Fast and High-Quality 3D Editing via Controlling Editability and Identity Preservation",
      "url": "https://huggingface.co/papers/2407.11394",
      "authors": [
        "Jaeyo Shin",
        "Jiho Choi",
        "Hyunjung Shim"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11394.pdf",
      "abstract": "Score distillation sampling (SDS) has emerged as an effective framework in\ntext-driven 3D editing tasks due to its inherent 3D consistency. However,\nexisting SDS-based 3D editing methods suffer from extensive training time and\nlead to low-quality results, primarily because these methods deviate from the\nsampling dynamics of diffusion models. In this paper, we propose DreamCatalyst,\na novel framework that interprets SDS-based editing as a diffusion reverse\nprocess. Our objective function considers the sampling dynamics, thereby making\nthe optimization process of DreamCatalyst an approximation of the diffusion\nreverse process in editing tasks. DreamCatalyst aims to reduce training time\nand improve editing quality. DreamCatalyst presents two modes: (1) a faster\nmode, which edits the NeRF scene in only about 25 minutes, and (2) a\nhigh-quality mode, which produces superior results in less than 70 minutes.\nSpecifically, our high-quality mode outperforms current state-of-the-art NeRF\nediting methods both in terms of speed and quality. See more extensive results\non our project page: https://dream-catalyst.github.io.",
      "upvotes": 11
    },
    {
      "title": "EfficientQAT: Efficient Quantization-Aware Training for Large Language Models",
      "url": "https://huggingface.co/papers/2407.11062",
      "authors": [
        "Wenqi Shao",
        "Peng Xu",
        "Jiahao Wang",
        "Peng Gao",
        "Kaipeng Zhang",
        "Yu Qiao",
        "Ping Luo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11062.pdf",
      "abstract": "Large language models (LLMs) are integral to modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it demands substantial training\nresources to optimize model weights and quantization parameters. To address\nthis, we propose Efficient Quantization-Aware Training (EfficientQAT), a novel\nquantization technique for compressing LLMs. EfficientQAT involves two\nconsecutive phases: Block-wise training of all parameters (Block-AP) and\nend-to-end training of quantization parameters (E2E-QP). Block-AP sequentially\nconducts quantization-aware training for all parameters in each transformer\nblock with block-wise reconstruction, maintaining efficiency by avoiding\ntraining the entire LLM. Initialized with quantized model, E2E-QP then trains\nonly quantization parameters (step sizes) end-to-end, enhancing efficiency with\na fixed quantized backbone and reduced trainable parameter count. Extensive\nexperiments demonstrate that EfficientQAT outperforms previous quantization\nmethods across a range of models, including base LLMs, instruction-tuned LLMs,\nand multimodal LLMs, with scales from 7B to 70B parameters at various\nquantization bits. For instance, EfficientQAT obtains a 2-bit Llama-2-70B model\non a single A100-80GB GPU in 41 hours, with less than 3\\% accuracy degradation\ncompared to the full precision (69.48 vs. 72.41). Notably, this INT2 quantized\n70B model obtains a 1.67 accuracy gain over the Llama-2-13B model (69.48 vs.\n67.81) while requiring less memory (19.2GB vs. 24.2GB). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT.",
      "upvotes": 8
    },
    {
      "title": "FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models",
      "url": "https://huggingface.co/papers/2407.11522",
      "authors": [
        "Zhi Gao",
        "Bofei Zhang",
        "Tao Yuan",
        "Yuwei Wu",
        "Mehrtash Harandi",
        "Yunde Jia",
        "Song-Chun Zhu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11522.pdf",
      "abstract": "Vision language models (VLMs) have achieved impressive progress in diverse\napplications, becoming a prevalent research direction. In this paper, we build\nFIRE, a feedback-refinement dataset, consisting of 1.1M multi-turn\nconversations that are derived from 27 source datasets, empowering VLMs to\nspontaneously refine their responses based on user feedback across diverse\ntasks. To scale up the data collection, FIRE is collected in two components:\nFIRE-100K and FIRE-1M, where FIRE-100K is generated by GPT-4V, and FIRE-1M is\nfreely generated via models trained on FIRE-100K. Then, we build FIRE-Bench, a\nbenchmark to comprehensively evaluate the feedback-refining capability of VLMs,\nwhich contains 11K feedback-refinement conversations as the test data, two\nevaluation settings, and a model to provide feedback for VLMs. We develop the\nFIRE-LLaVA model by fine-tuning LLaVA on FIRE-100K and FIRE-1M, which shows\nremarkable feedback-refining capability on FIRE-Bench and outperforms untrained\nVLMs by 50%, making more efficient user-agent interactions and underscoring the\nsignificance of the FIRE dataset.",
      "upvotes": 8
    },
    {
      "title": "Efficient Training with Denoised Neural Weights",
      "url": "https://huggingface.co/papers/2407.11966",
      "authors": [
        "Yifan Gong",
        "Zheng Zhan",
        "Yanyu Li",
        "Yerlan Idelbayev",
        "Andrey Zharkov",
        "Kfir Aberman",
        "Sergey Tulyakov",
        "Yanzhi Wang",
        "Jian Ren"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11966.pdf",
      "abstract": "Good weight initialization serves as an effective measure to reduce the\ntraining cost of a deep neural network (DNN) model. The choice of how to\ninitialize parameters is challenging and may require manual tuning, which can\nbe time-consuming and prone to human error. To overcome such limitations, this\nwork takes a novel step towards building a weight generator to synthesize the\nneural weights for initialization. We use the image-to-image translation task\nwith generative adversarial networks (GANs) as an example due to the ease of\ncollecting model weights spanning a wide range. Specifically, we first collect\na dataset with various image editing concepts and their corresponding trained\nweights, which are later used for the training of the weight generator. To\naddress the different characteristics among layers and the substantial number\nof weights to be predicted, we divide the weights into equal-sized blocks and\nassign each block an index. Subsequently, a diffusion model is trained with\nsuch a dataset using both text conditions of the concept and the block indexes.\nBy initializing the image translation model with the denoised weights predicted\nby our diffusion model, the training requires only 43.3 seconds. Compared to\ntraining from scratch (i.e., Pix2pix), we achieve a 15x training time\nacceleration for a new concept while obtaining even better image generation\nquality.",
      "upvotes": 8
    },
    {
      "title": "Animate3D: Animating Any 3D Model with Multi-view Video Diffusion",
      "url": "https://huggingface.co/papers/2407.11398",
      "authors": [
        "Chaohui Yu",
        "Chenjie Cao",
        "Fan Wang",
        "Weiming Hu",
        "Jin Gao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11398.pdf",
      "abstract": "Recent advances in 4D generation mainly focus on generating 4D content by\ndistilling pre-trained text or single-view image-conditioned models. It is\ninconvenient for them to take advantage of various off-the-shelf 3D assets with\nmulti-view attributes, and their results suffer from spatiotemporal\ninconsistency owing to the inherent ambiguity in the supervision signals. In\nthis work, we present Animate3D, a novel framework for animating any static 3D\nmodel. The core idea is two-fold: 1) We propose a novel multi-view video\ndiffusion model (MV-VDM) conditioned on multi-view renderings of the static 3D\nobject, which is trained on our presented large-scale multi-view video dataset\n(MV-Video). 2) Based on MV-VDM, we introduce a framework combining\nreconstruction and 4D Score Distillation Sampling (4D-SDS) to leverage the\nmulti-view video diffusion priors for animating 3D objects. Specifically, for\nMV-VDM, we design a new spatiotemporal attention module to enhance spatial and\ntemporal consistency by integrating 3D and video diffusion models.\nAdditionally, we leverage the static 3D model's multi-view renderings as\nconditions to preserve its identity. For animating 3D models, an effective\ntwo-stage pipeline is proposed: we first reconstruct motions directly from\ngenerated multi-view videos, followed by the introduced 4D-SDS to refine both\nappearance and motion. Qualitative and quantitative experiments demonstrate\nthat Animate3D significantly outperforms previous approaches. Data, code, and\nmodels will be open-released.",
      "upvotes": 8
    },
    {
      "title": "OmniBind: Large-scale Omni Multimodal Representation via Binding Spaces",
      "url": "https://huggingface.co/papers/2407.11895",
      "authors": [
        "Zehan Wang",
        "Ziang Zhang",
        "Hang Zhang",
        "Luping Liu",
        "Rongjie Huang",
        "Xize Cheng",
        "Hengshuang Zhao",
        "Zhou Zhao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11895.pdf",
      "abstract": "Recently, human-computer interaction with various modalities has shown\npromising applications, like GPT-4o and Gemini. Given the foundational role of\nmultimodal joint representation in understanding and generation pipelines,\nhigh-quality omni joint representations would be a step toward co-processing\nmore diverse multimodal information. In this work, we present OmniBind,\nlarge-scale multimodal joint representation models ranging in scale from 7\nbillion to 30 billion parameters, which support 3D, audio, image, and language\ninputs. Due to the scarcity of data pairs across all modalities, instead of\ntraining large models from scratch, we propose remapping and binding the spaces\nof various pre-trained specialist models together. This approach enables\n\"scaling up\" by indirectly increasing the model parameters and the amount of\nseen data. To effectively integrate various spaces, we dynamically assign\nweights to different spaces by learning routers with two objectives:\ncross-modal overall alignment and language representation decoupling. Notably,\nsince binding and routing spaces both only require lightweight networks,\nOmniBind is extremely training-efficient. Learning the largest 30B model\nrequires merely unpaired unimodal data and approximately 3 days on a single\n8-4090 node. Extensive experiments demonstrate the versatility and superiority\nof OmniBind as an omni representation model, highlighting its great potential\nfor diverse applications, such as any-query and composable multimodal\nunderstanding.",
      "upvotes": 7
    },
    {
      "title": "From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients",
      "url": "https://huggingface.co/papers/2407.11239",
      "authors": [
        "Ajay Jaiswal",
        "Lu Yin",
        "Shiwei Liu",
        "Jiawei Zhao",
        "Yuandong Tian",
        "Zhangyang Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11239.pdf",
      "abstract": "Modern Large Language Models (LLMs) are composed of matrices with billions of\nelements, making their storage and processing quite demanding in terms of\ncomputational resources and memory usage. Being significantly large, such\nmatrices can often be expressed in low-rank format with potential to relax\nresource requirements. Unlike prior works which focus on developing novel\nmatrix decomposition algorithms, in this work we first study the emergence of\nlow-rank structures across matrices within different layers of LLMs and\nestablish a consequential relationship between the gradient dynamics and\nemerging low-rank expressiveness of matrices. Our findings reveal that\ndifferent layers exhibit varying levels of converged low-rank structure,\nnecessitating a non-uniform rank reduction across them to minimize performance\ndrop due to compression. In view of that, we present Weight Low-Rank Projection\n(WeLore) that unifies weight compression and memory-efficient fine-tuning as\nONE, in a data-agnostic and one-shot way. WeLore capitalizes the heavy-tail\ndistribution of singular values to identify a suitable rank reduction ratio for\nmatrices within LLMs. Going beyond only as a compression technique, WeLore\ncategorizes weight matrices into Low-rank Components (LRCs) and Non-Low-rank\nComponents (N-LRCs) based on their ability to express themselves as low-rank.\nOur gradient perspective and extensive experiments illustrate that LRCs tend to\nhave better finetuning capabilities and can closely mimic (sometimes\noutperform) the training loss trajectory and performance of full-finetuning\nwith notable memory and compute footprint reduction. For example, finetuning a\n50\\% compressed LLaMa-2 7B model using only a fraction of parameters in LRCs\n(WeLore) can outperform its full finetuning with ~3x better throughput and\n~0.6x GPU requirement. Our codes are available at\nhttps://github.com/VITA-Group/welore",
      "upvotes": 7
    },
    {
      "title": "YouTube-SL-25: A Large-Scale, Open-Domain Multilingual Sign Language Parallel Corpus",
      "url": "https://huggingface.co/papers/2407.11144",
      "authors": [
        "Garrett Tanzer"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11144.pdf",
      "abstract": "Even for better-studied sign languages like American Sign Language (ASL),\ndata is the bottleneck for machine learning research. The situation is worse\nyet for the many other sign languages used by Deaf/Hard of Hearing communities\naround the world. In this paper, we present YouTube-SL-25, a large-scale,\nopen-domain multilingual corpus of sign language videos with seemingly\nwell-aligned captions drawn from YouTube. With >3000 hours of videos across >25\nsign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest\nparallel sign language dataset to date, and c) the first or largest parallel\ndataset for many of its component languages. We provide baselines for\nsign-to-text tasks using a unified multilingual multitask model based on T5 and\nreport scores on benchmarks across 4 sign languages. The results demonstrate\nthat multilingual transfer benefits both higher- and lower-resource sign\nlanguages within YouTube-SL-25.",
      "upvotes": 7
    },
    {
      "title": "Grasping Diverse Objects with Simulated Humanoids",
      "url": "https://huggingface.co/papers/2407.11385",
      "authors": [
        "Jinkun Cao",
        "Sammy Christen",
        "Alexander Winkler",
        "Kris Kitani",
        "Weipeng Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11385.pdf",
      "abstract": "We present a method for controlling a simulated humanoid to grasp an object\nand move it to follow an object trajectory. Due to the challenges in\ncontrolling a humanoid with dexterous hands, prior methods often use a\ndisembodied hand and only consider vertical lifts or short trajectories. This\nlimited scope hampers their applicability for object manipulation required for\nanimation and simulation. To close this gap, we learn a controller that can\npick up a large number (>1200) of objects and carry them to follow randomly\ngenerated trajectories. Our key insight is to leverage a humanoid motion\nrepresentation that provides human-like motor skills and significantly speeds\nup training. Using only simplistic reward, state, and object representations,\nour method shows favorable scalability on diverse object and trajectories. For\ntraining, we do not need dataset of paired full-body motion and object\ntrajectories. At test time, we only require the object mesh and desired\ntrajectories for grasping and transporting. To demonstrate the capabilities of\nour method, we show state-of-the-art success rates in following object\ntrajectories and generalizing to unseen objects. Code and models will be\nreleased.",
      "upvotes": 5
    },
    {
      "title": "Vibravox: A Dataset of French Speech Captured with Body-conduction Audio Sensors",
      "url": "https://huggingface.co/papers/2407.11828",
      "authors": [
        "Malo Olivier",
        "Thomas Joubaud",
        "Christophe Langrenne",
        "Sarah Poirée",
        "Véronique Zimpfer"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11828.pdf",
      "abstract": "Vibravox is a dataset compliant with the General Data Protection Regulation\n(GDPR) containing audio recordings using five different body-conduction audio\nsensors : two in-ear microphones, two bone conduction vibration pickups and a\nlaryngophone. The data set also includes audio data from an airborne microphone\nused as a reference. The Vibravox corpus contains 38 hours of speech samples\nand physiological sounds recorded by 188 participants under different acoustic\nconditions imposed by an high order ambisonics 3D spatializer. Annotations\nabout the recording conditions and linguistic transcriptions are also included\nin the corpus. We conducted a series of experiments on various speech-related\ntasks, including speech recognition, speech enhancement and speaker\nverification. These experiments were carried out using state-of-the-art models\nto evaluate and compare their performances on signals captured by the different\naudio sensors offered by the Vibravox dataset, with the aim of gaining a better\ngrasp of their individual characteristics.",
      "upvotes": 4
    },
    {
      "title": "Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development",
      "url": "https://huggingface.co/papers/2407.11784",
      "authors": [
        "Daoyuan Chen",
        "Haibin Wang",
        "Yilun Huang",
        "Ce Ge",
        "Yaliang Li",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11784.pdf",
      "abstract": "The emergence of large-scale multi-modal generative models has drastically\nadvanced artificial intelligence, introducing unprecedented levels of\nperformance and functionality. However, optimizing these models remains\nchallenging due to historically isolated paths of model-centric and\ndata-centric developments, leading to suboptimal outcomes and inefficient\nresource utilization. In response, we present a novel sandbox suite tailored\nfor integrated data-model co-development. This sandbox provides a comprehensive\nexperimental platform, enabling rapid iteration and insight-driven refinement\nof both data and models. Our proposed \"Probe-Analyze-Refine\" workflow,\nvalidated through applications on state-of-the-art LLaVA-like and DiT based\nmodels, yields significant performance boosts, such as topping the VBench\nleaderboard. We also uncover fruitful insights gleaned from exhaustive\nbenchmarks, shedding light on the critical interplay between data quality,\ndiversity, and model behavior. With the hope of fostering deeper understanding\nand future progress in multi-modal data and generative modeling, our codes,\ndatasets, and models are maintained and accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md.",
      "upvotes": 4
    },
    {
      "title": "Click-Gaussian: Interactive Segmentation to Any 3D Gaussians",
      "url": "https://huggingface.co/papers/2407.11793",
      "authors": [
        "Seokhun Choi",
        "Hyeonseop Song",
        "Jaechul Kim",
        "Taehyeong Kim",
        "Hoseok Do"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11793.pdf",
      "abstract": "Interactive segmentation of 3D Gaussians opens a great opportunity for\nreal-time manipulation of 3D scenes thanks to the real-time rendering\ncapability of 3D Gaussian Splatting. However, the current methods suffer from\ntime-consuming post-processing to deal with noisy segmentation output. Also,\nthey struggle to provide detailed segmentation, which is important for\nfine-grained manipulation of 3D scenes. In this study, we propose\nClick-Gaussian, which learns distinguishable feature fields of two-level\ngranularity, facilitating segmentation without time-consuming post-processing.\nWe delve into challenges stemming from inconsistently learned feature fields\nresulting from 2D segmentation obtained independently from a 3D scene. 3D\nsegmentation accuracy deteriorates when 2D segmentation results across the\nviews, primary cues for 3D segmentation, are in conflict. To overcome these\nissues, we propose Global Feature-guided Learning (GFL). GFL constructs the\nclusters of global feature candidates from noisy 2D segments across the views,\nwhich smooths out noises when training the features of 3D Gaussians. Our method\nruns in 10 ms per click, 15 to 130 times as fast as the previous methods, while\nalso significantly improving segmentation accuracy. Our project page is\navailable at https://seokhunchoi.github.io/Click-Gaussian",
      "upvotes": 3
    },
    {
      "title": "Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models",
      "url": "https://huggingface.co/papers/2407.11282",
      "authors": [
        "Qingcheng Zeng",
        "Mingyu Jin",
        "Qinkai Yu",
        "Zhenting Wang",
        "Wenyue Hua",
        "Zihao Zhou",
        "Guangyan Sun",
        "Yanda Meng",
        "Shiqing Ma",
        "Qifan Wang",
        "Felix Juefei-Xu",
        "Kaize Ding",
        "Fan Yang",
        "Ruixiang Tang",
        "Yongfeng Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11282.pdf",
      "abstract": "Large Language Models (LLMs) are employed across various high-stakes domains,\nwhere the reliability of their outputs is crucial. One commonly used method to\nassess the reliability of LLMs' responses is uncertainty estimation, which\ngauges the likelihood of their answers being correct. While many studies focus\non improving the accuracy of uncertainty estimations for LLMs, our research\ninvestigates the fragility of uncertainty estimation and explores potential\nattacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,\nwhen activated by a specific trigger in the input, manipulates the model's\nuncertainty without affecting the final output. Specifically, the proposed\nbackdoor attack method can alter an LLM's output probability distribution,\ncausing the probability distribution to converge towards an attacker-predefined\ndistribution while ensuring that the top-1 prediction remains unchanged. Our\nexperimental results demonstrate that this attack effectively undermines the\nmodel's self-evaluation reliability in multiple-choice questions. For instance,\nwe achieved a 100 attack success rate (ASR) across three different triggering\nstrategies in four models. Further, we investigate whether this manipulation\ngeneralizes across different prompts and domains. This work highlights a\nsignificant threat to the reliability of LLMs and underscores the need for\nfuture defenses against such attacks. The code is available at\nhttps://github.com/qcznlp/uncertainty_attack.",
      "upvotes": 1
    }
  ]
}