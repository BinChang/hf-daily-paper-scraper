{
  "date": "2024-06-24",
  "papers": [
    {
      "title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs",
      "url": "https://huggingface.co/papers/2406.15319",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.15319.pdf",
      "abstract": "In traditional RAG framework, the basic retrieval units are normally short.\nThe common retrievers like DPR normally work with 100-word Wikipedia\nparagraphs. Such a design forces the retriever to search over a large corpus to\nfind the `needle' unit. In contrast, the readers only need to extract answers\nfrom the short retrieved units. Such an imbalanced `heavy' retriever and\n`light' reader design can lead to sub-optimal performance. In order to\nalleviate the imbalance, we propose a new framework LongRAG, consisting of a\n`long retriever' and a `long reader'. LongRAG processes the entire Wikipedia\ninto 4K-token units, which is 30x longer than before. By increasing the unit\nsize, we significantly reduce the total units from 22M to 700K. This\nsignificantly lowers the burden of retriever, which leads to a remarkable\nretrieval score: answer recall@1=71% on NQ (previously 52%) and answer\nrecall@2=72% (previously 47%) on HotpotQA (full-wiki). Then we feed the top-k\nretrieved units (approx 30K tokens) to an existing long-context LLM to\nperform zero-shot answer extraction. Without requiring any training, LongRAG\nachieves an EM of 62.7% on NQ, which is the best known result. LongRAG also\nachieves 64.3% on HotpotQA (full-wiki), which is on par of the SoTA model. Our\nstudy offers insights into the future roadmap for combining RAG with\nlong-context LLMs.",
      "upvotes": 61
    },
    {
      "title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges",
      "url": "https://huggingface.co/papers/2406.12624",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.12624.pdf",
      "abstract": "Offering a promising solution to the scalability challenges associated with\nhuman evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an\napproach to evaluating large language models (LLMs). However, there are still\nmany open questions about the strengths and weaknesses of this paradigm, and\nwhat potential biases it may hold. In this paper, we present a comprehensive\nstudy of the performance of various LLMs acting as judges. We leverage TriviaQA\nas a benchmark for assessing objective knowledge reasoning of LLMs and evaluate\nthem alongside human annotations which we found to have a high inter-annotator\nagreement. Our study includes 9 judge models and 9 exam taker models -- both\nbase and instruction-tuned. We assess the judge model's alignment across\ndifferent model sizes, families, and judge prompts. Among other results, our\nresearch rediscovers the importance of using Cohen's kappa as a metric of\nalignment as opposed to simple percent agreement, showing that judges with high\npercent agreement can still assign vastly different scores. We find that both\nLlama-3 70B and GPT-4 Turbo have an excellent alignment with humans, but in\nterms of ranking exam taker models, they are outperformed by both JudgeLM-7B\nand the lexical judge Contains, which have up to 34 points lower human\nalignment. Through error analysis and various other studies, including the\neffects of instruction length and leniency bias, we hope to provide valuable\nlessons for using LLMs as judges in the future.",
      "upvotes": 36
    },
    {
      "title": "Complexity of Symbolic Representation in Working Memory of Transformer Correlates with the Complexity of a Task",
      "url": "https://huggingface.co/papers/2406.14213",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.14213.pdf",
      "abstract": "Even though Transformers are extensively used for Natural Language Processing\ntasks, especially for machine translation, they lack an explicit memory to\nstore key concepts of processed texts. This paper explores the properties of\nthe content of symbolic working memory added to the Transformer model decoder.\nSuch working memory enhances the quality of model predictions in machine\ntranslation task and works as a neural-symbolic representation of information\nthat is important for the model to make correct translations. The study of\nmemory content revealed that translated text keywords are stored in the working\nmemory, pointing to the relevance of memory content to the processed text.\nAlso, the diversity of tokens and parts of speech stored in memory correlates\nwith the complexity of the corpora for machine translation task.",
      "upvotes": 20
    },
    {
      "title": "Towards Retrieval Augmented Generation over Large Video Libraries",
      "url": "https://huggingface.co/papers/2406.14938",
      "authors": [
        "Frédéric Petitpont"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14938.pdf",
      "abstract": "Video content creators need efficient tools to repurpose content, a task that\noften requires complex manual or automated searches. Crafting a new video from\nlarge video libraries remains a challenge. In this paper we introduce the task\nof Video Library Question Answering (VLQA) through an interoperable\narchitecture that applies Retrieval Augmented Generation (RAG) to video\nlibraries. We propose a system that uses large language models (LLMs) to\ngenerate search queries, retrieving relevant video moments indexed by speech\nand visual metadata. An answer generation module then integrates user queries\nwith this metadata to produce responses with specific video timestamps. This\napproach shows promise in multimedia content retrieval, and AI-assisted video\ncontent creation.",
      "upvotes": 19
    },
    {
      "title": "Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework",
      "url": "https://huggingface.co/papers/2406.14783",
      "authors": [
        "Zackary Rackauckas",
        "Jakub Zavrel"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14783.pdf",
      "abstract": "Challenges in the automated evaluation of Retrieval-Augmented Generation\n(RAG) Question-Answering (QA) systems include hallucination problems in\ndomain-specific knowledge and the lack of gold standard benchmarks for company\ninternal tasks. This results in difficulties in evaluating RAG variations, like\nRAG-Fusion (RAGF), in the context of a product QA task at Infineon\nTechnologies. To solve these problems, we propose a comprehensive evaluation\nframework, which leverages Large Language Models (LLMs) to generate large\ndatasets of synthetic queries based on real user queries and in-domain\ndocuments, uses LLM-as-a-judge to rate retrieved documents and answers,\nevaluates the quality of answers, and ranks different variants of\nRetrieval-Augmented Generation (RAG) agents with RAGElo's automated Elo-based\ncompetition. LLM-as-a-judge rating of a random sample of synthetic queries\nshows a moderate, positive correlation with domain expert scoring in relevance,\naccuracy, completeness, and precision. While RAGF outperformed RAG in Elo\nscore, a significance analysis against expert annotations also shows that RAGF\nsignificantly outperforms RAG in completeness, but underperforms in precision.\nIn addition, Infineon's RAGF assistant demonstrated slightly higher performance\nin document relevance based on MRR@5 scores. We find that RAGElo positively\naligns with the preferences of human annotators, though due caution is still\nrequired. Finally, RAGF's approach leads to more complete answers based on\nexpert annotations and better answers overall based on RAGElo's evaluation\ncriteria.",
      "upvotes": 16
    },
    {
      "title": "EvTexture: Event-driven Texture Enhancement for Video Super-Resolution",
      "url": "https://huggingface.co/papers/2406.13457",
      "authors": [
        "Jiayao Lu",
        "Xiaoyan Sun"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13457.pdf",
      "abstract": "Event-based vision has drawn increasing attention due to its unique\ncharacteristics, such as high temporal resolution and high dynamic range. It\nhas been used in video super-resolution (VSR) recently to enhance the flow\nestimation and temporal alignment. Rather than for motion learning, we propose\nin this paper the first VSR method that utilizes event signals for texture\nenhancement. Our method, called EvTexture, leverages high-frequency details of\nevents to better recover texture regions in VSR. In our EvTexture, a new\ntexture enhancement branch is presented. We further introduce an iterative\ntexture enhancement module to progressively explore the\nhigh-temporal-resolution event information for texture restoration. This allows\nfor gradual refinement of texture regions across multiple iterations, leading\nto more accurate and rich high-resolution details. Experimental results show\nthat our EvTexture achieves state-of-the-art performance on four datasets. For\nthe Vid4 dataset with rich textures, our method can get up to 4.67dB gain\ncompared with recent event-based methods. Code:\nhttps://github.com/DachunKai/EvTexture.",
      "upvotes": 16
    },
    {
      "title": "Stylebreeder: Exploring and Democratizing Artistic Styles through Text-to-Image Models",
      "url": "https://huggingface.co/papers/2406.14599",
      "authors": [
        "Federico Tombari",
        "Joel Simon",
        "Pinar Yanardag"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14599.pdf",
      "abstract": "Text-to-image models are becoming increasingly popular, revolutionizing the\nlandscape of digital art creation by enabling highly detailed and creative\nvisual content generation. These models have been widely employed across\nvarious domains, particularly in art generation, where they facilitate a broad\nspectrum of creative expression and democratize access to artistic creation. In\nthis paper, we introduce STYLEBREEDER, a comprehensive dataset of 6.8M\nimages and 1.8M prompts generated by 95K users on Artbreeder, a platform that\nhas emerged as a significant hub for creative exploration with over 13M users.\nWe introduce a series of tasks with this dataset aimed at identifying diverse\nartistic styles, generating personalized content, and recommending styles based\non user interests. By documenting unique, user-generated styles that transcend\nconventional categories like 'cyberpunk' or 'Picasso,' we explore the potential\nfor unique, crowd-sourced styles that could provide deep insights into the\ncollective creative psyche of users worldwide. We also evaluate different\npersonalization methods to enhance artistic expression and introduce a style\natlas, making these models available in LoRA format for public use. Our\nresearch demonstrates the potential of text-to-image diffusion models to\nuncover and promote unique artistic expressions, further democratizing AI in\nart and fostering a more diverse and inclusive artistic community. The dataset,\ncode and models are available at https://stylebreeder.github.io under a Public\nDomain (CC0) license.",
      "upvotes": 16
    },
    {
      "title": "MantisScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation",
      "url": "https://huggingface.co/papers/2406.15252",
      "authors": [
        "Haonan Chen",
        "Kai Wang",
        "Yaswanth Narsupalli",
        "Rongqi Fan",
        "Zhiheng Lyu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15252.pdf",
      "abstract": "The recent years have witnessed great advances in video generation. However,\nthe development of automatic video metrics is lagging significantly behind.\nNone of the existing metric is able to provide reliable scores over generated\nvideos. The main barrier is the lack of large-scale human-annotated dataset. In\nthis paper, we release VideoFeedback, the first large-scale dataset containing\nhuman-provided multi-aspect score over 37.6K synthesized videos from 11\nexisting video generative models. We train MantisScore (initialized from\nMantis) based on VideoFeedback to enable automatic video quality assessment.\nExperiments show that the Spearman correlation between MantisScore and humans\ncan reach 77.1 on VideoFeedback-test, beating the prior best metrics by about\n50 points. Further result on other held-out EvalCrafter, GenAI-Bench, and\nVBench show that MantisScore has consistently much higher correlation with\nhuman judges than other metrics. Due to these results, we believe MantisScore\ncan serve as a great proxy for human raters to (1) rate different video models\nto track progress (2) simulate fine-grained human feedback in Reinforcement\nLearning with Human Feedback (RLHF) to improve current video generation models.",
      "upvotes": 14
    },
    {
      "title": "Two Giraffes in a Dirt Field: Using Game Play to Investigate Situation Modelling in Large Multimodal Models",
      "url": "https://huggingface.co/papers/2406.14035",
      "authors": [
        "Yan Weiser"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14035.pdf",
      "abstract": "While the situation has improved for text-only models, it again seems to be\nthe case currently that multimodal (text and image) models develop faster than\nways to evaluate them. In this paper, we bring a recently developed evaluation\nparadigm from text models to multimodal models, namely evaluation through the\ngoal-oriented game (self) play, complementing reference-based and\npreference-based evaluation. Specifically, we define games that challenge a\nmodel's capability to represent a situation from visual information and align\nsuch representations through dialogue. We find that the largest closed models\nperform rather well on the games that we define, while even the best\nopen-weight models struggle with them. On further analysis, we find that the\nexceptional deep captioning capabilities of the largest models drive some of\nthe performance. There is still room to grow for both kinds of models, ensuring\nthe continued relevance of the benchmark.",
      "upvotes": 12
    },
    {
      "title": "Reward Steering with Evolutionary Heuristics for Decoding-time Alignment",
      "url": "https://huggingface.co/papers/2406.15193",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.15193.pdf",
      "abstract": "The widespread applicability and increasing omnipresence of LLMs have\ninstigated a need to align LLM responses to user and stakeholder preferences.\nMany preference optimization approaches have been proposed that fine-tune LLM\nparameters to achieve good alignment. However, such parameter tuning is known\nto interfere with model performance on many tasks. Moreover, keeping up with\nshifting user preferences is tricky in such a situation. Decoding-time\nalignment with reward model guidance solves these issues at the cost of\nincreased inference time. However, most of such methods fail to strike the\nright balance between exploration and exploitation of reward -- often due to\nthe conflated formulation of these two aspects - to give well-aligned\nresponses. To remedy this we decouple these two aspects and implement them in\nan evolutionary fashion: exploration is enforced by decoding from mutated\ninstructions and exploitation is represented as the periodic replacement of\npoorly-rewarded generations with well-rewarded ones. Empirical evidences\nindicate that this strategy outperforms many preference optimization and\ndecode-time alignment approaches on two widely accepted alignment benchmarks\nAlpacaEval 2 and MT-Bench. Our implementation will be available at:\nhttps://darwin-alignment.github.io.",
      "upvotes": 12
    },
    {
      "title": "Jailbreaking as a Reward Misspecification Problem",
      "url": "https://huggingface.co/papers/2406.14393",
      "authors": [
        "Jiahui Gao",
        "Zhenguo Li",
        "Lingpeng Kong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14393.pdf",
      "abstract": "The widespread adoption of large language models (LLMs) has raised concerns\nabout their safety and reliability, particularly regarding their vulnerability\nto adversarial attacks. In this paper, we propose a novel perspective that\nattributes this vulnerability to reward misspecification during the alignment\nprocess. We introduce a metric ReGap to quantify the extent of reward\nmisspecification and demonstrate its effectiveness and robustness in detecting\nharmful backdoor prompts. Building upon these insights, we present ReMiss, a\nsystem for automated red teaming that generates adversarial prompts against\nvarious target aligned LLMs. ReMiss achieves state-of-the-art attack success\nrates on the AdvBench benchmark while preserving the human readability of the\ngenerated prompts. Detailed analysis highlights the unique advantages brought\nby the proposed reward misspecification objective compared to previous methods.",
      "upvotes": 12
    },
    {
      "title": "Cognitive Map for Language Models: Optimal Planning via Verbally Representing the World Model",
      "url": "https://huggingface.co/papers/2406.15275",
      "authors": [
        "Jongwon Lee"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15275.pdf",
      "abstract": "Language models have demonstrated impressive capabilities across various\nnatural language processing tasks, yet they struggle with planning tasks\nrequiring multi-step simulations. Inspired by human cognitive processes, this\npaper investigates the optimal planning power of language models that can\nconstruct a cognitive map of a given environment. Our experiments demonstrate\nthat cognitive map significantly enhances the performance of both optimal and\nreachable planning generation ability in the Gridworld path planning task. We\nobserve that our method showcases two key characteristics similar to human\ncognition: generalization of its planning ability to extrapolated\nenvironments and rapid adaptation with limited training data. We hope our\nfindings in the Gridworld task provide insights into modeling human cognitive\nprocesses in language models, potentially leading to the development of more\nadvanced and robust systems that better resemble human cognition.",
      "upvotes": 10
    },
    {
      "title": "Data Contamination Can Cross Language Barriers",
      "url": "https://huggingface.co/papers/2406.13236",
      "authors": [
        "Sunan Xu",
        "Animesh Kumar",
        "Jingbo Shang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13236.pdf",
      "abstract": "The opacity in developing large language models (LLMs) is raising growing\nconcerns about the potential contamination of public benchmarks in the\npre-training data. Existing contamination detection methods are typically based\non the text overlap between training and evaluation data, which can be too\nsuperficial to reflect deeper forms of contamination. In this paper, we first\npresent a cross-lingual form of contamination that inflates LLMs' performance\nwhile evading current detection methods, deliberately injected by overfitting\nLLMs on the translated versions of benchmark test sets. Then, we propose\ngeneralization-based approaches to unmask such deeply concealed contamination.\nSpecifically, we examine the LLM's performance change after modifying the\noriginal benchmark by replacing the false answer choices with correct ones from\nother questions. Contaminated models can hardly generalize to such easier\nsituations, where the false choices can be not even wrong, as all\nchoices are correct in their memorization. Experimental results demonstrate\nthat cross-lingual contamination can easily fool existing detection methods,\nbut not ours. In addition, we discuss the potential utilization of\ncross-lingual contamination in interpreting LLMs' working mechanisms and in\npost-training LLMs for enhanced multilingual capabilities. The code and dataset\nwe use can be obtained from https://github.com/ShangDataLab/Deep-Contam.",
      "upvotes": 8
    },
    {
      "title": "DELLA-Merging: Reducing Interference in Model Merging through Magnitude-Based Sampling",
      "url": "https://huggingface.co/papers/2406.11617",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.11617.pdf",
      "abstract": "With the proliferation of domain-specific models, model merging has emerged\nas a set of techniques that combine the capabilities of multiple models into\none that can multitask without the cost of additional training. In this paper,\nwe propose a new model merging technique, Drop and rEscaLe via sampLing with\nmAgnitude (DELLA-Merging), that employs a novel pruning technique, MAGPRUNE,\nwhich shows significant advantages over DARE and TIES. MAGPRUNE first ranks the\nparameters in order of their magnitude and assigns higher dropout probabilities\n(p) to parameters with lower ranks corresponding to lower magnitudes. To\napproximate the original embeddings, MAGPRUNE employs a rescaling operation on\nthe parameters that survive the random dropping by 1/(1 - p). On three\ndifferent expert models considered for merging (LM, Math, Code) and\ncorresponding benchmark datasets (AlpacaEval, GSM8K, MBPP), DELLA shows an\naverage improvement of 2.4 points over baseline methods employing delta\nparameter pruning (an improvement of 3.6 points over TIES, 1.2 points over\nDARE), and 11.1 points over the no-pruning baseline (TA). We release the source\ncode at: https://github.com/declare-lab/della.",
      "upvotes": 8
    },
    {
      "title": "4K4DGen: Panoramic 4D Generation at 4K Resolution",
      "url": "https://huggingface.co/papers/2406.13527",
      "authors": [
        "Panwang Pan",
        "Bangbang Yang",
        "Shijie Zhou",
        "Xuanyang Zhang",
        "Zeming Li",
        "Achuta Kadambi",
        "Zhangyang Wang",
        "Zhiwen Fan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13527.pdf",
      "abstract": "The blooming of virtual reality and augmented reality (VR/AR) technologies\nhas driven an increasing demand for the creation of high-quality, immersive,\nand dynamic environments. However, existing generative techniques either focus\nsolely on dynamic objects or perform outpainting from a single perspective\nimage, failing to meet the needs of VR/AR applications. In this work, we tackle\nthe challenging task of elevating a single panorama to an immersive 4D\nexperience. For the first time, we demonstrate the capability to generate\nomnidirectional dynamic scenes with 360-degree views at 4K resolution, thereby\nproviding an immersive user experience. Our method introduces a pipeline that\nfacilitates natural scene animations and optimizes a set of 4D Gaussians using\nefficient splatting techniques for real-time exploration. To overcome the lack\nof scene-scale annotated 4D data and models, especially in panoramic formats,\nwe propose a novel Panoramic Denoiser that adapts generic 2D diffusion priors\nto animate consistently in 360-degree images, transforming them into panoramic\nvideos with dynamic scenes at targeted regions. Subsequently, we elevate the\npanoramic video into a 4D immersive environment while preserving spatial and\ntemporal consistency. By transferring prior knowledge from 2D models in the\nperspective domain to the panoramic domain and the 4D lifting with spatial\nappearance and geometry regularization, we achieve high-quality Panorama-to-4D\ngeneration at a resolution of (4096 times 2048) for the first time. See the\nproject website at https://4k4dgen.github.io.",
      "upvotes": 8
    },
    {
      "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
      "url": "https://huggingface.co/papers/2406.14972",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.14972.pdf",
      "abstract": "Retrieval Augmented Generation (RAG) represents a significant advancement in\nartificial intelligence combining a retrieval phase with a generative phase,\nwith the latter typically being powered by large language models (LLMs). The\ncurrent common practices in RAG involve using \"instructed\" LLMs, which are\nfine-tuned with supervised training to enhance their ability to follow\ninstructions and are aligned with human preferences using state-of-the-art\ntechniques. Contrary to popular belief, our study demonstrates that base models\noutperform their instructed counterparts in RAG tasks by 20% on average under\nour experimental settings. This finding challenges the prevailing assumptions\nabout the superiority of instructed LLMs in RAG applications. Further\ninvestigations reveal a more nuanced situation, questioning fundamental aspects\nof RAG and suggesting the need for broader discussions on the topic; or, as\nFromm would have it, \"Seldom is a glance at the statistics enough to understand\nthe meaning of the figures\".",
      "upvotes": 7
    },
    {
      "title": "Ruby Teaming: Improving Quality Diversity Search with Memory for Automated Red Teaming",
      "url": "https://huggingface.co/papers/2406.11654",
      "authors": [
        "Vernon Toh Yan Han"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11654.pdf",
      "abstract": "We propose Ruby Teaming, a method that improves on Rainbow Teaming by\nincluding a memory cache as its third dimension. The memory dimension provides\ncues to the mutator to yield better-quality prompts, both in terms of attack\nsuccess rate (ASR) and quality diversity. The prompt archive generated by Ruby\nTeaming has an ASR of 74%, which is 20% higher than the baseline. In terms of\nquality diversity, Ruby Teaming outperforms Rainbow Teaming by 6% and 3% on\nShannon's Evenness Index (SEI) and Simpson's Diversity Index (SDI),\nrespectively.",
      "upvotes": 6
    },
    {
      "title": "Learning Molecular Representation in a Cell",
      "url": "https://huggingface.co/papers/2406.12056",
      "authors": [
        "Srijit Seal",
        "Anne E. Carpenter",
        "Shantanu Singh"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12056.pdf",
      "abstract": "Predicting drug efficacy and safety in vivo requires information on\nbiological responses (e.g., cell morphology and gene expression) to small\nmolecule perturbations. However, current molecular representation learning\nmethods do not provide a comprehensive view of cell states under these\nperturbations and struggle to remove noise, hindering model generalization. We\nintroduce the Information Alignment (InfoAlign) approach to learn molecular\nrepresentations through the information bottleneck method in cells. We\nintegrate molecules and cellular response data as nodes into a context graph,\nconnecting them with weighted edges based on chemical, biological, and\ncomputational criteria. For each molecule in a training batch, InfoAlign\noptimizes the encoder's latent representation with a minimality objective to\ndiscard redundant structural information. A sufficiency objective decodes the\nrepresentation to align with different feature spaces from the molecule's\nneighborhood in the context graph. We demonstrate that the proposed sufficiency\nobjective for alignment is tighter than existing encoder-based contrastive\nmethods. Empirically, we validate representations from InfoAlign in two\ndownstream tasks: molecular property prediction against up to 19 baseline\nmethods across four datasets, plus zero-shot molecule-morphology matching.",
      "upvotes": 6
    },
    {
      "title": "ICAL: Continual Learning of Multimodal Agents by Transforming Trajectories into Actionable Insights",
      "url": "https://huggingface.co/papers/2406.14596",
      "authors": [
        "Michael J. Tarr",
        "William W. Cohen",
        "Kenneth Marino",
        "Katerina Fragkiadaki"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14596.pdf",
      "abstract": "Large-scale generative language and vision-language models (LLMs and VLMs)\nexcel in few-shot in-context learning for decision making and instruction\nfollowing. However, they require high-quality exemplar demonstrations to be\nincluded in their context window. In this work, we ask: Can LLMs and VLMs\ngenerate their own prompt examples from generic, sub-optimal demonstrations? We\npropose In-Context Abstraction Learning (ICAL), a method that builds a memory\nof multimodal experience insights from sub-optimal demonstrations and human\nfeedback. Given a noisy demonstration in a new domain, VLMs abstract the\ntrajectory into a general program by fixing inefficient actions and annotating\ncognitive abstractions: task relationships, object state changes, temporal\nsubgoals, and task construals. These abstractions are refined and adapted\ninteractively through human feedback while the agent attempts to execute the\ntrajectory in a similar environment. The resulting abstractions, when used as\nexemplars in the prompt, significantly improve decision-making in\nretrieval-augmented LLM and VLM agents. Our ICAL agent surpasses the\nstate-of-the-art in dialogue-based instruction following in TEACh, multimodal\nweb agents in VisualWebArena, and action anticipation in Ego4D. In TEACh, we\nachieve a 12.6% improvement in goal-condition success. In VisualWebArena, our\ntask success rate improves over the SOTA from 14.3% to 22.7%. In Ego4D action\nforecasting, we improve over few-shot GPT-4V and remain competitive with\nsupervised models. We show finetuning our retrieval-augmented in-context agent\nyields additional improvements. Our approach significantly reduces reliance on\nexpert-crafted examples and consistently outperforms in-context learning from\naction plans that lack such insights.",
      "upvotes": 5
    },
    {
      "title": "Style-NeRF2NeRF: 3D Style Transfer From Style-Aligned Multi-View Images",
      "url": "https://huggingface.co/papers/2406.13393",
      "authors": [
        "Tatsuya Harada"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13393.pdf",
      "abstract": "We propose a simple yet effective pipeline for stylizing a 3D scene,\nharnessing the power of 2D image diffusion models. Given a NeRF model\nreconstructed from a set of multi-view images, we perform 3D style transfer by\nrefining the source NeRF model using stylized images generated by a\nstyle-aligned image-to-image diffusion model. Given a target style prompt, we\nfirst generate perceptually similar multi-view images by leveraging a\ndepth-conditioned diffusion model with an attention-sharing mechanism. Next,\nbased on the stylized multi-view images, we propose to guide the style transfer\nprocess with the sliced Wasserstein loss based on the feature maps extracted\nfrom a pre-trained CNN model. Our pipeline consists of decoupled steps,\nallowing users to test various prompt ideas and preview the stylized 3D result\nbefore proceeding to the NeRF fine-tuning stage. We demonstrate that our method\ncan transfer diverse artistic styles to real-world 3D scenes with competitive\nquality.",
      "upvotes": 5
    },
    {
      "title": "NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking",
      "url": "https://huggingface.co/papers/2406.15349",
      "authors": [
        "Tianyu Li",
        "Xinshuo Weng",
        "Zhiyu Huang",
        "Hongyang Li",
        "Boris Ivanovic",
        "Marco Pavone"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15349.pdf",
      "abstract": "Benchmarking vision-based driving policies is challenging. On one hand,\nopen-loop evaluation with real data is easy, but these results do not reflect\nclosed-loop performance. On the other, closed-loop evaluation is possible in\nsimulation, but is hard to scale due to its significant computational demands.\nFurther, the simulators available today exhibit a large domain gap to real\ndata. This has resulted in an inability to draw clear conclusions from the\nrapidly growing body of research on end-to-end autonomous driving. In this\npaper, we present NAVSIM, a middle ground between these evaluation paradigms,\nwhere we use large datasets in combination with a non-reactive simulator to\nenable large-scale real-world benchmarking. Specifically, we gather\nsimulation-based metrics, such as progress and time to collision, by unrolling\nbird's eye view abstractions of the test scenes for a short simulation horizon.\nOur simulation is non-reactive, i.e., the evaluated policy and environment do\nnot influence each other. As we demonstrate empirically, this decoupling allows\nopen-loop metric computation while being better aligned with closed-loop\nevaluations than traditional displacement errors. NAVSIM enabled a new\ncompetition held at CVPR 2024, where 143 teams submitted 463 entries, resulting\nin several new insights. On a large set of challenging scenarios, we observe\nthat simple methods with moderate compute requirements such as TransFuser can\nmatch recent large-scale end-to-end driving architectures such as UniAD. Our\nmodular framework can potentially be extended with new datasets, data curation\nstrategies, and metrics, and will be continually maintained to host future\nchallenges. Our code is available at\nhttps://github.com/autonomousvision/navsim.",
      "upvotes": 5
    },
    {
      "title": "RE-AdaptIR: Improving Information Retrieval through Reverse Engineered Adaptation",
      "url": "https://huggingface.co/papers/2406.14764",
      "authors": [
        "Benjamin Van Durme"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14764.pdf",
      "abstract": "Large language models (LLMs) fine-tuned for text-retrieval have demonstrated\nstate-of-the-art results across several information retrieval (IR) benchmarks.\nHowever, supervised training for improving these models requires numerous\nlabeled examples, which are generally unavailable or expensive to acquire. In\nthis work, we explore the effectiveness of extending reverse engineered\nadaptation to the context of information retrieval (RE-AdaptIR). We use\nRE-AdaptIR to improve LLM-based IR models using only unlabeled data. We\ndemonstrate improved performance both in training domains as well as zero-shot\nin domains where the models have seen no queries. We analyze performance\nchanges in various fine-tuning scenarios and offer findings of immediate use to\npractitioners.",
      "upvotes": 4
    },
    {
      "title": "Multimodal Structured Generation: CVPR's 2nd MMFM Challenge Technical Report",
      "url": "https://huggingface.co/papers/2406.11403",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.11403.pdf",
      "abstract": "Multimodal Foundation Models (MMFMs) have shown remarkable performance on\nvarious computer vision and natural language processing tasks. However, their\nperformance on particular tasks such as document understanding is still\nlimited. They also require more compute, time, and engineering resources to\nfinetune and deploy compared to traditional, unimodal models. In this report,\nwe present Multimodal Structured Generation, a general framework which\nconstrains the output logits of frozen MMFMs to force them to reason before\nresponding with structured outputs that downstream APIs can parse and use. We\nprovide a detailed account of our approach, including the technical details,\ntheoretical discussions, and final evaluation results in the 2nd Multimodal\nFoundation Models Challenge hosted by the Computer Vision and Pattern\nRecognition (CVPR) conference. Our approach achieved the second highest score\nin the hidden test set for Phase 2 and third highest overall. This shows the\nmethod's ability to generalize to unseen tasks. And that simple engineering can\nbeat expensive & complicated modelling steps as we first discussed in our\npaper, Retrieval Augmented Structured Generation: Business Document Information\nExtraction as Tool Use. All of our scripts, deployment steps, and evaluation\nresults can be accessed in https://github.com/leloykun/MMFM-Challenge",
      "upvotes": 4
    },
    {
      "title": "Low-Resource Machine Translation through the Lens of Personalized Federated Learning",
      "url": "https://huggingface.co/papers/2406.12564",
      "authors": [
        "Nazarii Tupitsa",
        "Chris Biemann",
        "Samuel Horváth",
        "Eduard Gorbunov"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12564.pdf",
      "abstract": "We present a new approach based on the Personalized Federated Learning\nalgorithm MeritFed that can be applied to Natural Language Tasks with\nheterogeneous data. We evaluate it on the Low-Resource Machine Translation\ntask, using the dataset from the Large-Scale Multilingual Machine Translation\nShared Task (Small Track #2) and the subset of Sami languages from the\nmultilingual benchmark for Finno-Ugric languages. In addition to its\neffectiveness, MeritFed is also highly interpretable, as it can be applied to\ntrack the impact of each language used for training. Our analysis reveals that\ntarget dataset size affects weight distribution across auxiliary languages,\nthat unrelated languages do not interfere with the training, and auxiliary\noptimizer parameters have minimal impact. Our approach is easy to apply with a\nfew lines of code, and we provide scripts for reproducing the experiments at\nhttps://github.com/VityaVitalich/MeritFed",
      "upvotes": 3
    },
    {
      "title": "ToVo: Toxicity Taxonomy via Voting",
      "url": "https://huggingface.co/papers/2406.14835",
      "authors": [
        "Linh Ngo Van",
        "Thien Huu Nguyen",
        "Diep Thi-Ngoc Nguyen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14835.pdf",
      "abstract": "Existing toxic detection models face significant limitations, such as lack of\ntransparency, customization, and reproducibility. These challenges stem from\nthe closed-source nature of their training data and the paucity of explanations\nfor their evaluation mechanism. To address these issues, we propose a dataset\ncreation mechanism that integrates voting and chain-of-thought processes,\nproducing a high-quality open-source dataset for toxic content detection. Our\nmethodology ensures diverse classification metrics for each sample and includes\nboth classification scores and explanatory reasoning for the classifications.\n  We utilize the dataset created through our proposed mechanism to train our\nmodel, which is then compared against existing widely-used detectors. Our\napproach not only enhances transparency and customizability but also\nfacilitates better fine-tuning for specific use cases. This work contributes a\nrobust framework for developing toxic content detection models, emphasizing\nopenness and adaptability, thus paving the way for more effective and\nuser-specific content moderation solutions.",
      "upvotes": 3
    },
    {
      "title": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions",
      "url": "https://huggingface.co/papers/2406.14805",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2406.14805.pdf",
      "abstract": "Large Language Models (LLMs) attempt to imitate human behavior by responding\nto humans in a way that pleases them, including by adhering to their values.\nHowever, humans come from diverse cultures with different values. It is\ncritical to understand whether LLMs showcase different values to the user based\non the stereotypical values of a user's known country. We prompt different LLMs\nwith a series of advice requests based on 5 Hofstede Cultural Dimensions -- a\nquantifiable way of representing the values of a country. Throughout each\nprompt, we incorporate personas representing 36 different countries and,\nseparately, languages predominantly tied to each country to analyze the\nconsistency in the LLMs' cultural understanding. Through our analysis of the\nresponses, we found that LLMs can differentiate between one side of a value and\nanother, as well as understand that countries have differing values, but will\nnot always uphold the values when giving advice, and fail to understand the\nneed to answer differently based on different cultural values. Rooted in these\nfindings, we present recommendations for training value-aligned and culturally\nsensitive LLMs. More importantly, the methodology and the framework developed\nhere can help further understand and mitigate culture and language alignment\nissues with LLMs.",
      "upvotes": 3
    }
  ]
}