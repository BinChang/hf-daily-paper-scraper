{
  "date": "2024-07-10",
  "papers": [
    {
      "title": "Vision language models are blind",
      "url": "https://huggingface.co/papers/2407.06581",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2407.06581.pdf",
      "abstract": "Large language models with vision capabilities (VLMs), e.g., GPT-4o and\nGemini 1.5 Pro are powering countless image-text applications and scoring high\non many vision-understanding benchmarks. Yet, we find that VLMs fail on 7\nvisual tasks absurdly easy to humans such as identifying (a) whether two\ncircles overlap; (b) whether two lines intersect; (c) which letter is being\ncircled in a word; and (d) counting the number of circles in a Olympic-like\nlogo. The shockingly poor performance of four state-of-the-art VLMs suggests\ntheir vision is, at best, like of a person with myopia seeing fine details as\nblurry, and at worst, like an intelligent person that is blind making educated\nguesses. Code is available at: https://vlmsareblind.github.io/",
      "upvotes": 82
    },
    {
      "title": "AgentInstruct: Toward Generative Teaching with Agentic Flows",
      "url": "https://huggingface.co/papers/2407.03502",
      "authors": [
        "Dany Rouhana",
        "Yadong Lu",
        "Wei-ge Chen",
        "Olga Vrousgos",
        "Yash Lara"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03502.pdf",
      "abstract": "Synthetic data is becoming increasingly important for accelerating the\ndevelopment of language models, both large and small. Despite several\nsuccessful use cases, researchers also raised concerns around model collapse\nand drawbacks of imitating other models. This discrepancy can be attributed to\nthe fact that synthetic data varies in quality and diversity. Effective use of\nsynthetic data usually requires significant human effort in curating the data.\nWe focus on using synthetic data for post-training, specifically creating data\nby powerful models to teach a new skill or behavior to another model, we refer\nto this setting as Generative Teaching. We introduce AgentInstruct, an\nextensible agentic framework for automatically creating large amounts of\ndiverse and high-quality synthetic data. AgentInstruct can create both the\nprompts and responses, using only raw data sources like text documents and code\nfiles as seeds. We demonstrate the utility of AgentInstruct by creating a post\ntraining dataset of 25M pairs to teach language models different skills, such\nas text editing, creative writing, tool usage, coding, reading comprehension,\netc. The dataset can be used for instruction tuning of any base model. We\npost-train Mistral-7b with the data. When comparing the resulting model Orca-3\nto Mistral-7b-Instruct (which uses the same base model), we observe significant\nimprovements across many benchmarks. For example, 40% improvement on AGIEval,\n19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement on BBH and\n45% improvement on AlpacaEval. Additionally, it consistently outperforms other\nmodels such as LLAMA-8B-instruct and GPT-3.5-turbo.",
      "upvotes": 44
    },
    {
      "title": "Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence",
      "url": "https://huggingface.co/papers/2407.07061",
      "authors": [
        "Yitong Guan",
        "Chen Qian",
        "Cheng Yang",
        "Maosong Sun"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.07061.pdf",
      "abstract": "The rapid advancement of large language models (LLMs) has paved the way for\nthe development of highly capable autonomous agents. However, existing\nmulti-agent frameworks often struggle with integrating diverse capable\nthird-party agents due to reliance on agents defined within their own\necosystems. They also face challenges in simulating distributed environments,\nas most frameworks are limited to single-device setups. Furthermore, these\nframeworks often rely on hard-coded communication pipelines, limiting their\nadaptability to dynamic task requirements. Inspired by the concept of the\nInternet, we propose the Internet of Agents (IoA), a novel framework that\naddresses these limitations by providing a flexible and scalable platform for\nLLM-based multi-agent collaboration. IoA introduces an agent integration\nprotocol, an instant-messaging-like architecture design, and dynamic mechanisms\nfor agent teaming and conversation flow control. Through extensive experiments\non general assistant tasks, embodied AI tasks, and retrieval-augmented\ngeneration benchmarks, we demonstrate that IoA consistently outperforms\nstate-of-the-art baselines, showcasing its ability to facilitate effective\ncollaboration among heterogeneous agents. IoA represents a step towards linking\ndiverse agents in an Internet-like environment, where agents can seamlessly\ncollaborate to achieve greater intelligence and capabilities. Our codebase has\nbeen released at https://github.com/OpenBMB/IoA.",
      "upvotes": 26
    },
    {
      "title": "Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision",
      "url": "https://huggingface.co/papers/2407.06189",
      "authors": [
        "Idan Szpektor",
        "Serena Yeung-Levy"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06189.pdf",
      "abstract": "The performance of Large Vision Language Models (LVLMs) is dependent on the\nsize and quality of their training datasets. Existing video instruction tuning\ndatasets lack diversity as they are derived by prompting large language models\nwith video captions to generate question-answer pairs, and are therefore mostly\ndescriptive. Meanwhile, many labeled video datasets with diverse labels and\nsupervision exist - however, we find that their integration into LVLMs is\nnon-trivial. Herein, we present Video Self-Training with augmented Reasoning\n(Video-STaR), the first video self-training approach. Video-STaR allows the\nutilization of any labeled video dataset for video instruction tuning. In\nVideo-STaR, an LVLM cycles between instruction generation and finetuning, which\nwe show (I) improves general video understanding and (II) adapts LVLMs to novel\ndownstream tasks with existing supervision. During generation, an LVLM is\nprompted to propose an answer. The answers are then filtered only to those that\ncontain the original video labels, and the LVLM is then re-trained on the\ngenerated dataset. By only training on generated answers that contain the\ncorrect video labels, Video-STaR utilizes these existing video labels as weak\nsupervision for video instruction tuning. Our results demonstrate that\nVideo-STaR-enhanced LVLMs exhibit improved performance in (I) general video QA,\nwhere TempCompass performance improved by 10%, and (II) on downstream tasks,\nwhere Video-STaR improved Kinetics700-QA accuracy by 20% and action quality\nassessment on FineDiving by 15%.",
      "upvotes": 24
    },
    {
      "title": "Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary and Instruction Capabilities",
      "url": "https://huggingface.co/papers/2407.07080",
      "authors": [
        "Amir DN Cohen",
        "Moshe Koppel"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.07080.pdf",
      "abstract": "Training large language models (LLMs) in low-resource languages such as\nHebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and\nDictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a\nsubstantial corpus of approximately 200 billion tokens in both Hebrew and\nEnglish. Adapting a pre-trained model to a new language involves specialized\ntechniques that differ significantly from training a model from scratch or\nfurther training existing models on well-resourced languages such as English.\nWe outline these novel training methodologies, which facilitate effective\nlearning and adaptation to the linguistic properties of Hebrew. Additionally,\nwe fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to\nenhance its performance on task-specific instructions. To rigorously evaluate\nour models, we introduce a new benchmark suite for Hebrew LLM evaluation,\ncovering a diverse set of tasks including Question Answering, Sentiment\nAnalysis, Winograd Schema Challenge, Translation, and Summarization. Our work\nnot only addresses the intricacies of training LLMs in low-resource languages\nbut also proposes a framework that can be leveraged for adapting other LLMs to\nvarious non-English languages, contributing to the broader field of\nmultilingual NLP.",
      "upvotes": 21
    },
    {
      "title": "RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models",
      "url": "https://huggingface.co/papers/2407.06938",
      "authors": [
        "Bowen Zhang",
        "Ting Zhang",
        "Yansong Tang",
        "Feng Zhao",
        "Dong Chen",
        "Baining Guo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06938.pdf",
      "abstract": "We present RodinHD, which can generate high-fidelity 3D avatars from a\nportrait image. Existing methods fail to capture intricate details such as\nhairstyles which we tackle in this paper. We first identify an overlooked\nproblem of catastrophic forgetting that arises when fitting triplanes\nsequentially on many avatars, caused by the MLP decoder sharing scheme. To\novercome this issue, we raise a novel data scheduling strategy and a weight\nconsolidation regularization term, which improves the decoder's capability of\nrendering sharper details. Additionally, we optimize the guiding effect of the\nportrait image by computing a finer-grained hierarchical representation that\ncaptures rich 2D texture cues, and injecting them to the 3D diffusion model at\nmultiple layers via cross-attention. When trained on 46K avatars with a noise\nschedule optimized for triplanes, the resulting model can generate 3D avatars\nwith notably better details than previous methods and can generalize to\nin-the-wild portrait input.",
      "upvotes": 21
    },
    {
      "title": "MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions",
      "url": "https://huggingface.co/papers/2407.06358",
      "authors": [
        "Yiming Gao",
        "Ziyang Yuan",
        "Ailing Zeng",
        "Yu Xiong",
        "Qiang Xu",
        "Ying Shan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06358.pdf",
      "abstract": "Sora's high-motion intensity and long consistent videos have significantly\nimpacted the field of video generation, attracting unprecedented attention.\nHowever, existing publicly available datasets are inadequate for generating\nSora-like videos, as they mainly contain short videos with low motion intensity\nand brief captions. To address these issues, we propose MiraData, a\nhigh-quality video dataset that surpasses previous ones in video duration,\ncaption detail, motion strength, and visual quality. We curate MiraData from\ndiverse, manually selected sources and meticulously process the data to obtain\nsemantically consistent clips. GPT-4V is employed to annotate structured\ncaptions, providing detailed descriptions from four different perspectives\nalong with a summarized dense caption. To better assess temporal consistency\nand motion intensity in video generation, we introduce MiraBench, which\nenhances existing benchmarks by adding 3D consistency and tracking-based motion\nstrength metrics. MiraBench includes 150 evaluation prompts and 17 metrics\ncovering temporal consistency, motion strength, 3D consistency, visual quality,\ntext-video alignment, and distribution similarity. To demonstrate the utility\nand effectiveness of MiraData, we conduct experiments using our DiT-based video\ngeneration model, MiraDiT. The experimental results on MiraBench demonstrate\nthe superiority of MiraData, especially in motion strength.",
      "upvotes": 18
    },
    {
      "title": "BM25S: Orders of magnitude faster lexical search via eager sparse scoring",
      "url": "https://huggingface.co/papers/2407.03618",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2407.03618.pdf",
      "abstract": "We introduce BM25S, an efficient Python-based implementation of BM25 that\nonly depends on Numpy and Scipy. BM25S achieves up to a 500x speedup compared\nto the most popular Python-based framework by eagerly computing BM25 scores\nduring indexing and storing them into sparse matrices. It also achieves\nconsiderable speedups compared to highly optimized Java-based implementations,\nwhich are used by popular commercial products. Finally, BM25S reproduces the\nexact implementation of five BM25 variants based on Kamphuis et al. (2020) by\nextending eager scoring to non-sparse variants using a novel score shifting\nmethod. The code can be found at https://github.com/xhluca/bm25s",
      "upvotes": 11
    },
    {
      "title": "Knowledge Composition using Task Vectors with Learned Anisotropic Scaling",
      "url": "https://huggingface.co/papers/2407.02880",
      "authors": [
        "Anton van den Hengel",
        "Ehsan Abbasnejad"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.02880.pdf",
      "abstract": "Pre-trained models produce strong generic representations that can be adapted\nvia fine-tuning. The learned weight difference relative to the pre-trained\nmodel, known as a task vector, characterises the direction and stride of\nfine-tuning. The significance of task vectors is such that simple arithmetic\noperations on them can be used to combine diverse representations from\ndifferent domains. This paper builds on these properties of task vectors and\naims to answer (1) whether components of task vectors, particularly parameter\nblocks, exhibit similar characteristics, and (2) how such blocks can be used to\nenhance knowledge composition and transfer. To this end, we introduce aTLAS, an\nalgorithm that linearly combines parameter blocks with different learned\ncoefficients, resulting in anisotropic scaling at the task vector level. We\nshow that such linear combinations explicitly exploit the low intrinsic\ndimensionality of pre-trained models, with only a few coefficients being the\nlearnable parameters. Furthermore, composition of parameter blocks leverages\nthe already learned representations, thereby reducing the dependency on large\namounts of data. We demonstrate the effectiveness of our method in task\narithmetic, few-shot recognition and test-time adaptation, with supervised or\nunsupervised objectives. In particular, we show that (1) learned anisotropic\nscaling allows task vectors to be more disentangled, causing less interference\nin composition; (2) task vector composition excels with scarce or no labeled\ndata and is less prone to domain shift, thus leading to better\ngeneralisability; (3) mixing the most informative parameter blocks across\ndifferent task vectors prior to training can reduce the memory footprint and\nimprove the flexibility of knowledge transfer. Moreover, we show the potential\nof aTLAS as a PEFT method, particularly with less data, and demonstrate that\nits scalibility.",
      "upvotes": 11
    },
    {
      "title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps",
      "url": "https://huggingface.co/papers/2407.07071",
      "authors": [
        "Cheng-Yu Hsieh",
        "Yoon Kim"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.07071.pdf",
      "abstract": "When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task.",
      "upvotes": 11
    },
    {
      "title": "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts",
      "url": "https://huggingface.co/papers/2407.03203",
      "authors": [
        "Ruida Wang",
        "Jipeng Zhang",
        "Yizhen Jia",
        "Rui Pan",
        "Shizhe Diao",
        "Renjie Pi",
        "Tong Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03203.pdf",
      "abstract": "Proving mathematical theorems using computer-verifiable formal languages like\nLean significantly impacts mathematical reasoning. One approach to formal\ntheorem proving involves generating complete proofs using Large Language Models\n(LLMs) based on Natural Language (NL) proofs. Similar methods have shown\npromising results in code generation. However, most modern LLMs exhibit\nsuboptimal performance due to the scarcity of aligned NL and Formal Language\n(FL) theorem-proving data. This scarcity results in a paucity of methodologies\nfor training LLMs and techniques to fully utilize their capabilities in\ncomposing formal proofs. To address the challenges, this paper proposes\n**TheoremLlama**, an end-to-end framework to train a general-purpose LLM to\nbecome a Lean4 expert. This framework encompasses NL-FL aligned dataset\ngeneration methods, training approaches for the LLM formal theorem prover, and\ntechniques for LLM Lean4 proof writing. Using the dataset generation method, we\nprovide *Open Bootstrapped Theorems* (OBT), an NL-FL aligned and bootstrapped\ndataset. A key innovation in this framework is the NL-FL bootstrapping method,\nwhere NL proofs are integrated into Lean4 code for training datasets,\nleveraging the NL reasoning ability of LLMs for formal reasoning. The\n**TheoremLlama** framework achieves cumulative accuracies of 36.48% and 33.61%\non MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline\nof 22.95% and 25.41%. We have also open-sourced our model checkpoints and\ngenerated dataset, and will soon make all the code publicly available.",
      "upvotes": 10
    },
    {
      "title": "Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions",
      "url": "https://huggingface.co/papers/2407.06723",
      "authors": [
        "Hadi Pour Ansari",
        "Oncel Tuzel"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06723.pdf",
      "abstract": "Humans describe complex scenes with compositionality, using simple text\ndescriptions enriched with links and relationships. While vision-language\nresearch has aimed to develop models with compositional understanding\ncapabilities, this is not reflected yet in existing datasets which, for the\nmost part, still use plain text to describe images. In this work, we propose a\nnew annotation strategy, graph-based captioning (GBC) that describes an image\nusing a labelled graph structure, with nodes of various types. The nodes in GBC\nare created using, in a first stage, object detection and dense captioning\ntools nested recursively to uncover and describe entity nodes, further linked\ntogether in a second stage by highlighting, using new types of nodes,\ncompositions and relations among entities. Since all GBC nodes hold plain text\ndescriptions, GBC retains the flexibility found in natural language, but can\nalso encode hierarchical information in its edges. We demonstrate that GBC can\nbe produced automatically, using off-the-shelf multimodal LLMs and\nopen-vocabulary detection models, by building a new dataset, GBC10M, gathering\nGBC annotations for about 10M images of the CC12M dataset. We use GBC10M to\nshowcase the wealth of node captions uncovered by GBC, as measured with CLIP\ntraining. We show that using GBC nodes' annotations -- notably those stored in\ncomposition and relation nodes -- results in significant performance boost on\ndownstream models when compared to other dataset formats. To further explore\nthe opportunities provided by GBC, we also propose a new attention mechanism\nthat can leverage the entire GBC graph, with encouraging experimental results\nthat show the extra benefits of incorporating the graph structure. Our datasets\nare released at https://huggingface.co/graph-based-captions.",
      "upvotes": 10
    },
    {
      "title": "VIMI: Grounding Video Generation through Multi-modal Instruction",
      "url": "https://huggingface.co/papers/2407.06304",
      "authors": [
        "Kuan-Chien Wang",
        "Sergey Tulyakov"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06304.pdf",
      "abstract": "Existing text-to-video diffusion models rely solely on text-only encoders for\ntheir pretraining. This limitation stems from the absence of large-scale\nmultimodal prompt video datasets, resulting in a lack of visual grounding and\nrestricting their versatility and application in multimodal integration. To\naddress this, we construct a large-scale multimodal prompt dataset by employing\nretrieval methods to pair in-context examples with the given text prompts and\nthen utilize a two-stage training strategy to enable diverse video generation\ntasks within the same model. In the first stage, we propose a multimodal\nconditional video generation framework for pretraining on these augmented\ndatasets, establishing a foundational model for grounded video generation.\nSecondly, we finetune the model from the first stage on three video generation\ntasks, incorporating multi-modal instructions. This process further refines the\nmodel's ability to handle diverse inputs and tasks, ensuring seamless\nintegration of multi-modal information. After this two-stage train-ing process,\nVIMI demonstrates multimodal understanding capabilities, producing contextually\nrich and personalized videos grounded in the provided inputs, as shown in\nFigure 1. Compared to previous visual grounded video generation methods, VIMI\ncan synthesize consistent and temporally coherent videos with large motion\nwhile retaining the semantic control. Lastly, VIMI also achieves\nstate-of-the-art text-to-video generation results on UCF101 benchmark.",
      "upvotes": 9
    },
    {
      "title": "From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty",
      "url": "https://huggingface.co/papers/2407.06071",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2407.06071.pdf",
      "abstract": "Large language models (LLMs) often exhibit undesirable behaviors, such as\nhallucinations and sequence repetitions. We propose to view these behaviors as\nfallbacks that models exhibit under uncertainty, and investigate the connection\nbetween them. We categorize fallback behaviors -- sequence repetitions,\ndegenerate text, and hallucinations -- and extensively analyze them in models\nfrom the same family that differ by the amount of pretraining tokens, parameter\ncount, or the inclusion of instruction-following training. Our experiments\nreveal a clear and consistent ordering of fallback behaviors, across all these\naxes: the more advanced an LLM is (i.e., trained on more tokens, has more\nparameters, or instruction-tuned), its fallback behavior shifts from sequence\nrepetitions, to degenerate text, and then to hallucinations. Moreover, the same\nordering is observed throughout a single generation, even for the\nbest-performing models; as uncertainty increases, models shift from generating\nhallucinations to producing degenerate text and then sequence repetitions.\nLastly, we demonstrate that while common decoding techniques, such as random\nsampling, might alleviate some unwanted behaviors like sequence repetitions,\nthey increase harder-to-detect hallucinations.",
      "upvotes": 7
    },
    {
      "title": "How do you know that? Teaching Generative Language Models to Reference Answers to Biomedical Questions",
      "url": "https://huggingface.co/papers/2407.05015",
      "authors": [
        "Lorenzo Cassano"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05015.pdf",
      "abstract": "Large language models (LLMs) have recently become the leading source of\nanswers for users' questions online. Despite their ability to offer eloquent\nanswers, their accuracy and reliability can pose a significant challenge. This\nis especially true for sensitive domains such as biomedicine, where there is a\nhigher need for factually correct answers. This paper introduces a biomedical\nretrieval-augmented generation (RAG) system designed to enhance the reliability\nof generated responses. The system is based on a fine-tuned LLM for the\nreferenced question-answering, where retrieved relevant abstracts from PubMed\nare passed to LLM's context as input through a prompt. Its output is an answer\nbased on PubMed abstracts, where each statement is referenced accordingly,\nallowing the users to verify the answer. Our retrieval system achieves an\nabsolute improvement of 23% compared to the PubMed search engine. Based on the\nmanual evaluation on a small sample, our fine-tuned LLM component achieves\ncomparable results to GPT-4 Turbo in referencing relevant abstracts. We make\nthe dataset used to fine-tune the models and the fine-tuned models based on\nMistral-7B-instruct-v0.1 and v0.2 publicly available.",
      "upvotes": 4
    },
    {
      "title": "LETS-C: Leveraging Language Embedding for Time Series Classification",
      "url": "https://huggingface.co/papers/2407.06533",
      "authors": [
        "Zhen Zeng",
        "Tucker Balch",
        "Manuela Veloso"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06533.pdf",
      "abstract": "Recent advancements in language modeling have shown promising results when\napplied to time series data. In particular, fine-tuning pre-trained large\nlanguage models (LLMs) for time series classification tasks has achieved\nstate-of-the-art (SOTA) performance on standard benchmarks. However, these\nLLM-based models have a significant drawback due to the large model size, with\nthe number of trainable parameters in the millions. In this paper, we propose\nan alternative approach to leveraging the success of language modeling in the\ntime series domain. Instead of fine-tuning LLMs, we utilize a language\nembedding model to embed time series and then pair the embeddings with a simple\nclassification head composed of convolutional neural networks (CNN) and\nmultilayer perceptron (MLP). We conducted extensive experiments on\nwell-established time series classification benchmark datasets. We demonstrated\nLETS-C not only outperforms the current SOTA in classification accuracy but\nalso offers a lightweight solution, using only 14.5% of the trainable\nparameters on average compared to the SOTA model. Our findings suggest that\nleveraging language encoders to embed time series data, combined with a simple\nyet effective classification head, offers a promising direction for achieving\nhigh-performance time series classification while maintaining a lightweight\nmodel architecture.",
      "upvotes": 2
    }
  ]
}