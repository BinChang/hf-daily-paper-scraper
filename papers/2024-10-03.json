{
  "date": "2024-10-03",
  "papers": [
    {
      "title": "RATIONALYST: Pre-training Process-Supervision for Improving Reasoning",
      "url": "https://huggingface.co/papers/2410.01044",
      "authors": [
        "Guoxuan Wang",
        "Andrew Wang",
        "Benjamin Van Durme"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01044.pdf",
      "abstract": "The reasoning steps generated by LLMs might be incomplete, as they mimic\nlogical leaps common in everyday communication found in their pre-training\ndata: underlying rationales are frequently left implicit (unstated). To address\nthis challenge, we introduce RATIONALYST, a model for process-supervision of\nreasoning based on pre-training on a vast collection of rationale annotations\nextracted from unlabeled data. We extract 79k rationales from web-scale\nunlabelled dataset (the Pile) and a combination of reasoning datasets with\nminimal human intervention. This web-scale pre-training for reasoning allows\nRATIONALYST to consistently generalize across diverse reasoning tasks,\nincluding mathematical, commonsense, scientific, and logical reasoning.\nFine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by\nan average of 3.9% on 7 representative reasoning benchmarks. It also\ndemonstrates superior performance compared to significantly larger verifiers\nlike GPT-4 and similarly sized models fine-tuned on matching training sets.",
      "upvotes": 34
    },
    {
      "title": "PHI-S: Distribution Balancing for Label-Free Multi-Teacher Distillation",
      "url": "https://huggingface.co/papers/2410.01680",
      "authors": [
        "Jon Barker",
        "Pavlo Molchanov",
        "Andrew Tao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01680.pdf",
      "abstract": "Various visual foundation models have distinct strengths and weaknesses, both\nof which can be improved through heterogeneous multi-teacher knowledge\ndistillation without labels, termed \"agglomerative models.\" We build upon this\nbody of work by studying the effect of the teachers' activation statistics,\nparticularly the impact of the loss function on the resulting student model\nquality. We explore a standard toolkit of statistical normalization techniques\nto better align the different distributions and assess their effects. Further,\nwe examine the impact on downstream teacher-matching metrics, which motivates\nthe use of Hadamard matrices. With these matrices, we demonstrate useful\nproperties, showing how they can be used for isotropic standardization, where\neach dimension of a multivariate distribution is standardized using the same\nscale. We call this technique \"PHI Standardization\" (PHI-S) and empirically\ndemonstrate that it produces the best student model across the suite of methods\nstudied.",
      "upvotes": 32
    },
    {
      "title": "From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging",
      "url": "https://huggingface.co/papers/2410.01215",
      "authors": [
        "Songsong Wang",
        "Chengcheng Wan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01215.pdf",
      "abstract": "While large language models have made significant strides in code generation,\nthe pass rate of the generated code is bottlenecked on subtle errors, often\nrequiring human intervention to pass tests, especially for complex problems.\nExisting LLM-based debugging systems treat generated programs as monolithic\nunits, failing to address bugs at multiple levels of granularity, from\nlow-level syntax errors to high-level algorithmic flaws. In this paper, we\nintroduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger\nby isolating, identifying, and resolving bugs at various levels of granularity.\nMGDebugger decomposes problematic code into a hierarchical tree structure of\nsubfunctions, with each level representing a particular granularity of error.\nDuring debugging, it analyzes each subfunction and iteratively resolves bugs in\na bottom-up manner. To effectively test each subfunction, we propose an\nLLM-simulated Python executor, which traces code execution and tracks important\nvariable states to pinpoint errors accurately. Extensive experiments\ndemonstrate that MGDebugger outperforms existing debugging systems, achieving\nan 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6%\nrepair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes\nbugs across different categories and difficulty levels, demonstrating its\nrobustness and effectiveness.",
      "upvotes": 30
    },
    {
      "title": "3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection",
      "url": "https://huggingface.co/papers/2410.01647",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.01647.pdf",
      "abstract": "Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and\nhave been adapted for 3D Object Detection (3DOD), offering a promising approach\nto 3DOD through view-synthesis representation. However, NeRF faces inherent\nlimitations: (i) limited representational capacity for 3DOD due to its implicit\nnature, and (ii) slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS)\nhas emerged as an explicit 3D representation that addresses these limitations.\nInspired by these advantages, this paper introduces 3DGS into 3DOD for the\nfirst time, identifying two main challenges: (i) Ambiguous spatial distribution\nof Gaussian blobs: 3DGS primarily relies on 2D pixel-level supervision,\nresulting in unclear 3D spatial distribution of Gaussian blobs and poor\ndifferentiation between objects and background, which hinders 3DOD; (ii)\nExcessive background blobs: 2D images often include numerous background pixels,\nleading to densely reconstructed 3DGS with many noisy Gaussian blobs\nrepresenting the background, negatively affecting detection. To tackle the\nchallenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D\nimages, and propose an elegant and efficient solution by incorporating 2D\nBoundary Guidance to significantly enhance the spatial distribution of Gaussian\nblobs, resulting in clearer differentiation between objects and their\nbackground. To address the challenge (ii), we propose a Box-Focused Sampling\nstrategy using 2D boxes to generate object probability distribution in 3D\nspaces, allowing effective probabilistic sampling in 3D to retain more object\nblobs and reduce noisy background blobs. Benefiting from our designs, our\n3DGS-DET significantly outperforms the SOTA NeRF-based method, NeRF-Det,\nachieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet\ndataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset.",
      "upvotes": 28
    },
    {
      "title": "Not All LLM Reasoners Are Created Equal",
      "url": "https://huggingface.co/papers/2410.01748",
      "authors": [
        "Alessandro Sordoni",
        "Daniel Toyama",
        "Aaron Courville"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01748.pdf",
      "abstract": "We study the depth of grade-school math (GSM) problem-solving capabilities of\nLLMs. To this end, we evaluate their performance on pairs of existing math word\nproblems together so that the answer to the second problem depends on correctly\nanswering the first problem. Our findings reveal a significant reasoning gap in\nmost LLMs, that is performance difference between solving the compositional\npairs and solving each question independently. This gap is more pronounced in\nsmaller, more cost-efficient, and math-specialized models. Moreover,\ninstruction-tuning recipes and code generation have varying effects across LLM\nsizes, while finetuning on GSM can lead to task overfitting. Our analysis\nindicates that large reasoning gaps are not because of test-set leakage, but\ndue to distraction from additional context and poor second-hop reasoning.\nOverall, LLMs exhibit systematic differences in their reasoning abilities,\ndespite what their performance on standard benchmarks indicates.",
      "upvotes": 27
    },
    {
      "title": "LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks",
      "url": "https://huggingface.co/papers/2410.01744",
      "authors": [
        "Tianqing Fang",
        "Zhihan Zhang",
        "Siru Ouyang",
        "Hongming Zhang",
        "Dong Yu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01744.pdf",
      "abstract": "Text-rich images, where text serves as the central visual element guiding the\noverall understanding, are prevalent in real-world applications, such as\npresentation slides, scanned documents, and webpage snapshots. Tasks involving\nmultiple text-rich images are especially challenging, as they require not only\nunderstanding the content of individual images but reasoning about\ninter-relationships and logical flows across multiple visual inputs. Despite\nthe importance of these scenarios, current multimodal large language models\n(MLLMs) struggle to handle such tasks due to two key challenges: (1) the\nscarcity of high-quality instruction tuning datasets for text-rich multi-image\nscenarios, and (2) the difficulty in balancing image resolution with visual\nfeature sequence length. To address these challenges, we propose \\OurMethod, a\nMLLM designed specifically for handling vision-language tasks involving\nmultiple text-rich images. First, we curated about one million high-quality\nmultimodal instruction-tuning data, tailored to text-rich, multi-image\nscenarios. Second, we developed an adaptive high-resolution multi-image\nencoding module to dynamically optimize the allocation of visual sequence\nlength based on the original aspect ratios and resolutions of the input images.\nExperiments across a wide range of benchmarks demonstrate our model's superior\ncapabilities in text-rich, multi-image evaluations and competitive performance\nin general domain evaluations.",
      "upvotes": 25
    },
    {
      "title": "HelpSteer2-Preference: Complementing Ratings with Preferences",
      "url": "https://huggingface.co/papers/2410.01257",
      "authors": [
        "Jiaqi Zeng",
        "Yi Dong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01257.pdf",
      "abstract": "Reward models are critical for aligning models to follow instructions, and\nare typically trained following one of two popular paradigms: Bradley-Terry\nstyle or Regression style. However, there is a lack of evidence that either\napproach is better than the other, when adequately matched for data. This is\nprimarily because these approaches require data collected in different (but\nincompatible) formats, meaning that adequately matched data is not available in\nexisting public datasets. To tackle this problem, we release preference\nannotations (designed for Bradley-Terry training) to complement existing\nratings (designed for Regression style training) in the HelpSteer2 dataset. To\nimprove data interpretability, preference annotations are accompanied with\nhuman-written justifications. Using this data, we conduct the first\nhead-to-head comparison of Bradley-Terry and Regression models when adequately\nmatched for data. Based on insights derived from such a comparison, we propose\na novel approach to combine Bradley-Terry and Regression reward modeling. A\nLlama-3.1-70B-Instruct model tuned with this approach scores 94.1 on\nRewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. We\nalso demonstrate the effectiveness of this reward model at aligning models to\nfollow instructions in RLHF. We open-source this dataset (CC-BY-4.0 license) at\nhttps://huggingface.co/datasets/nvidia/HelpSteer2 and openly release the\ntrained Reward Model at\nhttps://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward",
      "upvotes": 19
    },
    {
      "title": "Selective Aggregation for Low-Rank Adaptation in Federated Learning",
      "url": "https://huggingface.co/papers/2410.01463",
      "authors": [
        "Huijie Fan",
        "Feifei Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01463.pdf",
      "abstract": "We investigate LoRA in federated learning through the lens of the asymmetry\nanalysis of the learned A and B matrices. In doing so, we uncover that A\nmatrices are responsible for learning general knowledge, while B matrices\nfocus on capturing client-specific knowledge. Based on this finding, we\nintroduce Federated Share-A Low-Rank Adaptation (FedSA-LoRA), which employs two\nlow-rank trainable matrices A and B to model the weight update, but only\nA matrices are shared with the server for aggregation. Moreover, we delve\ninto the relationship between the learned A and B matrices in other LoRA\nvariants, such as rsLoRA and VeRA, revealing a consistent pattern.\nConsequently, we extend our FedSA-LoRA method to these LoRA variants, resulting\nin FedSA-rsLoRA and FedSA-VeRA. In this way, we establish a general paradigm\nfor integrating LoRA with FL, offering guidance for future work on subsequent\nLoRA variants combined with FL. Extensive experimental results on natural\nlanguage understanding and generation tasks demonstrate the effectiveness of\nthe proposed method.",
      "upvotes": 18
    },
    {
      "title": "Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis",
      "url": "https://huggingface.co/papers/2409.20059",
      "authors": [
        "Ricardo Rei"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.20059.pdf",
      "abstract": "Neural metrics for machine translation (MT) evaluation have become\nincreasingly prominent due to their superior correlation with human judgments\ncompared to traditional lexical metrics. Researchers have therefore utilized\nneural metrics through quality-informed decoding strategies, achieving better\nresults than likelihood-based methods. With the rise of Large Language Models\n(LLMs), preference-based alignment techniques have gained attention for their\npotential to enhance translation quality by optimizing model weights directly\non preferences induced by quality estimators. This study focuses on Contrastive\nPreference Optimization (CPO) and conducts extensive experiments to evaluate\nthe impact of preference-based alignment on translation quality. Our findings\nindicate that while CPO consistently outperforms Supervised Fine-Tuning (SFT)\non high-quality data with regard to the alignment metric, it may lead to\ninstability across downstream evaluation metrics, particularly between neural\nand lexical ones. Additionally, we demonstrate that relying solely on the base\nmodel for generating candidate translations achieves performance comparable to\nusing multiple external systems, while ensuring better consistency across\ndownstream metrics.",
      "upvotes": 15
    },
    {
      "title": "ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation",
      "url": "https://huggingface.co/papers/2410.01731",
      "authors": [
        "Amit H. Bermano"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01731.pdf",
      "abstract": "The practical use of text-to-image generation has evolved from simple,\nmonolithic models to complex workflows that combine multiple specialized\ncomponents. While workflow-based approaches can lead to improved image quality,\ncrafting effective workflows requires significant expertise, owing to the large\nnumber of available components, their complex inter-dependence, and their\ndependence on the generation prompt. Here, we introduce the novel task of\nprompt-adaptive workflow generation, where the goal is to automatically tailor\na workflow to each user prompt. We propose two LLM-based approaches to tackle\nthis task: a tuning-based method that learns from user-preference data, and a\ntraining-free method that uses the LLM to select existing flows. Both\napproaches lead to improved image quality when compared to monolithic models or\ngeneric, prompt-independent workflows. Our work shows that prompt-dependent\nflow prediction offers a new pathway to improving text-to-image generation\nquality, complementing existing research directions in the field.",
      "upvotes": 15
    },
    {
      "title": "MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages",
      "url": "https://huggingface.co/papers/2410.01036",
      "authors": [
        "Marco Gaido",
        "Sara Papi",
        "Luisa Bentivogli",
        "Mauro Cettolo",
        "Roberto Gretter",
        "Matteo Negri"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01036.pdf",
      "abstract": "The rise of foundation models (FMs), coupled with regulatory efforts\naddressing their risks and impacts, has sparked significant interest in\nopen-source models. However, existing speech FMs (SFMs) fall short of full\ncompliance with the open-source principles, even if claimed otherwise, as no\nexisting SFM has model weights, code, and training data publicly available\nunder open-source terms. In this work, we take the first step toward filling\nthis gap by focusing on the 24 official languages of the European Union (EU).\nWe collect suitable training data by surveying automatic speech recognition\ndatasets and unlabeled speech corpora under open-source compliant licenses, for\na total of 950k hours. Additionally, we release automatic transcripts for 441k\nhours of unlabeled data under the permissive CC-BY license, thereby\nfacilitating the creation of open-source SFMs for the EU languages.",
      "upvotes": 14
    },
    {
      "title": "Quantifying Generalization Complexity for Large Language Models",
      "url": "https://huggingface.co/papers/2410.01769",
      "authors": [
        "Himabindu Lakkaraju"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01769.pdf",
      "abstract": "While large language models (LLMs) have shown exceptional capabilities in\nunderstanding complex queries and performing sophisticated tasks, their\ngeneralization abilities are often deeply entangled with memorization,\nnecessitating more precise evaluation. To address this challenge, we introduce\nScylla, a dynamic evaluation framework that quantitatively measures the\ngeneralization abilities of LLMs. Scylla disentangles generalization from\nmemorization via assessing model performance on both in-distribution (ID) and\nout-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.\nThrough extensive experiments, we uncover a non-monotonic relationship between\ntask complexity and the performance gap between ID and OOD data, which we term\nthe generalization valley. Specifically, this phenomenon reveals a critical\nthreshold - referred to as critical complexity - where reliance on\nnon-generalizable behavior peaks, indicating the upper bound of LLMs'\ngeneralization capabilities. As model size increases, the critical complexity\nshifts toward higher levels of task complexity, suggesting that larger models\ncan handle more complex reasoning tasks before over-relying on memorization.\nLeveraging Scylla and the concept of critical complexity, we benchmark 28LLMs\nincluding both open-sourced models such as LLaMA and Qwen families, and\nclose-sourced models like Claude and GPT, providing a more robust evaluation\nand establishing a clearer understanding of LLMs' generalization capabilities.",
      "upvotes": 13
    },
    {
      "title": "FactAlign: Long-form Factuality Alignment of Large Language Models",
      "url": "https://huggingface.co/papers/2410.01691",
      "authors": [
        "Yun-Nung Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01691.pdf",
      "abstract": "Large language models have demonstrated significant potential as the\nnext-generation information access engines. However, their reliability is\nhindered by issues of hallucination and generating non-factual content. This is\nparticularly problematic in long-form responses, where assessing and ensuring\nfactual accuracy is complex. In this paper, we address this gap by proposing\nFactAlign, a novel alignment framework designed to enhance the factuality of\nLLMs' long-form responses while maintaining their helpfulness. We introduce\nfKTO, a fine-grained, sentence-level alignment algorithm that extends the\nKahneman-Tversky Optimization (KTO) alignment method. Leveraging recent\nadvances in automatic factuality evaluation, FactAlign utilizes fine-grained\nfactuality assessments to guide the alignment process. Our experiments on\nopen-domain prompts and information-seeking questions demonstrate that\nFactAlign significantly improves the factual accuracy of LLM responses while\nalso improving their helpfulness. Further analyses identify that FactAlign is\ncapable of training LLMs to provide more information without losing factual\nprecision, thus improving the factual F1 score. Our source code, datasets, and\ntrained models are publicly available at https://github.com/MiuLab/FactAlign",
      "upvotes": 8
    },
    {
      "title": "General Preference Modeling with Preference Representations for Aligning Language Models",
      "url": "https://huggingface.co/papers/2410.02197",
      "authors": [
        "Yue Wu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.02197.pdf",
      "abstract": "Modeling human preferences is crucial for aligning foundation models with\nhuman values. Traditional reward modeling methods, such as the Bradley-Terry\n(BT) reward model, fall short in expressiveness, particularly in addressing\nintransitive preferences. Although supervised pair preference models (PairPM)\ncan express general preferences, their implementation is highly ad-hoc and\ncannot guarantee a consistent preference probability of compared pairs.\nAdditionally, they impose high computational costs due to their quadratic query\ncomplexity when comparing multiple responses. In this paper, we introduce\npreference representation learning, an approach that embeds responses into a\nlatent space to capture intricate preference structures efficiently, achieving\nlinear query complexity. Additionally, we propose preference score-based\nGeneral Preference Optimization (GPO), which generalizes reward-based\nreinforcement learning from human feedback. Experimental results show that our\nGeneral Preference representation model (GPM) outperforms the BT reward model\non the RewardBench benchmark with a margin of up to 5.6% and effectively models\ncyclic preferences where any BT reward model behaves like a random guess.\nFurthermore, evaluations on downstream tasks such as AlpacaEval2.0 and\nMT-Bench, following the language model post-training with GPO and our general\npreference model, reveal substantial performance improvements with margins up\nto 9.3%. These findings indicate that our method may enhance the alignment of\nfoundation models with nuanced human values. The code is available at\nhttps://github.com/general-preference/general-preference-model.",
      "upvotes": 7
    },
    {
      "title": "BordIRlines: A Dataset for Evaluating Cross-lingual Retrieval-Augmented Generation",
      "url": "https://huggingface.co/papers/2410.01171",
      "authors": [
        "Samar Haider",
        "Adwait Agashe",
        "Chris Callison-Burch"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01171.pdf",
      "abstract": "Large language models excel at creative generation but continue to struggle\nwith the issues of hallucination and bias. While retrieval-augmented generation\n(RAG) provides a framework for grounding LLMs' responses in accurate and\nup-to-date information, it still raises the question of bias: which sources\nshould be selected for inclusion in the context? And how should their\nimportance be weighted? In this paper, we study the challenge of cross-lingual\nRAG and present a dataset to investigate the robustness of existing systems at\nanswering queries about geopolitical disputes, which exist at the intersection\nof linguistic, cultural, and political boundaries. Our dataset is sourced from\nWikipedia pages containing information relevant to the given queries and we\ninvestigate the impact of including additional context, as well as the\ncomposition of this context in terms of language and source, on an LLM's\nresponse. Our results show that existing RAG systems continue to be challenged\nby cross-lingual use cases and suffer from a lack of consistency when they are\nprovided with competing information in multiple languages. We present case\nstudies to illustrate these issues and outline steps for future research to\naddress these challenges. We make our dataset and code publicly available at\nhttps://github.com/manestay/bordIRlines.",
      "upvotes": 5
    },
    {
      "title": "E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding",
      "url": "https://huggingface.co/papers/2409.18111",
      "authors": [
        "Zhongang Qi",
        "Yang Wu",
        "Ying Shan",
        "Chang Wen Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.18111.pdf",
      "abstract": "Recent advances in Video Large Language Models (Video-LLMs) have demonstrated\ntheir great potential in general-purpose video understanding. To verify the\nsignificance of these models, a number of benchmarks have been proposed to\ndiagnose their capabilities in different scenarios. However, existing\nbenchmarks merely evaluate models through video-level question-answering,\nlacking fine-grained event-level assessment and task diversity. To fill this\ngap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding\nBenchmark), a large-scale and high-quality benchmark for open-ended event-level\nvideo understanding. Categorized within a 3-level task taxonomy, E.T. Bench\nencompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length)\nunder 8 domains, providing comprehensive evaluations. We extensively evaluated\n8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that\nstate-of-the-art models for coarse-level (video-level) understanding struggle\nto solve our fine-grained tasks, e.g., grounding event-of-interests within\nvideos, largely due to the short video context length, improper time\nrepresentations, and lack of multi-event training data. Focusing on these\nissues, we further propose a strong baseline model, E.T. Chat, together with an\ninstruction-tuning dataset E.T. Instruct 164K tailored for fine-grained\nevent-level understanding. Our simple but effective solution demonstrates\nsuperior performance in multiple scenarios.",
      "upvotes": 5
    },
    {
      "title": "EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis",
      "url": "https://huggingface.co/papers/2410.01804",
      "authors": [
        "George Kopanas",
        "David Futschik",
        "Falko Kuester",
        "Jon Barron",
        "Yinda Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01804.pdf",
      "abstract": "We present Exact Volumetric Ellipsoid Rendering (EVER), a method for\nreal-time differentiable emission-only volume rendering. Unlike recent\nrasterization based approach by 3D Gaussian Splatting (3DGS), our primitive\nbased representation allows for exact volume rendering, rather than alpha\ncompositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does\nnot suffer from popping artifacts and view dependent density, but still\nachieves frame rates of sim!30 FPS at 720p on an NVIDIA RTX4090. Since our\napproach is built upon ray tracing it enables effects such as defocus blur and\ncamera distortion (e.g. such as from fisheye cameras), which are difficult to\nachieve by rasterization. We show that our method is more accurate with fewer\nblending issues than 3DGS and follow-up work on view-consistent rendering,\nespecially on the challenging large-scale scenes from the Zip-NeRF dataset\nwhere it achieves sharpest results among real-time techniques.",
      "upvotes": 5
    },
    {
      "title": "VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data",
      "url": "https://huggingface.co/papers/2410.00296",
      "authors": [
        "Reshmi Ghosh",
        "Robert Sim",
        "Ahmed Salem",
        "Vitor Carvalho",
        "Emily Lawton",
        "Yixuan Li",
        "Jack W. Stokes"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00296.pdf",
      "abstract": "Vision-language models (VLMs) are essential for contextual understanding of\nboth visual and textual information. However, their vulnerability to\nadversarially manipulated inputs presents significant risks, leading to\ncompromised outputs and raising concerns about the reliability in\nVLM-integrated applications. Detecting these malicious prompts is thus crucial\nfor maintaining trust in VLM generations. A major challenge in developing a\nsafeguarding prompt classifier is the lack of a large amount of labeled benign\nand malicious data. To address the issue, we introduce VLMGuard, a novel\nlearning framework that leverages the unlabeled user prompts in the wild for\nmalicious prompt detection. These unlabeled prompts, which naturally arise when\nVLMs are deployed in the open world, consist of both benign and malicious\ninformation. To harness the unlabeled data, we present an automated\nmaliciousness estimation score for distinguishing between benign and malicious\nsamples within this unlabeled mixture, thereby enabling the training of a\nbinary prompt classifier on top. Notably, our framework does not require extra\nhuman annotations, offering strong flexibility and practicality for real-world\napplications. Extensive experiment shows VLMGuard achieves superior detection\nresults, significantly outperforming state-of-the-art methods. Disclaimer: This\npaper may contain offensive examples; reader discretion is advised.",
      "upvotes": 4
    },
    {
      "title": "EmoKnob: Enhance Voice Cloning with Fine-Grained Emotion Control",
      "url": "https://huggingface.co/papers/2410.00316",
      "authors": [
        "Run Chen",
        "Julia Hirschberg"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00316.pdf",
      "abstract": "While recent advances in Text-to-Speech (TTS) technology produce natural and\nexpressive speech, they lack the option for users to select emotion and control\nintensity. We propose EmoKnob, a framework that allows fine-grained emotion\ncontrol in speech synthesis with few-shot demonstrative samples of arbitrary\nemotion. Our framework leverages the expressive speaker representation space\nmade possible by recent advances in foundation voice cloning models. Based on\nthe few-shot capability of our emotion control framework, we propose two\nmethods to apply emotion control on emotions described by open-ended text,\nenabling an intuitive interface for controlling a diverse array of nuanced\nemotions. To facilitate a more systematic emotional speech synthesis field, we\nintroduce a set of evaluation metrics designed to rigorously assess the\nfaithfulness and recognizability of emotion control frameworks. Through\nobjective and subjective evaluations, we show that our emotion control\nframework effectively embeds emotions into speech and surpasses emotion\nexpressiveness of commercial TTS services.",
      "upvotes": 4
    },
    {
      "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache in Diffusion Transformer Acceleration",
      "url": "https://huggingface.co/papers/2410.01723",
      "authors": [
        "Zining Wang",
        "Ruihao Gong",
        "Jing Liu",
        "Xinjie Zhang",
        "Jinyang Guo",
        "Xianglong Liu",
        "Jun Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01723.pdf",
      "abstract": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep.",
      "upvotes": 4
    },
    {
      "title": "Old Optimizer, New Norm: An Anthology",
      "url": "https://huggingface.co/papers/2409.20325",
      "authors": [
        "Jeremy Bernstein",
        "Laker Newhouse"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.20325.pdf",
      "abstract": "Deep learning optimizers are often motivated through a mix of convex and\napproximate second-order theory. We select three such methods -- Adam, Shampoo\nand Prodigy -- and argue that each method can instead be understood as a\nsquarely first-order method without convexity assumptions. In fact, after\nswitching off exponential moving averages, each method is equivalent to\nsteepest descent under a particular norm. By generalizing this observation, we\nchart a new design space for training algorithms. Different operator norms\nshould be assigned to different tensors based on the role that the tensor plays\nwithin the network. For example, while linear and embedding layers may have the\nsame weight space of R^{mtimes n}, these layers play different\nroles and should be assigned different norms. We hope that this idea of\ncarefully metrizing the neural architecture might lead to more stable, scalable\nand indeed faster training.",
      "upvotes": 3
    },
    {
      "title": "Closed-loop Long-horizon Robotic Planning via Equilibrium Sequence Modeling",
      "url": "https://huggingface.co/papers/2410.01440",
      "authors": [
        "Fei Li",
        "Cao Sheng",
        "Jiazhong Yu",
        "Yadong Mu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01440.pdf",
      "abstract": "In the endeavor to make autonomous robots take actions, task planning is a\nmajor challenge that requires translating high-level task descriptions into\nlong-horizon action sequences. Despite recent advances in language model\nagents, they remain prone to planning errors and limited in their ability to\nplan ahead. To address these limitations in robotic planning, we advocate a\nself-refining scheme that iteratively refines a draft plan until an equilibrium\nis reached. Remarkably, this process can be optimized end-to-end from an\nanalytical perspective without the need to curate additional verifiers or\nreward models, allowing us to train self-refining planners in a simple\nsupervised learning fashion. Meanwhile, a nested equilibrium sequence modeling\nprocedure is devised for efficient closed-loop planning that incorporates\nuseful feedback from the environment (or an internal world model). Our method\nis evaluated on the VirtualHome-Env benchmark, showing advanced performance\nwith better scaling for inference computation. Code is available at\nhttps://github.com/Singularity0104/equilibrium-planner.",
      "upvotes": 3
    },
    {
      "title": "SonicSim: A customizable simulation platform for speech processing in moving sound source scenarios",
      "url": "https://huggingface.co/papers/2410.01481",
      "authors": [
        "Wendi Sang",
        "Chang Zeng",
        "Runxuan Yang",
        "Guo Chen",
        "Xiaolin Hu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01481.pdf",
      "abstract": "The systematic evaluation of speech separation and enhancement models under\nmoving sound source conditions typically requires extensive data comprising\ndiverse scenarios. However, real-world datasets often contain insufficient data\nto meet the training and evaluation requirements of models. Although synthetic\ndatasets offer a larger volume of data, their acoustic simulations lack\nrealism. Consequently, neither real-world nor synthetic datasets effectively\nfulfill practical needs. To address these issues, we introduce SonicSim, a\nsynthetic toolkit de-designed to generate highly customizable data for moving\nsound sources. SonicSim is developed based on the embodied AI simulation\nplatform, Habitat-sim, supporting multi-level adjustments, including\nscene-level, microphone-level, and source-level, thereby generating more\ndiverse synthetic data. Leveraging SonicSim, we constructed a moving sound\nsource benchmark dataset, SonicSet, using the Librispeech, the Freesound\nDataset 50k (FSD50K) and Free Music Archive (FMA), and 90 scenes from the\nMatterport3D to evaluate speech separation and enhancement models.\nAdditionally, to validate the differences between synthetic data and real-world\ndata, we randomly selected 5 hours of raw data without reverberation from the\nSonicSet validation set to record a real-world speech separation dataset, which\nwas then compared with the corresponding synthetic datasets. Similarly, we\nutilized the real-world speech enhancement dataset RealMAN to validate the\nacoustic gap between other synthetic datasets and the SonicSet dataset for\nspeech enhancement. The results indicate that the synthetic data generated by\nSonicSim can effectively generalize to real-world scenarios. Demo and code are\npublicly available at https://cslikai.cn/SonicSim/.",
      "upvotes": 2
    },
    {
      "title": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs",
      "url": "https://huggingface.co/papers/2410.01518",
      "authors": [
        "Kyuhong Shim",
        "Jungwook Choi",
        "Simyung Chang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01518.pdf",
      "abstract": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.",
      "upvotes": 2
    }
  ]
}