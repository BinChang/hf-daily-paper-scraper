{
  "date": "2024-11-07",
  "papers": [
    {
      "title": "Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level",
      "url": "https://huggingface.co/papers/2411.03562",
      "authors": [
        "Antoine Grosnit",
        "Alexandre Maraval",
        "James Doran",
        "Giuseppe Paolo",
        "Albert Thomas",
        "Refinath Shahul Hameed Nabeezath Beevi",
        "Jonas Gonzalez",
        "Khyati Khandelwal",
        "Ignacio Iacobacci",
        "Abdelhakim Benechehab",
        "Hamza Cherkaoui",
        "Youssef Attia El-Hili",
        "Kun Shao",
        "Jianye Hao",
        "Jun Yao",
        "Balazs Kegl",
        "Haitham Bou-Ammar",
        "Jun Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.03562.pdf",
      "abstract": "We introduce Agent K v1.0, an end-to-end autonomous data science agent\ndesigned to automate, optimise, and generalise across diverse data science\ntasks. Fully automated, Agent K v1.0 manages the entire data science life cycle\nby learning from experience. It leverages a highly flexible structured\nreasoning framework to enable it to dynamically process memory in a nested\nstructure, effectively learning from accumulated experience stored to handle\ncomplex reasoning tasks. It optimises long- and short-term memory by\nselectively storing and retrieving key information, guiding future decisions\nbased on environmental rewards. This iterative approach allows it to refine\ndecisions without fine-tuning or backpropagation, achieving continuous\nimprovement through experiential learning. We evaluate our agent's apabilities\nusing Kaggle competitions as a case study. Following a fully automated\nprotocol, Agent K v1.0 systematically addresses complex and multimodal data\nscience tasks, employing Bayesian optimisation for hyperparameter tuning and\nfeature engineering. Our new evaluation framework rigorously assesses Agent K\nv1.0's end-to-end capabilities to generate and send submissions starting from a\nKaggle competition URL. Results demonstrate that Agent K v1.0 achieves a 92.5\\%\nsuccess rate across tasks, spanning tabular, computer vision, NLP, and\nmultimodal domains. When benchmarking against 5,856 human Kaggle competitors by\ncalculating Elo-MMR scores for each, Agent K v1.0 ranks in the top 38\\%,\ndemonstrating an overall skill level comparable to Expert-level users. Notably,\nits Elo-MMR score falls between the first and third quartiles of scores\nachieved by human Grandmasters. Furthermore, our results indicate that Agent K\nv1.0 has reached a performance level equivalent to Kaggle Grandmaster, with a\nrecord of 6 gold, 3 silver, and 7 bronze medals, as defined by Kaggle's\nprogression system.",
      "upvotes": 47
    },
    {
      "title": "Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination",
      "url": "https://huggingface.co/papers/2411.03823",
      "authors": [
        "Dingjie Song",
        "Sicheng Lai",
        "Shunian Chen",
        "Lichao Sun",
        "Benyou Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.03823.pdf",
      "abstract": "The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting dataset contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is sensitive to varying degrees of contamination and can\nhighlight significant performance improvements due to leakage of the training\nset of multimodal benchmarks. Furthermore, We also explore the possibility of\ncontamination originating from the pre-training phase of LLMs used by MLLMs and\nthe fine-tuning phase of MLLMs, offering new insights into the stages at which\ncontamination may be introduced.",
      "upvotes": 41
    },
    {
      "title": "Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models",
      "url": "https://huggingface.co/papers/2411.03884",
      "authors": [
        "Zhijian Zhuo",
        "Ya Wang",
        "Yutao Zeng",
        "Xiaoqing Li",
        "Xun Zhou",
        "Jinwen Ma"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.03884.pdf",
      "abstract": "Transformers have found extensive applications across various domains due to\nthe powerful fitting capabilities. This success can be partially attributed to\ntheir inherent nonlinearity. Thus, in addition to the ReLU function employed in\nthe original transformer architecture, researchers have explored alternative\nmodules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment\nrepresentational capacity. In this paper, we propose a novel category of\npolynomial composition activations (PolyCom), designed to optimize the dynamics\nof transformers. Theoretically, we provide a comprehensive mathematical\nanalysis of PolyCom, highlighting its enhanced expressivity and efficacy\nrelative to other activation functions. Notably, we demonstrate that networks\nincorporating PolyCom achieve the optimal approximation rate,\nindicating that PolyCom networks require minimal parameters to approximate\ngeneral smooth functions in Sobolev spaces. We conduct empirical experiments on\nthe pre-training configurations of large language models (LLMs), including both\ndense and sparse architectures. By substituting conventional activation\nfunctions with PolyCom, we enable LLMs to capture higher-order interactions\nwithin the data, thus improving performance metrics in terms of accuracy and\nconvergence rates. Extensive experimental results demonstrate the effectiveness\nof our method, showing substantial improvements over other activation\nfunctions. Code is available at https://github.com/BryceZhuo/PolyCom.",
      "upvotes": 19
    },
    {
      "title": "Self-Consistency Preference Optimization",
      "url": "https://huggingface.co/papers/2411.04109",
      "authors": [
        "Archiki Prasad",
        "Weizhe Yuan",
        "Richard Yuanzhe Pang",
        "Jing Xu",
        "Maryam Fazel-Zarandi",
        "Mohit Bansal",
        "Sainbayar Sukhbaatar",
        "Jason Weston",
        "Jane Yu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04109.pdf",
      "abstract": "Self-alignment, whereby models learn to improve themselves without human\nannotation, is a rapidly growing research area. However, existing techniques\noften fail to improve complex reasoning tasks due to the difficulty of\nassigning correct rewards. An orthogonal approach that is known to improve\ncorrectness is self-consistency, a method applied at inference time based on\nmultiple sampling in order to find the most consistent answer. In this work, we\nextend the self-consistency concept to help train models. We thus introduce\nself-consistency preference optimization (ScPO), which iteratively trains\nconsistent answers to be preferred over inconsistent ones on unsupervised new\nproblems. We show ScPO leads to large improvements over conventional reward\nmodel training on reasoning tasks such as GSM8K and MATH, closing the gap with\nsupervised training with gold answers or preferences, and that combining ScPO\nwith standard supervised learning improves results even further. On ZebraLogic,\nScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and\nClaude-3 Haiku.",
      "upvotes": 11
    },
    {
      "title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond",
      "url": "https://huggingface.co/papers/2411.03590",
      "authors": [
        "Naoto Usuyama",
        "Nicholas King",
        "Scott Mayer McKinney",
        "Xavier Fernandes",
        "Sheng Zhang",
        "Eric Horvitz"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.03590.pdf",
      "abstract": "Run-time steering strategies like Medprompt are valuable for guiding large\nlanguage models (LLMs) to top performance on challenging tasks. Medprompt\ndemonstrates that a general LLM can be focused to deliver state-of-the-art\nperformance on specialized domains like medicine by using a prompt to elicit a\nrun-time strategy involving chain of thought reasoning and ensembling. OpenAI's\no1-preview model represents a new paradigm, where a model is designed to do\nrun-time reasoning before generating final responses. We seek to understand the\nbehavior of o1-preview on a diverse set of medical challenge problem\nbenchmarks. Following on the Medprompt study with GPT-4, we systematically\nevaluate the o1-preview model across various medical benchmarks. Notably, even\nwithout prompting techniques, o1-preview largely outperforms the GPT-4 series\nwith Medprompt. We further systematically study the efficacy of classic prompt\nengineering strategies, as represented by Medprompt, within the new paradigm of\nreasoning models. We found that few-shot prompting hinders o1's performance,\nsuggesting that in-context learning may no longer be an effective steering\napproach for reasoning-native models. While ensembling remains viable, it is\nresource-intensive and requires careful cost-performance optimization. Our cost\nand accuracy analysis across run-time strategies reveals a Pareto frontier,\nwith GPT-4o representing a more affordable option and o1-preview achieving\nstate-of-the-art performance at higher cost. Although o1-preview offers top\nperformance, GPT-4o with steering strategies like Medprompt retains value in\nspecific contexts. Moreover, we note that the o1-preview model has reached\nnear-saturation on many existing medical benchmarks, underscoring the need for\nnew, challenging benchmarks. We close with reflections on general directions\nfor inference-time computation with LLMs.",
      "upvotes": 9
    }
  ]
}