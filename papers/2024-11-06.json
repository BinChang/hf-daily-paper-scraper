{
  "date": "2024-11-06",
  "papers": [
    {
      "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems",
      "url": "https://huggingface.co/papers/2411.02959",
      "authors": [
        "Jiejun Tan",
        "Zhicheng Dou",
        "Wen Wang",
        "Mang Wang",
        "Weipeng Chen",
        "Ji-Rong Wen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02959.pdf",
      "abstract": "Retrieval-Augmented Generation (RAG) has been shown to improve knowledge\ncapabilities and alleviate the hallucination problem of LLMs. The Web is a\nmajor source of external knowledge used in RAG systems, and many commercial\nsystems such as ChatGPT and Perplexity have used Web search engines as their\nmajor retrieval systems. Typically, such RAG systems retrieve search results,\ndownload HTML sources of the results, and then extract plain texts from the\nHTML sources. Plain text documents or chunks are fed into the LLMs to augment\nthe generation. However, much of the structural and semantic information\ninherent in HTML, such as headings and table structures, is lost during this\nplain-text-based RAG process. To alleviate this problem, we propose HtmlRAG,\nwhich uses HTML instead of plain text as the format of retrieved knowledge in\nRAG. We believe HTML is better than plain text in modeling knowledge in\nexternal documents, and most LLMs possess robust capacities to understand HTML.\nHowever, utilizing HTML presents new challenges. HTML contains additional\ncontent such as tags, JavaScript, and CSS specifications, which bring extra\ninput tokens and noise to the RAG system. To address this issue, we propose\nHTML cleaning, compression, and pruning strategies, to shorten the HTML while\nminimizing the loss of information. Specifically, we design a two-step\nblock-tree-based pruning method that prunes useless HTML blocks and keeps only\nthe relevant part of the HTML. Experiments on six QA datasets confirm the\nsuperiority of using HTML in RAG systems.",
      "upvotes": 53
    },
    {
      "title": "LLaMo: Large Language Model-based Molecular Graph Assistant",
      "url": "https://huggingface.co/papers/2411.00871",
      "authors": [
        "Jinyoung Park",
        "Minseong Bae",
        "Dohwan Ko",
        "Hyunwoo J. Kim"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00871.pdf",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable generalization and\ninstruction-following capabilities with instruction tuning. The advancements in\nLLMs and instruction tuning have led to the development of Large\nVision-Language Models (LVLMs). However, the competency of the LLMs and\ninstruction tuning have been less explored in the molecular domain. Thus, we\npropose LLaMo: Large Language Model-based Molecular graph assistant, which is\nan end-to-end trained large molecular graph-language model. To bridge the\ndiscrepancy between the language and graph modalities, we present the\nmulti-level graph projector that transforms graph representations into graph\ntokens by abstracting the output representations of each GNN layer and motif\nrepresentations with the cross-attention mechanism. We also introduce\nmachine-generated molecular graph instruction data to instruction-tune the\nlarge molecular graph-language model for general-purpose molecule and language\nunderstanding. Our extensive experiments demonstrate that LLaMo shows the best\nperformance on diverse tasks, such as molecular description generation,\nproperty prediction, and IUPAC name prediction. The code of LLaMo is available\nat https://github.com/mlvlab/LLaMo.",
      "upvotes": 20
    },
    {
      "title": "Controlling Language and Diffusion Models by Transporting Activations",
      "url": "https://huggingface.co/papers/2410.23054",
      "authors": [
        "Pau Rodriguez",
        "Arno Blaas",
        "Michal Klein",
        "Luca Zappella",
        "Nicholas Apostoloff",
        "Marco Cuturi",
        "Xavier Suau"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.23054.pdf",
      "abstract": "The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation.",
      "upvotes": 15
    },
    {
      "title": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution",
      "url": "https://huggingface.co/papers/2411.02359",
      "authors": [
        "Yang Yue",
        "Yulin Wang",
        "Bingyi Kang",
        "Yizeng Han",
        "Shenzhi Wang",
        "Shiji Song",
        "Jiashi Feng",
        "Gao Huang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02359.pdf",
      "abstract": "MLLMs have demonstrated remarkable comprehension and reasoning capabilities\nwith complex language and visual data. These advances have spurred the vision\nof establishing a generalist robotic MLLM proficient in understanding complex\nhuman instructions and accomplishing various embodied tasks. However,\ndeveloping MLLMs for real-world robots is challenging due to the typically\nlimited computation and memory capacities available on robotic platforms. In\ncontrast, the inference of MLLMs involves storing billions of parameters and\nperforming tremendous computation, imposing significant hardware demands. In\nour paper, we propose a Dynamic Early-Exit Framework for Robotic\nVision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically\nadjusts the size of the activated MLLM based on each situation at hand. The\napproach leverages a multi-exit architecture in MLLMs, which allows the model\nto terminate processing once a proper size of the model has been activated for\na specific situation, thus avoiding further redundant computation.\nAdditionally, we develop novel algorithms that establish early-termination\ncriteria for DeeR, conditioned on predefined demands such as average\ncomputational cost (i.e., power consumption), as well as peak computational\nconsumption (i.e., latency) and GPU memory usage. These enhancements ensure\nthat DeeR operates efficiently under varying resource constraints while\nmaintaining competitive performance. On the CALVIN robot manipulation\nbenchmark, DeeR demonstrates significant reductions in computational costs of\nLLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance.\nCode and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.",
      "upvotes": 12
    },
    {
      "title": "Adaptive Length Image Tokenization via Recurrent Allocation",
      "url": "https://huggingface.co/papers/2411.02393",
      "authors": [
        "Shivam Duggal",
        "Phillip Isola",
        "Antonio Torralba",
        "William T. Freeman"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02393.pdf",
      "abstract": "Current vision systems typically assign fixed-length representations to\nimages, regardless of the information content. This contrasts with human\nintelligence - and even large language models - which allocate varying\nrepresentational capacities based on entropy, context and familiarity. Inspired\nby this, we propose an approach to learn variable-length token representations\nfor 2D images. Our encoder-decoder architecture recursively processes 2D image\ntokens, distilling them into 1D latent tokens over multiple iterations of\nrecurrent rollouts. Each iteration refines the 2D tokens, updates the existing\n1D latent tokens, and adaptively increases representational capacity by adding\nnew tokens. This enables compression of images into a variable number of\ntokens, ranging from 32 to 256. We validate our tokenizer using reconstruction\nloss and FID metrics, demonstrating that token count aligns with image entropy,\nfamiliarity and downstream task requirements. Recurrent token processing with\nincreasing representational capacity in each iteration shows signs of token\nspecialization, revealing potential for object / part discovery.",
      "upvotes": 11
    },
    {
      "title": "Sample-Efficient Alignment for LLMs",
      "url": "https://huggingface.co/papers/2411.01493",
      "authors": [
        "Zichen Liu",
        "Changyu Chen",
        "Chao Du",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01493.pdf",
      "abstract": "We study methods for efficiently aligning large language models (LLMs) with\nhuman preferences given budgeted online feedback. We first formulate the LLM\nalignment problem in the frame of contextual dueling bandits. This formulation,\nsubsuming recent paradigms such as online RLHF and online DPO, inherently\nquests for sample-efficient algorithms that incorporate online active\nexploration. Leveraging insights from bandit theory, we introduce a unified\nalgorithm based on Thompson sampling and highlight its applications in two\ndistinct LLM alignment scenarios. The practical agent that efficiently\nimplements this algorithm, named SEA (Sample-Efficient Alignment), is\nempirically validated through extensive experiments across three model scales\n(1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The\nresults demonstrate that SEA achieves highly sample-efficient alignment with\noracle's preferences, outperforming recent active exploration methods for LLMs.\nAdditionally, we release the implementation of SEA together with an efficient\ncodebase designed for online alignment of LLMs, aiming to accelerate future\nresearch in this field.",
      "upvotes": 10
    },
    {
      "title": "DreamPolish: Domain Score Distillation With Progressive Geometry Generation",
      "url": "https://huggingface.co/papers/2411.01602",
      "authors": [
        "Yean Cheng",
        "Ming Ding",
        "Wendi Zheng",
        "Shiyu Huang",
        "Yuxiao Dong",
        "Jie Tang",
        "Boxin Shi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01602.pdf",
      "abstract": "We introduce DreamPolish, a text-to-3D generation model that excels in\nproducing refined geometry and high-quality textures. In the geometry\nconstruction phase, our approach leverages multiple neural representations to\nenhance the stability of the synthesis process. Instead of relying solely on a\nview-conditioned diffusion prior in the novel sampled views, which often leads\nto undesired artifacts in the geometric surface, we incorporate an additional\nnormal estimator to polish the geometry details, conditioned on viewpoints with\nvarying field-of-views. We propose to add a surface polishing stage with only a\nfew training steps, which can effectively refine the artifacts attributed to\nlimited guidance from previous stages and produce 3D objects with more\ndesirable geometry. The key topic of texture generation using pretrained\ntext-to-image models is to find a suitable domain in the vast latent\ndistribution of these models that contains photorealistic and consistent\nrenderings. In the texture generation phase, we introduce a novel score\ndistillation objective, namely domain score distillation (DSD), to guide neural\nrepresentations toward such a domain. We draw inspiration from the\nclassifier-free guidance (CFG) in textconditioned image generation tasks and\nshow that CFG and variational distribution guidance represent distinct aspects\nin gradient guidance and are both imperative domains for the enhancement of\ntexture quality. Extensive experiments show our proposed model can produce 3D\nassets with polished surfaces and photorealistic textures, outperforming\nexisting state-of-the-art methods.",
      "upvotes": 9
    },
    {
      "title": "Inference Optimal VLMs Need Only One Visual Token but Larger Models",
      "url": "https://huggingface.co/papers/2411.03312",
      "authors": [
        "Kevin Y. Li",
        "Sachin Goyal",
        "Joao D. Semedo",
        "J. Zico Kolter"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.03312.pdf",
      "abstract": "Vision Language Models (VLMs) have demonstrated strong capabilities across\nvarious visual understanding and reasoning tasks. However, their real-world\ndeployment is often constrained by high latency during inference due to\nsubstantial compute required to process the large number of input tokens\n(predominantly from the image) by the LLM. To reduce inference costs, one can\neither downsize the LLM or reduce the number of input image-tokens, the latter\nof which has been the focus of many recent works around token compression.\nHowever, it is unclear what the optimal trade-off is, as both the factors\ndirectly affect the VLM performance. We first characterize this optimal\ntrade-off between the number of visual tokens and LLM parameters by\nestablishing scaling laws that capture variations in performance with these two\nfactors. Our results reveal a surprising trend: for visual reasoning tasks, the\ninference-optimal behavior in VLMs, i.e., minimum downstream error at any given\nfixed inference compute, is achieved when using the largest LLM that fits\nwithin the inference budget while minimizing visual token count - often to a\nsingle token. While the token reduction literature has mainly focused on\nmaintaining base model performance by modestly reducing the token count (e.g.,\n5-10times), our results indicate that the compute-optimal inference regime\nrequires operating under even higher token compression ratios. Based on these\ninsights, we take some initial steps towards building approaches tailored for\nhigh token compression settings. Code is available at\nhttps://github.com/locuslab/llava-token-compression.",
      "upvotes": 6
    },
    {
      "title": "GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details",
      "url": "https://huggingface.co/papers/2411.03047",
      "authors": [
        "Zhongjin Luo",
        "Haolin Liu",
        "Chenghong Li",
        "Wanghao Du",
        "Zirong Jin",
        "Wanhu Sun",
        "Yinyu Nie",
        "Weikai Chen",
        "Xiaoguang Han"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.03047.pdf",
      "abstract": "Neural implicit functions have brought impressive advances to the\nstate-of-the-art of clothed human digitization from multiple or even single\nimages. However, despite the progress, current arts still have difficulty\ngeneralizing to unseen images with complex cloth deformation and body poses. In\nthis work, we present GarVerseLOD, a new dataset and framework that paves the\nway to achieving unprecedented robustness in high-fidelity 3D garment\nreconstruction from a single unconstrained image. Inspired by the recent\nsuccess of large generative models, we believe that one key to addressing the\ngeneralization challenge lies in the quantity and quality of 3D garment data.\nTowards this end, GarVerseLOD collects 6,000 high-quality cloth models with\nfine-grained geometry details manually created by professional artists. In\naddition to the scale of training data, we observe that having disentangled\ngranularities of geometry can play an important role in boosting the\ngeneralization capability and inference accuracy of the learned model. We hence\ncraft GarVerseLOD as a hierarchical dataset with levels of details (LOD),\nspanning from detail-free stylized shape to pose-blended garment with\npixel-aligned details. This allows us to make this highly under-constrained\nproblem tractable by factorizing the inference into easier tasks, each narrowed\ndown with smaller searching space. To ensure GarVerseLOD can generalize well to\nin-the-wild images, we propose a novel labeling paradigm based on conditional\ndiffusion models to generate extensive paired images for each garment model\nwith high photorealism. We evaluate our method on a massive amount of\nin-the-wild images. Experimental results demonstrate that GarVerseLOD can\ngenerate standalone garment pieces with significantly better quality than prior\napproaches. Project page: https://garverselod.github.io/",
      "upvotes": 6
    },
    {
      "title": "Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge",
      "url": "https://huggingface.co/papers/2411.02657",
      "authors": [
        "Andrew Langdon",
        "Catalina Villouta",
        "Chinmay Agrawal",
        "Lashaw Salta",
        "Braian Peetoom",
        "Gianmarco Bellucci",
        "Orion J Buske"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02657.pdf",
      "abstract": "Rare diseases present unique challenges in healthcare, often suffering from\ndelayed diagnosis and fragmented information landscapes. The scarcity of\nreliable knowledge in these conditions poses a distinct challenge for Large\nLanguage Models (LLMs) in supporting clinical management and delivering precise\npatient information underscoring the need for focused training on these 'zebra'\ncases. We present Zebra-Llama, a specialized context-aware language model with\nhigh precision Retrieval Augmented Generation (RAG) capability, focusing on\nEhlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000\nindividuals, exemplifies the complexities of rare diseases with its diverse\nsymptoms, multiple subtypes, and evolving diagnostic criteria. By implementing\na novel context-aware fine-tuning methodology trained on questions derived from\nmedical literature, patient experiences, and clinical resources, along with\nexpertly curated responses, Zebra-Llama demonstrates unprecedented capabilities\nin handling EDS-related queries. On a test set of real-world questions\ncollected from EDS patients and clinicians, medical experts evaluated the\nresponses generated by both models, revealing Zebra-Llama's substantial\nimprovements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs.\n70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation\nreliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama\nnot only provides more accessible and reliable EDS information but also\nestablishes a framework for developing specialized AI solutions for other rare\nconditions. This work represents a crucial step towards democratizing\nexpert-level knowledge in rare disease management, potentially transforming how\nhealthcare providers and patients navigate the complex landscape of rare\ndiseases.",
      "upvotes": 5
    },
    {
      "title": "Correlation of Object Detection Performance with Visual Saliency and Depth Estimation",
      "url": "https://huggingface.co/papers/2411.02844",
      "authors": [
        "Dylan Seychell"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02844.pdf",
      "abstract": "As object detection techniques continue to evolve, understanding their\nrelationships with complementary visual tasks becomes crucial for optimising\nmodel architectures and computational resources. This paper investigates the\ncorrelations between object detection accuracy and two fundamental visual\ntasks: depth prediction and visual saliency prediction. Through comprehensive\nexperiments using state-of-the-art models (DeepGaze IIE, Depth Anything,\nDPT-Large, and Itti's model) on COCO and Pascal VOC datasets, we find that\nvisual saliency shows consistently stronger correlations with object detection\naccuracy (mArho up to 0.459 on Pascal VOC) compared to depth prediction\n(mArho up to 0.283). Our analysis reveals significant variations in these\ncorrelations across object categories, with larger objects showing correlation\nvalues up to three times higher than smaller objects. These findings suggest\nincorporating visual saliency features into object detection architectures\ncould be more beneficial than depth information, particularly for specific\nobject categories. The observed category-specific variations also provide\ninsights for targeted feature engineering and dataset design improvements,\npotentially leading to more efficient and accurate object detection systems.",
      "upvotes": 3
    }
  ]
}