{
  "date": "2024-07-03",
  "papers": [
    {
      "title": "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems",
      "url": "https://huggingface.co/papers/2407.01370",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2407.01370.pdf",
      "abstract": "LLMs and RAG systems are now capable of handling millions of input tokens or\nmore. However, evaluating the output quality of such systems on long-context\ntasks remains challenging, as tasks like Needle-in-a-Haystack lack complexity.\nIn this work, we argue that summarization can play a central role in such\nevaluation. We design a procedure to synthesize Haystacks of documents,\nensuring that specific insights repeat across documents. The \"Summary\nof a Haystack\" (SummHay) task then requires a system to process the Haystack\nand generate, given a query, a summary that identifies the relevant insights\nand precisely cites the source documents. Since we have precise knowledge of\nwhat insights should appear in a haystack summary and what documents should be\ncited, we implement a highly reproducible automatic evaluation that can score\nsummaries on two aspects - Coverage and Citation. We generate Haystacks in two\ndomains (conversation, news), and perform a large-scale evaluation of 10 LLMs\nand corresponding 50 RAG systems. Our findings indicate that SummHay is an open\nchallenge for current systems, as even systems provided with an Oracle signal\nof document relevance lag our estimate of human performance (56\\%) by 10+\npoints on a Joint Score. Without a retriever, long-context LLMs like GPT-4o and\nClaude 3 Opus score below 20% on SummHay. We show SummHay can also be used to\nstudy enterprise RAG systems and position bias in long-context models. We hope\nfuture systems can equal and surpass human performance on SummHay.",
      "upvotes": 85
    },
    {
      "title": "OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation",
      "url": "https://huggingface.co/papers/2407.02371",
      "authors": [
        "Xiang Li",
        "Jian Yang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.02371.pdf",
      "abstract": "Text-to-video (T2V) generation has recently garnered significant attention\nthanks to the large multi-modality model Sora. However, T2V generation still\nfaces two important challenges: 1) Lacking a precise open sourced high-quality\ndataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M,\nare either with low quality or too large for most research institutions.\nTherefore, it is challenging but crucial to collect a precise high-quality\ntext-video pairs for T2V generation. 2) Ignoring to fully utilize textual\ninformation. Recent T2V methods have focused on vision transformers, using a\nsimple cross attention module for video generation, which falls short of\nthoroughly extracting semantic information from text prompt. To address these\nissues, we introduce OpenVid-1M, a precise high-quality dataset with expressive\ncaptions. This open-scenario dataset contains over 1 million text-video pairs,\nfacilitating research on T2V generation. Furthermore, we curate 433K 1080p\nvideos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definition\nvideo generation. Additionally, we propose a novel Multi-modal Video Diffusion\nTransformer (MVDiT) capable of mining both structure information from visual\ntokens and semantic information from text tokens. Extensive experiments and\nablation studies verify the superiority of OpenVid-1M over previous datasets\nand the effectiveness of our MVDiT.",
      "upvotes": 49
    },
    {
      "title": "Agentless: Demystifying LLM-based Software Engineering Agents",
      "url": "https://huggingface.co/papers/2407.01489",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2407.01489.pdf",
      "abstract": "Recent advancements in large language models (LLMs) have significantly\nadvanced the automation of software development tasks, including code\nsynthesis, program repair, and test generation. More recently, researchers and\nindustry practitioners have developed various autonomous LLM agents to perform\nend-to-end software development tasks. These agents are equipped with the\nability to use tools, run commands, observe feedback from the environment, and\nplan for future actions. However, the complexity of these agent-based\napproaches, together with the limited abilities of current LLMs, raises the\nfollowing question: Do we really have to employ complex autonomous software\nagents? To attempt to answer this question, we build Agentless -- an agentless\napproach to automatically solve software development problems. Compared to the\nverbose and complex setup of agent-based approaches, Agentless employs a\nsimplistic two-phase process of localization followed by repair, without\nletting the LLM decide future actions or operate with complex tools. Our\nresults on the popular SWE-bench Lite benchmark show that surprisingly the\nsimplistic Agentless is able to achieve both the highest performance (27.33%)\nand lowest cost (\\$0.34) compared with all existing open-source software\nagents! Furthermore, we manually classified the problems in SWE-bench Lite and\nfound problems with exact ground truth patch or insufficient/misleading issue\ndescriptions. As such, we construct SWE-bench Lite-S by excluding such\nproblematic issues to perform more rigorous evaluation and comparison. Our work\nhighlights the current overlooked potential of a simple, interpretable\ntechnique in autonomous software development. We hope Agentless will help reset\nthe baseline, starting point, and horizon for autonomous software agents, and\ninspire future work along this crucial direction.",
      "upvotes": 42
    },
    {
      "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
      "url": "https://huggingface.co/papers/2407.02490",
      "authors": [
        "Surin Ahn",
        "Amir H. Abdi",
        "Dongsheng Li",
        "Chin-Yew Lin",
        "Yuqing Yang",
        "Lili Qiu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.02490.pdf",
      "abstract": "The computational challenges of Large Language Model (LLM) inference remain a\nsignificant barrier to their widespread deployment, especially as prompt\nlengths continue to increase. Due to the quadratic complexity of the attention\ncomputation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens\n(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for\nspeeding up prefilling often fail to maintain acceptable accuracy or efficiency\nwhen applied to long-context LLMs. To address this gap, we introduce MInference\n(Milliontokens Inference), a sparse calculation method designed to accelerate\npre-filling of long-sequence processing. Specifically, we identify three unique\npatterns in long-context attention matrices-the A-shape, Vertical-Slash, and\nBlock-Sparsethat can be leveraged for efficient sparse computation on GPUs. We\ndetermine the optimal pattern for each attention head offline and dynamically\nbuild sparse indices based on the assigned pattern during inference. With the\npattern and sparse indices, we perform efficient sparse attention calculations\nvia our optimized GPU kernels to significantly reduce the latency in the\npre-filling stage of long-context LLMs. Our proposed technique can be directly\napplied to existing LLMs without any modifications to the pre-training setup or\nadditional fine-tuning. By evaluating on a wide range of downstream tasks,\nincluding InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models\nincluding LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we\ndemonstrate that MInference effectively reduces inference latency by up to 10x\nfor pre-filling on an A100, while maintaining accuracy. Our code is available\nat https://aka.ms/MInference.",
      "upvotes": 23
    },
    {
      "title": "Understanding Alignment in Multimodal LLMs: A Comprehensive Study",
      "url": "https://huggingface.co/papers/2407.02477",
      "authors": [
        "Elmira Amirloo",
        "Christian Kerl",
        "Rinu Boney",
        "Zirui Wang",
        "Afshin Dehghan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.02477.pdf",
      "abstract": "Preference alignment has become a crucial component in enhancing the\nperformance of Large Language Models (LLMs), yet its impact in Multimodal Large\nLanguage Models (MLLMs) remains comparatively underexplored. Similar to\nlanguage models, MLLMs for image understanding tasks encounter challenges like\nhallucination. In MLLMs, hallucination can occur not only by stating incorrect\nfacts but also by producing responses that are inconsistent with the image\ncontent. A primary objective of alignment for MLLMs is to encourage these\nmodels to align responses more closely with image information. Recently,\nmultiple works have introduced preference datasets for MLLMs and examined\ndifferent alignment methods, including Direct Preference Optimization (DPO) and\nProximal Policy Optimization (PPO). However, due to variations in datasets,\nbase model types, and alignment methods, it remains unclear which specific\nelements contribute most significantly to the reported improvements in these\nworks. In this paper, we independently analyze each aspect of preference\nalignment in MLLMs. We start by categorizing the alignment algorithms into two\ngroups, offline (such as DPO), and online (such as online-DPO), and show that\ncombining offline and online methods can improve the performance of the model\nin certain scenarios. We review a variety of published multimodal preference\ndatasets and discuss how the details of their construction impact model\nperformance. Based on these insights, we introduce a novel way of creating\nmultimodal preference data called Bias-Driven Hallucination Sampling (BDHS)\nthat needs neither additional annotation nor external models, and show that it\ncan achieve competitive performance to previously published alignment work for\nmultimodal models across a range of benchmarks.",
      "upvotes": 21
    },
    {
      "title": "Magic Insert: Style-Aware Drag-and-Drop",
      "url": "https://huggingface.co/papers/2407.02489",
      "authors": [
        "Yuanzhen Li",
        "Neal Wadhwa",
        "Yael Pritch",
        "Michael Rubinstein",
        "David E. Jacobs",
        "Shlomi Fruchter"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.02489.pdf",
      "abstract": "We present Magic Insert, a method for dragging-and-dropping subjects from a\nuser-provided image into a target image of a different style in a physically\nplausible manner while matching the style of the target image. This work\nformalizes the problem of style-aware drag-and-drop and presents a method for\ntackling it by addressing two sub-problems: style-aware personalization and\nrealistic object insertion in stylized images. For style-aware personalization,\nour method first fine-tunes a pretrained text-to-image diffusion model using\nLoRA and learned text tokens on the subject image, and then infuses it with a\nCLIP representation of the target style. For object insertion, we use\nBootstrapped Domain Adaption to adapt a domain-specific photorealistic object\ninsertion model to the domain of diverse artistic styles. Overall, the method\nsignificantly outperforms traditional approaches such as inpainting. Finally,\nwe present a dataset, SubjectPlop, to facilitate evaluation and future progress\nin this area. Project page: https://magicinsert.github.io/",
      "upvotes": 20
    },
    {
      "title": "Revealing Fine-Grained Values and Opinions in Large Language Models",
      "url": "https://huggingface.co/papers/2406.19238",
      "authors": [
        "Serge Belongie"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.19238.pdf",
      "abstract": "Uncovering latent values and opinions in large language models (LLMs) can\nhelp identify biases and mitigate potential harm. Recently, this has been\napproached by presenting LLMs with survey questions and quantifying their\nstances towards morally and politically charged statements. However, the\nstances generated by LLMs can vary greatly depending on how they are prompted,\nand there are many ways to argue for or against a given position. In this work,\nwe propose to address this by analysing a large and robust dataset of 156k LLM\nresponses to the 62 propositions of the Political Compass Test (PCT) generated\nby 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of\ntheir generated stances and fine-grained analysis of the plain text\njustifications for those stances. For fine-grained analysis, we propose to\nidentify tropes in the responses: semantically similar phrases that are\nrecurrent and consistent across different prompts, revealing patterns in the\ntext that a given LLM is prone to produce. We find that demographic features\nadded to prompts significantly affect outcomes on the PCT, reflecting bias, as\nwell as disparities between the results of tests when eliciting closed-form vs.\nopen domain responses. Additionally, patterns in the plain text rationales via\ntropes show that similar justifications are repeatedly generated across models\nand prompts even with disparate stances.",
      "upvotes": 14
    },
    {
      "title": "Consistency Flow Matching: Defining Straight Flows with Velocity Consistency",
      "url": "https://huggingface.co/papers/2407.02398",
      "authors": [
        "Zixiang Zhang",
        "Wentao Zhang",
        "Chenlin Meng",
        "Bin Cui"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.02398.pdf",
      "abstract": "Flow matching (FM) is a general framework for defining probability paths via\nOrdinary Differential Equations (ODEs) to transform between noise and data\nsamples. Recent approaches attempt to straighten these flow trajectories to\ngenerate high-quality samples with fewer function evaluations, typically\nthrough iterative rectification methods or optimal transport solutions. In this\npaper, we introduce Consistency Flow Matching (Consistency-FM), a novel FM\nmethod that explicitly enforces self-consistency in the velocity field.\nConsistency-FM directly defines straight flows starting from different times to\nthe same endpoint, imposing constraints on their velocity values. Additionally,\nwe propose a multi-segment training approach for Consistency-FM to enhance\nexpressiveness, achieving a better trade-off between sampling quality and\nspeed. Preliminary experiments demonstrate that our Consistency-FM\nsignificantly improves training efficiency by converging 4.4x faster than\nconsistency models and 1.7x faster than rectified flow models while achieving\nbetter generation quality. Our code is available at:\nhttps://github.com/YangLing0818/consistency_flow_matching",
      "upvotes": 14
    },
    {
      "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
      "url": "https://huggingface.co/papers/2407.01920",
      "authors": [
        "Qingbin Liu",
        "Xi Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01920.pdf",
      "abstract": "Large Language Models (LLMs) trained on extensive corpora inevitably retain\nsensitive data, such as personal privacy information and copyrighted material.\nRecent advancements in knowledge unlearning involve updating LLM parameters to\nerase specific knowledge. However, current unlearning paradigms are mired in\nvague forgetting boundaries, often erasing knowledge indiscriminately. In this\nwork, we introduce KnowUnDo, a benchmark containing copyrighted content and\nuser privacy domains to evaluate if the unlearning process inadvertently erases\nessential knowledge. Our findings indicate that existing unlearning methods\noften suffer from excessive unlearning. To address this, we propose a simple\nyet effective method, MemFlex, which utilizes gradient information to precisely\ntarget and unlearn sensitive parameters. Experimental results show that MemFlex\nis superior to existing methods in both precise knowledge unlearning and\ngeneral knowledge retaining of LLMs. Code and dataset will be released at\nhttps://github.com/zjunlp/KnowUnDo.",
      "upvotes": 13
    },
    {
      "title": "What Matters in Detecting AI-Generated Videos like Sora?",
      "url": "https://huggingface.co/papers/2406.19568",
      "authors": [
        "Zhengzhe Liu",
        "Xiaoyang Lyu",
        "Xiaojuan Qi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.19568.pdf",
      "abstract": "Recent advancements in diffusion-based video generation have showcased\nremarkable results, yet the gap between synthetic and real-world videos remains\nunder-explored. In this study, we examine this gap from three fundamental\nperspectives: appearance, motion, and geometry, comparing real-world videos\nwith those generated by a state-of-the-art AI model, Stable Video Diffusion. To\nachieve this, we train three classifiers using 3D convolutional networks, each\ntargeting distinct aspects: vision foundation model features for appearance,\noptical flow for motion, and monocular depth for geometry. Each classifier\nexhibits strong performance in fake video detection, both qualitatively and\nquantitatively. This indicates that AI-generated videos are still easily\ndetectable, and a significant gap between real and fake videos persists.\nFurthermore, utilizing the Grad-CAM, we pinpoint systematic failures of\nAI-generated videos in appearance, motion, and geometry. Finally, we propose an\nEnsemble-of-Experts model that integrates appearance, optical flow, and depth\ninformation for fake video detection, resulting in enhanced robustness and\ngeneralization ability. Our model is capable of detecting videos generated by\nSora with high accuracy, even without exposure to any Sora videos during\ntraining. This suggests that the gap between real and fake videos can be\ngeneralized across various video generative models. Project page:\nhttps://justin-crchang.github.io/3DCNNDetection.github.io/",
      "upvotes": 13
    },
    {
      "title": "FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds",
      "url": "https://huggingface.co/papers/2407.01494",
      "authors": [
        "Yicheng Gu",
        "Kai Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01494.pdf",
      "abstract": "We study Neural Foley, the automatic generation of high-quality sound effects\nsynchronizing with videos, enabling an immersive audio-visual experience.\nDespite its wide range of applications, existing approaches encounter\nlimitations when it comes to simultaneously synthesizing high-quality and\nvideo-aligned (i.e.,, semantic relevant and temporal synchronized) sounds. To\novercome these limitations, we propose FoleyCrafter, a novel framework that\nleverages a pre-trained text-to-audio model to ensure high-quality audio\ngeneration. FoleyCrafter comprises two key components: the semantic adapter for\nsemantic alignment and the temporal controller for precise audio-video\nsynchronization. The semantic adapter utilizes parallel cross-attention layers\nto condition audio generation on video features, producing realistic sound\neffects that are semantically relevant to the visual content. Meanwhile, the\ntemporal controller incorporates an onset detector and a timestampbased adapter\nto achieve precise audio-video alignment. One notable advantage of FoleyCrafter\nis its compatibility with text prompts, enabling the use of text descriptions\nto achieve controllable and diverse video-to-audio generation according to user\nintents. We conduct extensive quantitative and qualitative experiments on\nstandard benchmarks to verify the effectiveness of FoleyCrafter. Models and\ncodes are available at https://github.com/open-mmlab/FoleyCrafter.",
      "upvotes": 13
    },
    {
      "title": "Î¼-Bench: A Vision-Language Benchmark for Microscopy Understanding",
      "url": "https://huggingface.co/papers/2407.01791",
      "authors": [
        "Serena Yeung-Levy"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01791.pdf",
      "abstract": "Recent advances in microscopy have enabled the rapid generation of terabytes\nof image data in cell biology and biomedical research. Vision-language models\n(VLMs) offer a promising solution for large-scale biological image analysis,\nenhancing researchers' efficiency, identifying new image biomarkers, and\naccelerating hypothesis generation and scientific discovery. However, there is\na lack of standardized, diverse, and large-scale vision-language benchmarks to\nevaluate VLMs' perception and cognition capabilities in biological image\nunderstanding. To address this gap, we introduce {\\mu}-Bench, an expert-curated\nbenchmark encompassing 22 biomedical tasks across various scientific\ndisciplines (biology, pathology), microscopy modalities (electron,\nfluorescence, light), scales (subcellular, cellular, tissue), and organisms in\nboth normal and abnormal states. We evaluate state-of-the-art biomedical,\npathology, and general VLMs on {\\mu}-Bench and find that: i) current models\nstruggle on all categories, even for basic tasks such as distinguishing\nmicroscopy modalities; ii) current specialist models fine-tuned on biomedical\ndata often perform worse than generalist models; iii) fine-tuning in specific\nmicroscopy domains can cause catastrophic forgetting, eroding prior biomedical\nknowledge encoded in their base model. iv) weight interpolation between\nfine-tuned and pre-trained models offers one solution to forgetting and\nimproves general performance across biomedical tasks. We release {\\mu}-Bench\nunder a permissive license to accelerate the research and development of\nmicroscopy foundation models.",
      "upvotes": 5
    }
  ]
}