{
  "date": "2024-07-09",
  "papers": [
    {
      "title": "MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?",
      "url": "https://huggingface.co/papers/2407.04842",
      "authors": [
        "Zhengwei Tong",
        "Jiawei Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.04842.pdf",
      "abstract": "While text-to-image models like DALLE-3 and Stable Diffusion are rapidly\nproliferating, they often encounter challenges such as hallucination, bias, and\nthe production of unsafe, low-quality output. To effectively address these\nissues, it is crucial to align these models with desired behaviors based on\nfeedback from a multimodal judge. Despite their significance, current\nmultimodal judges frequently undergo inadequate evaluation of their\ncapabilities and limitations, potentially leading to misalignment and unsafe\nfine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel\nbenchmark which incorporates a comprehensive preference dataset to evaluate\nmultimodal judges in providing feedback for image generation models across four\nkey perspectives: alignment, safety, image quality, and bias. Specifically, we\nevaluate a large variety of multimodal judges including smaller-sized\nCLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and\nclose-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our\npreference dataset. Experiments reveal that close-source VLMs generally provide\nbetter feedback, with GPT-4o outperforming other judges in average. Compared\nwith open-source VLMs, smaller-sized scoring models can provide better feedback\nregarding text-image alignment and image quality, while VLMs provide more\naccurate feedback regarding safety and generation bias due to their stronger\nreasoning capabilities. Further studies in feedback scale reveal that VLM\njudges can generally provide more accurate and stable feedback in natural\nlanguage (Likert-scale) than numerical scales. Notably, human evaluations on\nend-to-end fine-tuned models using separate feedback from these multimodal\njudges provide similar conclusions, further confirming the effectiveness of\nMJ-Bench. All data, code, models are available at\nhttps://huggingface.co/MJ-Bench.",
      "upvotes": 52
    },
    {
      "title": "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages",
      "url": "https://huggingface.co/papers/2407.05975",
      "authors": [
        "Yinquan Lu",
        "Lei Li",
        "Yu Qiao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05975.pdf",
      "abstract": "Large Language Models~(LLMs) demonstrate remarkable translation capabilities\nin high-resource language tasks, yet their performance in low-resource\nlanguages is hindered by insufficient multilingual data during pre-training. To\naddress this, we dedicate 35,000 A100-SXM4-80GB GPU hours in conducting\nextensive multilingual continual pre-training on the LLaMA series models,\nenabling translation support across more than 100 languages. Through a\ncomprehensive analysis of training strategies, such as vocabulary expansion and\ndata augmentation, we develop LLaMAX. Remarkably, without sacrificing its\ngeneralization ability, LLaMAX achieves significantly higher translation\nperformance compared to existing open-source LLMs~(by more than 10 spBLEU\npoints) and performs on-par with specialized translation model~(M2M-100-12B) on\nthe Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve\nas a robust multilingual foundation model. The\ncode~\\url{https://github.com/CONE-MT/LLaMAX/.} and\nmodels~\\url{https://huggingface.co/LLaMAX/.} are publicly available.",
      "upvotes": 34
    },
    {
      "title": "Associative Recurrent Memory Transformer",
      "url": "https://huggingface.co/papers/2407.04841",
      "authors": [
        "Aydar Bulatov"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.04841.pdf",
      "abstract": "This paper addresses the challenge of creating a neural architecture for very\nlong sequences that requires constant time for processing new information at\neach time step. Our approach, Associative Recurrent Memory Transformer (ARMT),\nis based on transformer self-attention for local context and segment-level\nrecurrence for storage of task specific information distributed over a long\ncontext. We demonstrate that ARMT outperfors existing alternatives in\nassociative retrieval tasks and sets a new performance record in the recent\nBABILong multi-task long-context benchmark by answering single-fact questions\nover 50 million tokens with an accuracy of 79.9%. The source code for training\nand evaluation is available on github.",
      "upvotes": 31
    },
    {
      "title": "Learning Action and Reasoning-Centric Image Editing from Videos and Simulations",
      "url": "https://huggingface.co/papers/2407.03471",
      "authors": [
        "Eva Portelance",
        "Christopher Pal",
        "Siva Reddy"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03471.pdf",
      "abstract": "An image editing model should be able to perform diverse edits, ranging from\nobject replacement, changing attributes or style, to performing actions or\nmovement, which require many forms of reasoning. Current general\ninstruction-guided editing models have significant shortcomings with action and\nreasoning-centric edits. Object, attribute or stylistic changes can be learned\nfrom visually static datasets. On the other hand, high-quality data for action\nand reasoning-centric edits is scarce and has to come from entirely different\nsources that cover e.g. physical dynamics, temporality and spatial reasoning.\nTo this end, we meticulously curate the AURORA Dataset\n(Action-Reasoning-Object-Attribute), a collection of high-quality training\ndata, human-annotated and curated from videos and simulation engines. We focus\non a key aspect of quality training data: triplets (source image, prompt,\ntarget image) contain a single meaningful visual change described by the\nprompt, i.e., truly minimal changes between source and target images. To\ndemonstrate the value of our dataset, we evaluate an AURORA-finetuned model on\na new expert-curated benchmark (AURORA-Bench) covering 8 diverse editing tasks.\nOur model significantly outperforms previous editing models as judged by human\nraters. For automatic evaluations, we find important flaws in previous metrics\nand caution their use for semantically hard editing tasks. Instead, we propose\na new automatic metric that focuses on discriminative understanding. We hope\nthat our efforts : (1) curating a quality training dataset and an evaluation\nbenchmark, (2) developing critical evaluations, and (3) releasing a\nstate-of-the-art model, will fuel further progress on general image editing.",
      "upvotes": 28
    },
    {
      "title": "ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation",
      "url": "https://huggingface.co/papers/2407.06135",
      "authors": [
        "Yan Ma"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06135.pdf",
      "abstract": "Previous open-source large multimodal models (LMMs) have faced several\nlimitations: (1) they often lack native integration, requiring adapters to\nalign visual representations with pre-trained large language models (LLMs); (2)\nmany are restricted to single-modal generation; (3) while some support\nmultimodal generation, they rely on separate diffusion models for visual\nmodeling and generation. To mitigate these limitations, we present Anole, an\nopen, autoregressive, native large multimodal model for interleaved image-text\ngeneration. We build Anole from Meta AI's Chameleon, adopting an innovative\nfine-tuning strategy that is both data-efficient and parameter-efficient. Anole\ndemonstrates high-quality, coherent multimodal generation capabilities. We have\nopen-sourced our model, training framework, and instruction tuning data.",
      "upvotes": 20
    },
    {
      "title": "Evaluating Language Model Context Windows: A \"Working Memory\" Test and Inference-time Correction",
      "url": "https://huggingface.co/papers/2407.03651",
      "authors": [
        "Changho Shin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03651.pdf",
      "abstract": "Large language models are prominently used in real-world applications, often\ntasked with reasoning over large volumes of documents. An exciting development\nin this space is models boasting extended context capabilities, with some\naccommodating over 2 million tokens. Such long context model capabilities\nremain uncertain in production systems, motivating the need to benchmark their\nperformance on real world use cases. We address this challenge by proposing\nSWiM, an evaluation framework that addresses the limitations of standard tests.\nTesting the framework on eight long context models, we find that even strong\nmodels such as GPT-4 and Claude 3 Opus degrade in performance when information\nis present in the middle of the context window (lost-in-the-middle effect).\nNext, in addition to our benchmark, we propose medoid voting, a simple, but\neffective training-free approach that helps alleviate this effect, by\ngenerating responses a few times, each time randomly permuting documents in the\ncontext, and selecting the medoid answer. We evaluate medoid voting on single\ndocument QA tasks, achieving up to a 24% lift in accuracy.",
      "upvotes": 15
    },
    {
      "title": "UltraEdit: Instruction-based Fine-Grained Image Editing at Scale",
      "url": "https://huggingface.co/papers/2407.05282",
      "authors": [
        "Rujie Wu",
        "Peiyu Yu",
        "Baobao Chang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05282.pdf",
      "abstract": "This paper presents UltraEdit, a large-scale (approximately 4 million editing\nsamples), automatically generated dataset for instruction-based image editing.\nOur key idea is to address the drawbacks in existing image editing datasets\nlike InstructPix2Pix and MagicBrush, and provide a systematic approach to\nproducing massive and high-quality image editing samples. UltraEdit offers\nseveral distinct advantages: 1) It features a broader range of editing\ninstructions by leveraging the creativity of large language models (LLMs)\nalongside in-context editing examples from human raters; 2) Its data sources\nare based on real images, including photographs and artworks, which provide\ngreater diversity and reduced bias compared to datasets solely generated by\ntext-to-image models; 3) It also supports region-based editing, enhanced by\nhigh-quality, automatically produced region annotations. Our experiments show\nthat canonical diffusion-based editing baselines trained on UltraEdit set new\nrecords on MagicBrush and Emu-Edit benchmarks. Our analysis further confirms\nthe crucial role of real image anchors and region-based editing data. The\ndataset, code, and models can be found in https://ultra-editing.github.io.",
      "upvotes": 12
    },
    {
      "title": "Compositional Video Generation as Flow Equalization",
      "url": "https://huggingface.co/papers/2407.06182",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2407.06182.pdf",
      "abstract": "Large-scale Text-to-Video (T2V) diffusion models have recently demonstrated\nunprecedented capability to transform natural language descriptions into\nstunning and photorealistic videos. Despite the promising results, a\nsignificant challenge remains: these models struggle to fully grasp complex\ncompositional interactions between multiple concepts and actions. This issue\narises when some words dominantly influence the final video, overshadowing\nother concepts.To tackle this problem, we introduce Vico, a generic\nframework for compositional video generation that explicitly ensures all\nconcepts are represented properly. At its core, Vico analyzes how input tokens\ninfluence the generated video, and adjusts the model to prevent any single\nconcept from dominating. Specifically, Vico extracts attention weights from all\nlayers to build a spatial-temporal attention graph, and then estimates the\ninfluence as the max-flow from the source text token to the video target\ntoken. Although the direct computation of attention flow in diffusion models is\ntypically infeasible, we devise an efficient approximation based on subgraph\nflows and employ a fast and vectorized implementation, which in turn makes the\nflow computation manageable and differentiable. By updating the noisy latent to\nbalance these flows, Vico captures complex interactions and consequently\nproduces videos that closely adhere to textual descriptions. We apply our\nmethod to multiple diffusion-based video models for compositional T2V and video\nediting. Empirical results demonstrate that our framework significantly\nenhances the compositional richness and accuracy of the generated videos. Visit\nour website\nat~https://adamdad.github.io/vico/{https://adamdad.github.io/vico/}.",
      "upvotes": 12
    },
    {
      "title": "Tailor3D: Customized 3D Assets Editing and Generation with Dual-Side Images",
      "url": "https://huggingface.co/papers/2407.06191",
      "authors": [
        "Long Xing",
        "Tong Wu",
        "Hengshuang Zhao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06191.pdf",
      "abstract": "Recent advances in 3D AIGC have shown promise in directly creating 3D objects\nfrom text and images, offering significant cost savings in animation and\nproduct design. However, detailed edit and customization of 3D assets remains a\nlong-standing challenge. Specifically, 3D Generation methods lack the ability\nto follow finely detailed instructions as precisely as their 2D image creation\ncounterparts. Imagine you can get a toy through 3D AIGC but with undesired\naccessories and dressing. To tackle this challenge, we propose a novel pipeline\ncalled Tailor3D, which swiftly creates customized 3D assets from editable\ndual-side images. We aim to emulate a tailor's ability to locally change\nobjects or perform overall style transfer. Unlike creating 3D assets from\nmultiple views, using dual-side images eliminates conflicts on overlapping\nareas that occur when editing individual views. Specifically, it begins by\nediting the front view, then generates the back view of the object through\nmulti-view diffusion. Afterward, it proceeds to edit the back views. Finally, a\nDual-sided LRM is proposed to seamlessly stitch together the front and back 3D\nfeatures, akin to a tailor sewing together the front and back of a garment. The\nDual-sided LRM rectifies imperfect consistencies between the front and back\nviews, enhancing editing capabilities and reducing memory burdens while\nseamlessly integrating them into a unified 3D representation with the LoRA\nTriplane Transformer. Experimental results demonstrate Tailor3D's effectiveness\nacross various 3D generation and editing tasks, including 3D generative fill\nand style transfer. It provides a user-friendly, efficient solution for editing\n3D assets, with each editing step taking only seconds to complete.",
      "upvotes": 10
    },
    {
      "title": "Multi-Object Hallucination in Vision-Language Models",
      "url": "https://huggingface.co/papers/2407.06192",
      "authors": [
        "David F. Fouhey",
        "Joyce Chai"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06192.pdf",
      "abstract": "Large vision language models (LVLMs) often suffer from object hallucination,\nproducing objects not present in the given images. While current benchmarks for\nobject hallucination primarily concentrate on the presence of a single object\nclass rather than individual entities, this work systematically investigates\nmulti-object hallucination, examining how models misperceive (e.g., invent\nnonexistent objects or become distracted) when tasked with focusing on multiple\nobjects simultaneously. We introduce Recognition-based Object Probing\nEvaluation (ROPE), an automated evaluation protocol that considers the\ndistribution of object classes within a single image during testing and uses\nvisual referring prompts to eliminate ambiguity. With comprehensive empirical\nstudies and analysis of potential factors leading to multi-object\nhallucination, we found that (1) LVLMs suffer more hallucinations when focusing\non multiple objects compared to a single object. (2) The tested object class\ndistribution affects hallucination behaviors, indicating that LVLMs may follow\nshortcuts and spurious correlations.(3) Hallucinatory behaviors are influenced\nby data-specific factors, salience and frequency, and model intrinsic\nbehaviors. We hope to enable LVLMs to recognize and reason about multiple\nobjects that often occur in realistic visual scenes, provide insights, and\nquantify our progress towards mitigating the issues.",
      "upvotes": 9
    },
    {
      "title": "InverseCoder: Unleashing the Power of Instruction-Tuned Code LLMs with Inverse-Instruct",
      "url": "https://huggingface.co/papers/2407.05700",
      "authors": [
        "Di Huang",
        "Wenxuan Shi",
        "Wei Wang",
        "Shihao Liu",
        "Ziyuan Nan",
        "Kaizhao Yuan",
        "Rui Zhang",
        "Xishan Zhang",
        "Zidong Du",
        "Qi Guo",
        "Yewen Pu",
        "Dawei Yin",
        "Xing Hu",
        "Yunji Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05700.pdf",
      "abstract": "Recent advancements in open-source code large language models (LLMs) have\ndemonstrated remarkable coding abilities by fine-tuning on the data generated\nfrom powerful closed-source LLMs such as GPT-3.5 and GPT-4 for instruction\ntuning. This paper explores how to further improve an instruction-tuned code\nLLM by generating data from itself rather than querying closed-source LLMs. Our\nkey observation is the misalignment between the translation of formal and\ninformal languages: translating formal language (i.e., code) to informal\nlanguage (i.e., natural language) is more straightforward than the reverse.\nBased on this observation, we propose INVERSE-INSTRUCT, which summarizes\ninstructions from code snippets instead of the reverse. Specifically, given an\ninstruction tuning corpus for code and the resulting instruction-tuned code\nLLM, we ask the code LLM to generate additional high-quality instructions for\nthe original corpus through code summarization and self-evaluation. Then, we\nfine-tune the base LLM on the combination of the original corpus and the\nself-generated one, which yields a stronger instruction-tuned LLM. We present a\nseries of code LLMs named InverseCoder, which surpasses the performance of the\noriginal code LLMs on a wide range of benchmarks, including Python text-to-code\ngeneration, multilingual coding, and data-science code generation.",
      "upvotes": 9
    },
    {
      "title": "PAS: Data-Efficient Plug-and-Play Prompt Augmentation System",
      "url": "https://huggingface.co/papers/2407.06027",
      "authors": [
        "Miao Zheng",
        "Hao Liang",
        "Haoze Sun",
        "Tianpeng Li",
        "Lingchu Xiong",
        "Yan Zhang",
        "Yozhen Wu",
        "Kun Li",
        "Yanjun Sheng",
        "Tao Zhang",
        "Yujing Qiao",
        "Kun Fang",
        "Weipeng Chen",
        "Bin Cui",
        "Wentao Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06027.pdf",
      "abstract": "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering.",
      "upvotes": 8
    },
    {
      "title": "Training Task Experts through Retrieval Based Distillation",
      "url": "https://huggingface.co/papers/2407.05463",
      "authors": [
        "Xueying Jia"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05463.pdf",
      "abstract": "One of the most reliable ways to create deployable models for specialized\ntasks is to obtain an adequate amount of high-quality task-specific data.\nHowever, for specialized tasks, often such datasets do not exist. Existing\nmethods address this by creating such data from large language models (LLMs)\nand then distilling such knowledge into smaller models. However, these methods\nare limited by the quality of the LLMs output, and tend to generate repetitive\nor incorrect data. In this work, we present Retrieval Based Distillation\n(ReBase), a method that first retrieves data from rich online sources and then\ntransforms them into domain-specific data. This method greatly enhances data\ndiversity. Moreover, ReBase generates Chain-of-Thought reasoning and distills\nthe reasoning capacity of LLMs. We test our method on 4 benchmarks and results\nshow that our method significantly improves performance by up to 7.8% on SQuAD,\n1.37% on MNLI, and 1.94% on BigBench-Hard.",
      "upvotes": 7
    },
    {
      "title": "Understanding Visual Feature Reliance through the Lens of Complexity",
      "url": "https://huggingface.co/papers/2407.06076",
      "authors": [
        "Andrew Kyle Lampinen",
        "Katherine Hermann"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06076.pdf",
      "abstract": "Recent studies suggest that deep learning models inductive bias towards\nfavoring simpler features may be one of the sources of shortcut learning. Yet,\nthere has been limited focus on understanding the complexity of the myriad\nfeatures that models learn. In this work, we introduce a new metric for\nquantifying feature complexity, based on V-information and\ncapturing whether a feature requires complex computational transformations to\nbe extracted. Using this V-information metric, we analyze the\ncomplexities of 10,000 features, represented as directions in the penultimate\nlayer, that were extracted from a standard ImageNet-trained vision model. Our\nstudy addresses four key questions: First, we ask what features look like as a\nfunction of complexity and find a spectrum of simple to complex features\npresent within the model. Second, we ask when features are learned during\ntraining. We find that simpler features dominate early in training, and more\ncomplex features emerge gradually. Third, we investigate where within the\nnetwork simple and complex features flow, and find that simpler features tend\nto bypass the visual hierarchy via residual connections. Fourth, we explore the\nconnection between features complexity and their importance in driving the\nnetworks decision. We find that complex features tend to be less important.\nSurprisingly, important features become accessible at earlier layers during\ntraining, like a sedimentation process, allowing the model to build upon these\nfoundational elements.",
      "upvotes": 5
    },
    {
      "title": "PartCraft: Crafting Creative Objects by Parts",
      "url": "https://huggingface.co/papers/2407.04604",
      "authors": [
        "Tao Xiang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.04604.pdf",
      "abstract": "This paper propels creative control in generative visual AI by allowing users\nto \"select\". Departing from traditional text or sketch-based methods, we for\nthe first time allow users to choose visual concepts by parts for their\ncreative endeavors. The outcome is fine-grained generation that precisely\ncaptures selected visual concepts, ensuring a holistically faithful and\nplausible result. To achieve this, we first parse objects into parts through\nunsupervised feature clustering. Then, we encode parts into text tokens and\nintroduce an entropy-based normalized attention loss that operates on them.\nThis loss design enables our model to learn generic prior topology knowledge\nabout object's part composition, and further generalize to novel part\ncompositions to ensure the generation looks holistically faithful. Lastly, we\nemploy a bottleneck encoder to project the part tokens. This not only enhances\nfidelity but also accelerates learning, by leveraging shared knowledge and\nfacilitating information exchange among instances. Visual results in the paper\nand supplementary material showcase the compelling power of PartCraft in\ncrafting highly customized, innovative creations, exemplified by the \"charming\"\nand creative birds. Code is released at https://github.com/kamwoh/partcraft.",
      "upvotes": 4
    },
    {
      "title": "LLMAEL: Large Language Models are Good Context Augmenters for Entity Linking",
      "url": "https://huggingface.co/papers/2407.04020",
      "authors": [
        "Kaisheng Zeng",
        "Xu Bin",
        "Lei Hou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.04020.pdf",
      "abstract": "Entity Linking (EL) models are well-trained at mapping mentions to their\ncorresponding entities according to a given context. However, EL models\nstruggle to disambiguate long-tail entities due to their limited training data.\nMeanwhile, large language models (LLMs) are more robust at interpreting\nuncommon mentions. Yet, due to a lack of specialized training, LLMs suffer at\ngenerating correct entity IDs. Furthermore, training an LLM to perform EL is\ncost-intensive. Building upon these insights, we introduce LLM-Augmented Entity\nLinking LLMAEL, a plug-and-play approach to enhance entity linking through LLM\ndata augmentation. We leverage LLMs as knowledgeable context augmenters,\ngenerating mention-centered descriptions as additional input, while preserving\ntraditional EL models for task specific processing. Experiments on 6 standard\ndatasets show that the vanilla LLMAEL outperforms baseline EL models in most\ncases, while the fine-tuned LLMAEL set the new state-of-the-art results across\nall 6 benchmarks.",
      "upvotes": 2
    },
    {
      "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models",
      "url": "https://huggingface.co/papers/2407.04693",
      "authors": [
        "Chengqi Lyu",
        "Kai Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.04693.pdf",
      "abstract": "Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.",
      "upvotes": 1
    }
  ]
}