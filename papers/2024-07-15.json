{
  "date": "2024-07-15",
  "papers": [
    {
      "title": "SpreadsheetLLM: Encoding Spreadsheets for Large Language Models",
      "url": "https://huggingface.co/papers/2407.09025",
      "authors": [
        "Yuzhang Tian",
        "Jianbo Zhao",
        "Haoyu Dong",
        "Junyu Xiong",
        "Shiyu Xia",
        "Mengyu Zhou",
        "Yun Lin",
        "José Cambronero",
        "Yeye He",
        "Shi Han",
        "Dongmei Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09025.pdf",
      "abstract": "Spreadsheets, with their extensive two-dimensional grids, various layouts,\nand diverse formatting options, present notable challenges for large language\nmodels (LLMs). In response, we introduce SpreadsheetLLM, pioneering an\nefficient encoding method designed to unleash and optimize LLMs' powerful\nunderstanding and reasoning capability on spreadsheets. Initially, we propose a\nvanilla serialization approach that incorporates cell addresses, values, and\nformats. However, this approach was limited by LLMs' token constraints, making\nit impractical for most applications. To tackle this challenge, we develop\nSheetCompressor, an innovative encoding framework that compresses spreadsheets\neffectively for LLMs. It comprises three modules: structural-anchor-based\ncompression, inverse index translation, and data-format-aware aggregation. It\nsignificantly improves performance in spreadsheet table detection task,\noutperforming the vanilla approach by 25.6% in GPT4's in-context learning\nsetting. Moreover, fine-tuned LLM with SheetCompressor has an average\ncompression ratio of 25 times, but achieves a state-of-the-art 78.9% F1 score,\nsurpassing the best existing models by 12.3%. Finally, we propose Chain of\nSpreadsheet for downstream tasks of spreadsheet understanding and validate in a\nnew and demanding spreadsheet QA task. We methodically leverage the inherent\nlayout and structure of spreadsheets, demonstrating that SpreadsheetLLM is\nhighly effective across a variety of spreadsheet tasks.",
      "upvotes": 128
    },
    {
      "title": "Human-like Episodic Memory for Infinite Context LLMs",
      "url": "https://huggingface.co/papers/2407.09450",
      "authors": [
        "Adnan Oomerjee",
        "Jun Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09450.pdf",
      "abstract": "Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs,\nenabling them to effectively handle practically infinite context lengths while\nmaintaining computational efficiency. EM-LLM organises sequences of tokens into\ncoherent episodic events using a combination of Bayesian surprise and\ngraph-theoretic boundary refinement in an on-line fashion. When needed, these\nevents are retrieved through a two-stage memory process, combining\nsimilarity-based and temporally contiguous retrieval for efficient and\nhuman-like access to relevant information. Experiments on the LongBench dataset\ndemonstrate EM-LLM's superior performance, outperforming the state-of-the-art\nInfLLM model with an overall relative improvement of 4.3% across various tasks,\nincluding a 33% improvement on the PassageRetrieval task. Furthermore, our\nanalysis reveals strong correlations between EM-LLM's event segmentation and\nhuman-perceived events, suggesting a bridge between this artificial system and\nits biological counterpart. This work not only advances LLM capabilities in\nprocessing extended contexts but also provides a computational framework for\nexploring human memory mechanisms, opening new avenues for interdisciplinary\nresearch in AI and cognitive science.",
      "upvotes": 60
    },
    {
      "title": "Toto: Time Series Optimized Transformer for Observability",
      "url": "https://huggingface.co/papers/2407.07874",
      "authors": [
        "Ben Cohen",
        "Kan Wang",
        "Charles Masson",
        "Elise Ramé",
        "Youssef Doubli",
        "Othmane Abou-Amal"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.07874.pdf",
      "abstract": "This technical report describes the Time Series Optimized Transformer for\nObservability (Toto), a new state of the art foundation model for time series\nforecasting developed by Datadog. In addition to advancing the state of the art\non generalized time series benchmarks in domains such as electricity and\nweather, this model is the first general-purpose time series forecasting\nfoundation model to be specifically tuned for observability metrics.\n  Toto was trained on a dataset of one trillion time series data points, the\nlargest among all currently published time series foundation models. Alongside\npublicly available time series datasets, 75% of the data used to train Toto\nconsists of fully anonymous numerical metric data points from the Datadog\nplatform.\n  In our experiments, Toto outperforms existing time series foundation models\non observability data. It does this while also excelling at general-purpose\nforecasting tasks, achieving state-of-the-art zero-shot performance on multiple\nopen benchmark datasets.",
      "upvotes": 29
    },
    {
      "title": "MUSCLE: A Model Update Strategy for Compatible LLM Evolution",
      "url": "https://huggingface.co/papers/2407.09435",
      "authors": [
        "Jessica Echterhoff",
        "Fartash Faghri",
        "Raviteja Vemulapalli",
        "Ting-Yao Hu",
        "Chun-Liang Li",
        "Oncel Tuzel"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09435.pdf",
      "abstract": "Large Language Models (LLMs) are frequently updated due to data or\narchitecture changes to improve their performance. When updating models,\ndevelopers often focus on increasing overall performance metrics with less\nemphasis on being compatible with previous model versions. However, users often\nbuild a mental model of the functionality and capabilities of a particular\nmachine learning model they are interacting with. They have to adapt their\nmental model with every update -- a draining task that can lead to user\ndissatisfaction. In practice, fine-tuned downstream task adapters rely on\npretrained LLM base models. When these base models are updated, these\nuser-facing downstream task models experience instance regression or negative\nflips -- previously correct instances are now predicted incorrectly. This\nhappens even when the downstream task training procedures remain identical. Our\nwork aims to provide seamless model updates to a user in two ways. First, we\nprovide evaluation metrics for a notion of compatibility to prior model\nversions, specifically for generative tasks but also applicable for\ndiscriminative tasks. We observe regression and inconsistencies between\ndifferent model versions on a diverse set of tasks and model updates. Second,\nwe propose a training strategy to minimize the number of inconsistencies in\nmodel updates, involving training of a compatibility model that can enhance\ntask fine-tuned language models. We reduce negative flips -- instances where a\nprior model version was correct, but a new model incorrect -- by up to 40% from\nLlama 1 to Llama 2.",
      "upvotes": 20
    },
    {
      "title": "Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing",
      "url": "https://huggingface.co/papers/2407.08770",
      "authors": [
        "Rui Lu",
        "Jingxin Shi",
        "Shiji Song",
        "Gao Huang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08770.pdf",
      "abstract": "Large Language Models (LLMs) have demonstrated great potential as generalist\nassistants, showcasing powerful task understanding and problem-solving\ncapabilities. To deploy LLMs as AI assistants, it is crucial that these models\nexhibit desirable behavioral traits, such as non-toxicity and resilience\nagainst jailbreak attempts. Current methods for detoxification or preventing\njailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement\nLearning from Human Feedback (RLHF), which requires finetuning billions of\nparameters through gradient descent with substantial computation cost.\nFurthermore, models modified through SFT and RLHF may deviate from the\npretrained models, potentially leading to a degradation in foundational LLM\ncapabilities. In this paper, we observe that surprisingly, directly editing a\nsmall subset of parameters can effectively modulate specific behaviors of LLMs,\nsuch as detoxification and resistance to jailbreaking. Specifically, for a\nbehavior that we aim to avoid, we employ a linear classifier, which we term the\nbehavior probe, to classify binary behavior labels within the hidden state\nspace of the LLM. Using this probe, we introduce an algorithm to identify a\ncritical subset of LLM parameters that significantly influence this targeted\nbehavior. Then we directly edit these selected parameters by shifting them\ntowards the behavior probe. Such a direct parameter editing method necessitates\nonly inference-level computational resources. Experiments demonstrate that in\nthe representative detoxification task, our approach achieves reductions of up\nto 90.0\\% in toxicity on the RealToxicityPrompts dataset and 49.2\\% on ToxiGen,\nwhile maintaining the LLM's general capabilities in areas such as common sense,\nquestion answering, and mathematics. Our code is available at\nhttps://github.com/lucywang720/model-surgery.",
      "upvotes": 19
    },
    {
      "title": "H2O-Danube3 Technical Report",
      "url": "https://huggingface.co/papers/2407.09276",
      "authors": [
        "Pascal Pfeiffer",
        "Philipp Singer",
        "Yauhen Babakhin",
        "Gabor Fodor",
        "Nischay Dhankhar",
        "Sri Satish Ambati"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09276.pdf",
      "abstract": "We present H2O-Danube3, a series of small language models consisting of\nH2O-Danube3-4B, trained on 6T tokens and H2O-Danube3-500M, trained on 4T\ntokens. Our models are pre-trained on high quality Web data consisting of\nprimarily English tokens in three stages with different data mixes before final\nsupervised tuning for chat version. The models exhibit highly competitive\nmetrics across a multitude of academic, chat, and fine-tuning benchmarks.\nThanks to its compact architecture, H2O-Danube3 can be efficiently run on a\nmodern smartphone, enabling local inference and rapid processing capabilities\neven on mobile devices. We make all models openly available under Apache 2.0\nlicense further democratizing LLMs to a wider audience economically.",
      "upvotes": 18
    },
    {
      "title": "GAVEL: Generating Games Via Evolution and Language Models",
      "url": "https://huggingface.co/papers/2407.09388",
      "authors": [
        "Graham Todd",
        "Alexander Padula",
        "Éric Piette",
        "Julian Togelius"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09388.pdf",
      "abstract": "Automatically generating novel and interesting games is a complex task.\nChallenges include representing game rules in a computationally workable form,\nsearching through the large space of potential games under most such\nrepresentations, and accurately evaluating the originality and quality of\npreviously unseen games. Prior work in automated game generation has largely\nfocused on relatively restricted rule representations and relied on\ndomain-specific heuristics. In this work, we explore the generation of novel\ngames in the comparatively expansive Ludii game description language, which\nencodes the rules of over 1000 board games in a variety of styles and modes of\nplay. We draw inspiration from recent advances in large language models and\nevolutionary computation in order to train a model that intelligently mutates\nand recombines games and mechanics expressed as code. We demonstrate both\nquantitatively and qualitatively that our approach is capable of generating new\nand interesting games, including in regions of the potential rules space not\ncovered by existing games in the Ludii dataset. A sample of the generated games\nare available to play online through the Ludii portal.",
      "upvotes": 14
    },
    {
      "title": "Transformer Layers as Painters",
      "url": "https://huggingface.co/papers/2407.09298",
      "authors": [
        "Marc Pickett",
        "Aakash Kumar Nain",
        "Llion Jones"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09298.pdf",
      "abstract": "Despite their nearly universal adoption for large language models, the\ninternal workings of transformers are not well understood. We aim to better\nunderstand the impact of removing or reorganizing information throughout the\nlayers of a pretrained transformer. Such an understanding could both yield\nbetter usage of existing models as well as to make architectural improvements\nto produce new variants. We present a series of empirical studies on frozen\nmodels that show that the lower and final layers of pretrained transformers\ndiffer from middle layers, but that middle layers have a surprising amount of\nuniformity. We further show that some classes of problems have robustness to\nskipping layers, running the layers in an order different from how they were\ntrained, or running the layers in parallel. Our observations suggest that even\nfrozen pretrained models may gracefully trade accuracy for latency by skipping\nlayers or running layers in parallel.",
      "upvotes": 13
    },
    {
      "title": "StyleSplat: 3D Object Style Transfer with Gaussian Splatting",
      "url": "https://huggingface.co/papers/2407.09473",
      "authors": [
        "Sahil Jain",
        "Avik Kuthiala",
        "Prabhdeep Singh Sethi",
        "Prakanshul Saxena"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09473.pdf",
      "abstract": "Recent advancements in radiance fields have opened new avenues for creating\nhigh-quality 3D assets and scenes. Style transfer can enhance these 3D assets\nwith diverse artistic styles, transforming creative expression. However,\nexisting techniques are often slow or unable to localize style transfer to\nspecific objects. We introduce StyleSplat, a lightweight method for stylizing\n3D objects in scenes represented by 3D Gaussians from reference style images.\nOur approach first learns a photorealistic representation of the scene using 3D\nGaussian splatting while jointly segmenting individual 3D objects. We then use\na nearest-neighbor feature matching loss to finetune the Gaussians of the\nselected objects, aligning their spherical harmonic coefficients with the style\nimage to ensure consistency and visual appeal. StyleSplat allows for quick,\ncustomizable style transfer and localized stylization of multiple objects\nwithin a scene, each with a different style. We demonstrate its effectiveness\nacross various 3D scenes and styles, showcasing enhanced control and\ncustomization in 3D creation.",
      "upvotes": 10
    },
    {
      "title": "SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers",
      "url": "https://huggingface.co/papers/2407.09413",
      "authors": [
        "Rama Chellappa"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09413.pdf",
      "abstract": "Seeking answers to questions within long scientific research articles is a\ncrucial area of study that aids readers in quickly addressing their inquiries.\nHowever, existing question-answering (QA) datasets based on scientific papers\nare limited in scale and focus solely on textual content. To address this\nlimitation, we introduce SPIQA (Scientific Paper Image Question Answering), the\nfirst large-scale QA dataset specifically designed to interpret complex figures\nand tables within the context of scientific research articles across various\ndomains of computer science. Leveraging the breadth of expertise and ability of\nmultimodal large language models (MLLMs) to understand figures, we employ\nautomatic and manual curation to create the dataset. We craft an\ninformation-seeking task involving multiple images that cover a wide variety of\nplots, charts, tables, schematic diagrams, and result visualizations. SPIQA\ncomprises 270K questions divided into training, validation, and three different\nevaluation splits. Through extensive experiments with 12 prominent foundational\nmodels, we evaluate the ability of current multimodal systems to comprehend the\nnuanced aspects of research articles. Additionally, we propose a\nChain-of-Thought (CoT) evaluation strategy with in-context retrieval that\nallows fine-grained, step-by-step assessment and improves model performance. We\nfurther explore the upper bounds of performance enhancement with additional\ntextual information, highlighting its promising potential for future research\nand the dataset's impact on revolutionizing how we interact with scientific\nliterature.",
      "upvotes": 9
    },
    {
      "title": "New Desiderata for Direct Preference Optimization",
      "url": "https://huggingface.co/papers/2407.09072",
      "authors": [
        "Xiangkun Hu",
        "David Wipf"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09072.pdf",
      "abstract": "Large language models in the past have typically relied on some form of\nreinforcement learning with human feedback (RLHF) to better align model\nresponses with human preferences. However, because of oft-observed\ninstabilities when implementing these RLHF pipelines, various\nreparameterization techniques have recently been introduced to sidestep the\nneed for separately learning an RL reward model. Instead, directly fine-tuning\nfor human preferences is achieved via the minimization of a single closed-form\ntraining objective, a process originally referred to as direct preference\noptimization (DPO) and followed by several notable descendants. Although\neffective in certain real-world settings, we introduce new evaluation criteria\nthat serve to highlight unresolved shortcomings in the ability of existing DPO\nmethods to interpolate between a pre-trained reference model and empirical\nmeasures of human preferences, as well as unavoidable trade-offs in how low-\nand high-quality responses are regularized and constraints are handled. Our\ninsights then motivate an alternative DPO-like loss that provably mitigates\nthese limitations. Empirical results serve to corroborate notable aspects of\nour analyses.",
      "upvotes": 9
    },
    {
      "title": "Characterizing Prompt Compression Methods for Long Context Inference",
      "url": "https://huggingface.co/papers/2407.08892",
      "authors": [
        "Lutfi Eren Erdogan",
        "Sehoon Kim",
        "Kurt Keutzer",
        "Amir Gholami"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08892.pdf",
      "abstract": "Long context inference presents challenges at the system level with increased\ncompute and memory requirements, as well as from an accuracy perspective in\nbeing able to reason over long contexts. Recently, several methods have been\nproposed to compress the prompt to reduce the context length. However, there\nhas been little work on comparing the different proposed methods across\ndifferent tasks through a standardized analysis. This has led to conflicting\nresults. To address this, here we perform a comprehensive characterization and\nevaluation of different prompt compression methods. In particular, we analyze\nextractive compression, summarization-based abstractive compression, and token\npruning methods. Surprisingly, we find that extractive compression often\noutperforms all the other approaches, and enables up to 10x compression with\nminimal accuracy degradation. Interestingly, we also find that despite several\nrecent claims, token pruning methods often lag behind extractive compression.\nWe only found marginal improvements on summarization tasks.",
      "upvotes": 9
    },
    {
      "title": "Speech Slytherin: Examining the Performance and Efficiency of Mamba for Speech Separation, Recognition, and Synthesis",
      "url": "https://huggingface.co/papers/2407.09732",
      "authors": [
        "Xilin Jiang",
        "Yinghao Aaron Li",
        "Adrian Nicolas Florea",
        "Cong Han",
        "Nima Mesgarani"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09732.pdf",
      "abstract": "It is too early to conclude that Mamba is a better alternative to\ntransformers for speech before comparing Mamba with transformers in terms of\nboth performance and efficiency in multiple speech-related tasks. To reach this\nconclusion, we propose and evaluate three models for three tasks: Mamba-TasNet\nfor speech separation, ConMamba for speech recognition, and VALL-M for speech\nsynthesis. We compare them with transformers of similar sizes in performance,\nmemory, and speed. Our Mamba or Mamba-transformer hybrid models show comparable\nor higher performance than their transformer counterparts: Sepformer,\nConformer, and VALL-E. They are more efficient than transformers in memory and\nspeed for speech longer than a threshold duration, inversely related to the\nresolution of a speech token. Mamba for separation is the most efficient, and\nMamba for recognition is the least. Further, we show that Mamba is not more\nefficient than transformer for speech shorter than the threshold duration and\nperforms worse in models that require joint modeling of text and speech, such\nas cross or masked attention of two inputs. Therefore, we argue that the\nsuperiority of Mamba or transformer depends on particular problems and models.\nCode available at https://github.com/xi-j/Mamba-TasNet and\nhttps://github.com/xi-j/Mamba-ASR.",
      "upvotes": 8
    },
    {
      "title": "TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models",
      "url": "https://huggingface.co/papers/2407.09012",
      "authors": [
        "Min-Jung Kim",
        "Junsoo Lee",
        "Jaegul Choo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09012.pdf",
      "abstract": "Pose-driven human-image animation diffusion models have shown remarkable\ncapabilities in realistic human video synthesis. Despite the promising results\nachieved by previous approaches, challenges persist in achieving temporally\nconsistent animation and ensuring robustness with off-the-shelf pose detectors.\nIn this paper, we present TCAN, a pose-driven human image animation method that\nis robust to erroneous poses and consistent over time. In contrast to previous\nmethods, we utilize the pre-trained ControlNet without fine-tuning to leverage\nits extensive pre-acquired knowledge from numerous pose-image-caption pairs. To\nkeep the ControlNet frozen, we adapt LoRA to the UNet layers, enabling the\nnetwork to align the latent space between the pose and appearance features.\nAdditionally, by introducing an additional temporal layer to the ControlNet, we\nenhance robustness against outliers of the pose detector. Through the analysis\nof attention maps over the temporal axis, we also designed a novel temperature\nmap leveraging pose information, allowing for a more static background.\nExtensive experiments demonstrate that the proposed method can achieve\npromising results in video synthesis tasks encompassing various poses, like\nchibi. Project Page: https://eccv2024tcan.github.io/",
      "upvotes": 8
    },
    {
      "title": "Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning",
      "url": "https://huggingface.co/papers/2406.02265",
      "authors": [
        "Rita Ramos",
        "Raphael Tang",
        "Desmond Elliott"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02265.pdf",
      "abstract": "Recent advances in retrieval-augmented models for image captioning highlight\nthe benefit of retrieving related captions for efficient, lightweight models\nwith strong domain-transfer capabilities. While these models demonstrate the\nsuccess of retrieval augmentation, retrieval models are still far from perfect\nin practice: the retrieved information can sometimes mislead the model,\nresulting in incorrect generation and worse performance. In this paper, we\nanalyze the robustness of a retrieval-augmented captioning model SmallCap. Our\nanalysis shows that the model is sensitive to tokens that appear in the\nmajority of the retrieved captions, and the input attribution shows that those\ntokens are likely copied into the generated output. Given these findings, we\npropose to train the model by sampling retrieved captions from more diverse\nsets. This decreases the chance that the model learns to copy majority tokens,\nand improves both in-domain and cross-domain performance.",
      "upvotes": 6
    },
    {
      "title": "Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training",
      "url": "https://huggingface.co/papers/2407.09121",
      "authors": [
        "Wenxiang Jiao",
        "Wenxuan Wang",
        "Jen-tse Huang",
        "Jiahao Xu",
        "Tian Liang",
        "Pinjia He",
        "Zhaopeng Tu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09121.pdf",
      "abstract": "This study addresses a critical gap in safety tuning practices for Large\nLanguage Models (LLMs) by identifying and tackling a refusal position bias\nwithin safety tuning data, which compromises the models' ability to\nappropriately refuse generating unsafe content. We introduce a novel approach,\nDecoupled Refusal Training (DeRTa), designed to empower LLMs to refuse\ncompliance to harmful prompts at any response position, significantly enhancing\ntheir safety capabilities. DeRTa incorporates two novel components: (1) Maximum\nLikelihood Estimation (MLE) with Harmful Response Prefix, which trains models\nto recognize and avoid unsafe content by appending a segment of harmful\nresponse to the beginning of a safe response, and (2) Reinforced Transition\nOptimization (RTO), which equips models with the ability to transition from\npotential harm to safety refusal consistently throughout the harmful response\nsequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model\nfamilies across six attack scenarios, demonstrates that our method not only\nimproves model safety without compromising performance but also surpasses\nwell-known models such as GPT-4 in defending against attacks. Importantly, our\napproach successfully defends recent advanced attack methods (e.g., CodeAttack)\nthat have jailbroken GPT-4 and LLaMA3-70B-Instruct. Our code and data can be\nfound at https://github.com/RobustNLP/DeRTa.",
      "upvotes": 5
    },
    {
      "title": "RRM: Relightable assets using Radiance guided Material extraction",
      "url": "https://huggingface.co/papers/2407.06397",
      "authors": [
        "Diego Gomez",
        "Adrien Kaiser",
        "Élie Michel"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06397.pdf",
      "abstract": "Synthesizing NeRFs under arbitrary lighting has become a seminal problem in\nthe last few years. Recent efforts tackle the problem via the extraction of\nphysically-based parameters that can then be rendered under arbitrary lighting,\nbut they are limited in the range of scenes they can handle, usually\nmishandling glossy scenes. We propose RRM, a method that can extract the\nmaterials, geometry, and environment lighting of a scene even in the presence\nof highly reflective objects. Our method consists of a physically-aware\nradiance field representation that informs physically-based parameters, and an\nexpressive environment light structure based on a Laplacian Pyramid. We\ndemonstrate that our contributions outperform the state-of-the-art on parameter\nretrieval tasks, leading to high-fidelity relighting and novel view synthesis\non surfacic scenes.",
      "upvotes": 4
    }
  ]
}