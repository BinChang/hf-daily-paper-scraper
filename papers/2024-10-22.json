{
  "date": "2024-10-22",
  "papers": [
    {
      "title": "FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors",
      "url": "https://huggingface.co/papers/2410.16271",
      "authors": [
        "Chang-Han Yeh",
        "Shih-Han Yen",
        "Cheng Sun"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16271.pdf",
      "abstract": "Neural Radiance Fields (NeRF) face significant challenges in few-shot\nscenarios, primarily due to overfitting and long training times for\nhigh-fidelity rendering. Existing methods, such as FreeNeRF and SparseNeRF, use\nfrequency regularization or pre-trained priors but struggle with complex\nscheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework\nthat leverages weight-sharing voxels across multiple scales to efficiently\nrepresent scene details. Our key contribution is a cross-scale geometric\nadaptation scheme that selects pseudo ground truth depth based on reprojection\nerrors across scales. This guides training without relying on externally\nlearned priors, enabling full utilization of the training data. It can also\nintegrate pre-trained priors, enhancing quality without slowing convergence.\nExperiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms\nother few-shot NeRF methods while significantly reducing training time, making\nit a practical solution for efficient and accurate 3D scene reconstruction.",
      "upvotes": 80
    },
    {
      "title": "SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree",
      "url": "https://huggingface.co/papers/2410.16268",
      "authors": [
        "Rui Qian",
        "Xiaoyi Dong",
        "Pan Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16268.pdf",
      "abstract": "The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation\nmodel for object segmentation in both images and videos, paving the way for\nvarious downstream video applications. The crucial design of SAM 2 for video\nsegmentation is its memory module, which prompts object-aware memories from\nprevious frames for current frame prediction. However, its greedy-selection\nmemory design suffers from the \"error accumulation\" problem, where an errored\nor missed mask will cascade and influence the segmentation of the subsequent\nframes, which limits the performance of SAM 2 toward complex long-term videos.\nTo this end, we introduce SAM2Long, an improved training-free video object\nsegmentation strategy, which considers the segmentation uncertainty within each\nframe and chooses the video-level optimal results from multiple segmentation\npathways in a constrained tree search manner. In practice, we maintain a fixed\nnumber of segmentation pathways throughout the video. For each frame, multiple\nmasks are proposed based on the existing pathways, creating various candidate\nbranches. We then select the same fixed number of branches with higher\ncumulative scores as the new pathways for the next frame. After processing the\nfinal frame, the pathway with the highest cumulative score is chosen as the\nfinal segmentation result. Benefiting from its heuristic search design,\nSAM2Long is robust toward occlusions and object reappearances, and can\neffectively segment and track objects for complex long-term videos. Notably,\nSAM2Long achieves an average improvement of 3.0 points across all 24\nhead-to-head comparisons, with gains of up to 5.3 points in J&F on long-term\nvideo object segmentation benchmarks such as SA-V and LVOS. The code is\nreleased at https://github.com/Mark12Ding/SAM2Long.",
      "upvotes": 65
    },
    {
      "title": "CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution",
      "url": "https://huggingface.co/papers/2410.16256",
      "authors": [
        "Maosong Cao",
        "Hongwei Liu",
        "Kai Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16256.pdf",
      "abstract": "Efficient and accurate evaluation is crucial for the continuous improvement\nof large language models (LLMs). Among various assessment methods, subjective\nevaluation has garnered significant attention due to its superior alignment\nwith real-world usage scenarios and human preferences. However, human-based\nevaluations are costly and lack reproducibility, making precise automated\nevaluators (judgers) vital in this process. In this report, we introduce\nCompassJudger-1, the first open-source all-in-one judge LLM.\nCompassJudger-1 is a general-purpose LLM that demonstrates remarkable\nversatility. It is capable of: 1. Performing unitary scoring and two-model\ncomparisons as a reward model; 2. Conducting evaluations according to specified\nformats; 3. Generating critiques; 4. Executing diverse tasks like a general\nLLM. To assess the evaluation capabilities of different judge models under a\nunified setting, we have also established JudgerBench, a new benchmark\nthat encompasses various subjective evaluation tasks and covers a wide range of\ntopics. CompassJudger-1 offers a comprehensive solution for various evaluation\ntasks while maintaining the flexibility to adapt to diverse requirements. Both\nCompassJudger and JudgerBench are released and available to the research\ncommunity athttps://github.com/open-compass/CompassJudger. We believe that by\nopen-sourcing these tools, we can foster collaboration and accelerate progress\nin LLM evaluation methodologies.",
      "upvotes": 58
    },
    {
      "title": "AutoTrain: No-code training for state-of-the-art models",
      "url": "https://huggingface.co/papers/2410.15735",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.15735.pdf",
      "abstract": "With the advancements in open-source models, training (or finetuning) models\non custom datasets has become a crucial part of developing solutions which are\ntailored to specific industrial or open-source applications. Yet, there is no\nsingle tool which simplifies the process of training across different types of\nmodalities or tasks. We introduce AutoTrain (aka AutoTrain Advanced) -- an\nopen-source, no code tool/library which can be used to train (or finetune)\nmodels for different kinds of tasks such as: large language model (LLM)\nfinetuning, text classification/regression, token classification,\nsequence-to-sequence task, finetuning of sentence transformers, visual language\nmodel (VLM) finetuning, image classification/regression and even classification\nand regression tasks on tabular data. AutoTrain Advanced is an open-source\nlibrary providing best practices for training models on custom datasets. The\nlibrary is available at https://github.com/huggingface/autotrain-advanced.\nAutoTrain can be used in fully local mode or on cloud machines and works with\ntens of thousands of models shared on Hugging Face Hub and their variations.",
      "upvotes": 55
    },
    {
      "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
      "url": "https://huggingface.co/papers/2410.13861",
      "authors": [
        "Kun Wang",
        "Hao Li",
        "Hao Tian",
        "Rui Zhao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.13861.pdf",
      "abstract": "Recent advancements in multimodal foundation models have yielded significant\nprogress in vision-language understanding. Initial attempts have also explored\nthe potential of multimodal large language models (MLLMs) for visual content\ngeneration. However, existing works have insufficiently addressed the varying\ngranularity demands of different image generation tasks within a unified MLLM\nparadigm - from the diversity required in text-to-image generation to the\nprecise controllability needed in image manipulation. In this work, we propose\nPUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA\nunifies multi-granular visual features as both inputs and outputs of MLLMs,\nelegantly addressing the different granularity requirements of various image\ngeneration tasks within a unified MLLM framework. Following multimodal\npretraining and task-specific instruction tuning, PUMA demonstrates proficiency\nin a wide range of multimodal tasks. This work represents a significant step\ntowards a truly unified MLLM capable of adapting to the granularity demands of\nvarious visual tasks. The code and model will be released in\nhttps://github.com/rongyaofang/PUMA.",
      "upvotes": 53
    },
    {
      "title": "Baichuan Alignment Technical Report",
      "url": "https://huggingface.co/papers/2410.14940",
      "authors": [
        "Fan Yang",
        "Haoze Sun",
        "Tao Zhang",
        "Chenzheng Zhu",
        "Tao Zhang",
        "Miao Zheng",
        "Xu Li",
        "Mingyang Chen",
        "Yanzhao Qin",
        "Youquan Li",
        "Hao Liang",
        "Fei Li",
        "Yadong Li",
        "Mang Wang",
        "Kun Fang",
        "Bin Cui"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.14940.pdf",
      "abstract": "We introduce Baichuan Alignment, a detailed analysis of the alignment\ntechniques employed in the Baichuan series of models. This represents the\nindustry's first comprehensive account of alignment methodologies, offering\nvaluable insights for advancing AI research. We investigate the critical\ncomponents that enhance model performance during the alignment process,\nincluding optimization methods, data strategies, capability enhancements, and\nevaluation processes. The process spans three key stages: Prompt Augmentation\nSystem (PAS), Supervised Fine-Tuning (SFT), and Preference Alignment. The\nproblems encountered, the solutions applied, and the improvements made are\nthoroughly recorded.\n  Through comparisons across well-established benchmarks, we highlight the\ntechnological advancements enabled by Baichuan Alignment. Baichuan-Instruct is\nan internal model, while Qwen2-Nova-72B and Llama3-PBM-Nova-70B are instruct\nversions of the Qwen2-72B and Llama-3-70B base models, optimized through\nBaichuan Alignment. Baichuan-Instruct demonstrates significant improvements in\ncore capabilities, with user experience gains ranging from 17% to 28%, and\nperforms exceptionally well on specialized benchmarks. In open-source benchmark\nevaluations, both Qwen2-Nova-72B and Llama3-PBM-Nova-70B consistently\noutperform their respective official instruct versions across nearly all\ndatasets. This report aims to clarify the key technologies behind the alignment\nprocess, fostering a deeper understanding within the community.\nLlama3-PBM-Nova-70B model is available at\nhttps://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.",
      "upvotes": 47
    },
    {
      "title": "SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation",
      "url": "https://huggingface.co/papers/2410.14745",
      "authors": [
        "Wei Ju",
        "Ming Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.14745.pdf",
      "abstract": "Supervised fine-tuning (SFT) is crucial in adapting large language models\n(LLMs) to a specific domain or task. However, only a limited amount of labeled\ndata is available in practical applications, which poses a severe challenge for\nSFT in yielding satisfactory results. Therefore, a data-efficient framework\nthat can fully exploit labeled and unlabeled data for LLM fine-tuning is highly\nanticipated. Towards this end, we introduce a semi-supervised fine-tuning\nframework named SemiEvol for LLM adaptation from a propagate-and-select manner.\nFor knowledge propagation, SemiEvol adopts a bi-level approach, propagating\nknowledge from labeled data to unlabeled data through both in-weight and\nin-context methods. For knowledge selection, SemiEvol incorporates a\ncollaborative learning mechanism, selecting higher-quality pseudo-response\nsamples. We conducted experiments using GPT-4o-mini and Llama-3.1 on seven\ngeneral or domain-specific datasets, demonstrating significant improvements in\nmodel performance on target data. Furthermore, we compared SemiEvol with SFT\nand self-evolution methods, highlighting its practicality in hybrid data\nscenarios.",
      "upvotes": 45
    },
    {
      "title": "Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages",
      "url": "https://huggingface.co/papers/2410.16153",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.16153.pdf",
      "abstract": "Despite recent advances in multimodal large language models (MLLMs), their\ndevelopment has predominantly focused on English- and western-centric datasets\nand tasks, leaving most of the world's languages and diverse cultural contexts\nunderrepresented. This paper introduces Pangea, a multilingual multimodal LLM\ntrained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages.\nPangeaIns features: 1) high-quality English instructions, 2) carefully\nmachine-translated instructions, and 3) culturally relevant multimodal tasks to\nensure cross-cultural coverage. To rigorously assess models' capabilities, we\nintroduce PangeaBench, a holistic evaluation suite encompassing 14 datasets\ncovering 47 languages. Results show that Pangea significantly outperforms\nexisting open-source models in multilingual settings and diverse cultural\ncontexts. Ablation studies further reveal the importance of English data\nproportions, language popularity, and the number of multimodal training samples\non overall performance. We fully open-source our data, code, and trained\ncheckpoints, to facilitate the development of inclusive and robust multilingual\nMLLMs, promoting equity and accessibility across a broader linguistic and\ncultural spectrum.",
      "upvotes": 42
    },
    {
      "title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style",
      "url": "https://huggingface.co/papers/2410.16184",
      "authors": [
        "Yantao Liu",
        "Rui Min",
        "Lei Hou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16184.pdf",
      "abstract": "Reward models are critical in techniques like Reinforcement Learning from\nHuman Feedback (RLHF) and Inference Scaling Laws, where they guide language\nmodel alignment and select optimal responses. Despite their importance,\nexisting reward model benchmarks often evaluate models by asking them to\ndistinguish between responses generated by models of varying power. However,\nthis approach fails to assess reward models on subtle but critical content\nchanges and variations in style, resulting in a low correlation with policy\nmodel performance. To this end, we introduce RM-Bench, a novel benchmark\ndesigned to evaluate reward models based on their sensitivity to subtle content\ndifferences and resistance to style biases. Extensive experiments demonstrate\nthat RM-Bench strongly correlates with policy model performance, making it a\nreliable reference for selecting reward models to align language models\neffectively. We evaluate nearly 40 reward models on RM-Bench. Our results\nreveal that even state-of-the-art models achieve an average performance of only\n46.6%, which falls short of random-level accuracy (50%) when faced with style\nbias interference. These findings highlight the significant room for\nimprovement in current reward models. Related code and data are available at\nhttps://github.com/THU-KEG/RM-Bench.",
      "upvotes": 23
    },
    {
      "title": "Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception",
      "url": "https://huggingface.co/papers/2410.12788",
      "authors": [
        "Zhiyuan Ji",
        "Bo Tang",
        "Feiyu Xiong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.12788.pdf",
      "abstract": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline, which impacts the quality of knowledge-intensive\ntasks. This paper introduces the concept of Meta-Chunking, which refers to a\ngranularity between sentences and paragraphs, consisting of a collection of\nsentences within a paragraph that have deep linguistic logical connections. To\nimplement Meta-Chunking, we designed two strategies based on LLMs: Margin\nSampling Chunking and Perplexity Chunking. The former employs LLMs to perform\nbinary classification on whether consecutive sentences need to be segmented,\nmaking decisions based on the probability difference obtained from margin\nsampling. The latter precisely identifies text chunk boundaries by analyzing\nthe characteristics of perplexity distribution. Additionally, considering the\ninherent complexity of different texts, we propose a strategy that combines\nMeta-Chunking with dynamic merging to achieve a balance between fine-grained\nand coarse-grained text chunking. Experiments conducted on eleven datasets\ndemonstrate that Meta-Chunking can more efficiently improve the performance of\nsingle-hop and multi-hop question answering based on RAG. For instance, on the\n2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only\nconsuming 45.8% of the time. Our code is available at\nhttps://github.com/IAAR-Shanghai/Meta-Chunking.",
      "upvotes": 21
    },
    {
      "title": "Pre-training Distillation for Large Language Models: A Design Space Exploration",
      "url": "https://huggingface.co/papers/2410.16215",
      "authors": [
        "Xin Lv",
        "Zijun Yao",
        "Lei Hou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16215.pdf",
      "abstract": "Knowledge distillation (KD) aims to transfer knowledge from a large teacher\nmodel to a smaller student model. Previous work applying KD in the field of\nlarge language models (LLMs) typically focused on the post-training phase,\nwhere the student LLM learns directly from instructions and corresponding\nresponses generated by the teacher model. In this paper, we extend KD to the\npre-training phase of LLMs, named pre-training distillation (PD). We first\nconduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a\n1.9B parameter student LLM, validating the effectiveness of PD. Considering the\nkey impact factors of distillation, we systematically explore the design space\nof pre-training distillation across four aspects: logits processing, loss\nselection, scaling law, and offline or online logits. We conduct extensive\nexperiments to explore the design space of pre-training distillation and find\nbetter configurations and interesting conclusions, such as larger student LLMs\ngenerally benefiting more from pre-training distillation, while a larger\nteacher LLM does not necessarily guarantee better results. We hope our\nexploration of the design space will inform future practices in pre-training\ndistillation.",
      "upvotes": 15
    },
    {
      "title": "Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation",
      "url": "https://huggingface.co/papers/2410.15748",
      "authors": [
        "Ping Wei"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15748.pdf",
      "abstract": "Formal proofs are challenging to write even for experienced experts. Recent\nprogress in Neural Theorem Proving (NTP) shows promise in expediting this\nprocess. However, the formal corpora available on the Internet are limited\ncompared to the general text, posing a significant data scarcity challenge for\nNTP. To address this issue, this work proposes Alchemy, a general framework for\ndata synthesis that constructs formal theorems through symbolic mutation.\nSpecifically, for each candidate theorem in Mathlib, we identify all invocable\ntheorems that can be used to rewrite or apply to it. Subsequently, we mutate\nthe candidate theorem by replacing the corresponding term in the statement with\nits equivalent form or antecedent. As a result, our method increases the number\nof theorems in Mathlib by an order of magnitude, from 110k to 6M. Furthermore,\nwe perform continual pretraining and supervised finetuning on this augmented\ncorpus for large language models. Experimental results demonstrate the\neffectiveness of our approach, achieving a 5% absolute performance improvement\non Leandojo benchmark. Additionally, our synthetic data achieve a 2.5% absolute\nperformance gain on the out-of-distribution miniF2F benchmark. To provide\nfurther insights, we conduct a comprehensive analysis of synthetic data\ncomposition and the training paradigm, offering valuable guidance for\ndeveloping a strong theorem prover.",
      "upvotes": 12
    },
    {
      "title": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant",
      "url": "https://huggingface.co/papers/2410.15316",
      "authors": [
        "Dinh Bach Vu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15316.pdf",
      "abstract": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their application to speech-based tasks remains challenging due to the\ncomplexities of integrating audio and text modalities. This paper introduces\nIchigo, a mixed-modal model that seamlessly processes interleaved sequences of\nspeech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes\nspeech into discrete tokens and employs a uniform transformer-based\narchitecture for both speech and text modalities. This method enables joint\nreasoning and generation across modalities without the need for separate\nadapters. We present a comprehensive training methodology, including\npre-training on multilingual speech recognition datasets and fine-tuning on a\ncurated instruction dataset. Ichigo demonstrates state-of-the-art performance\non speech question-answering benchmarks, outperforming existing open-source\nspeech language models and achieving comparable results to cascaded systems.\nNotably, Ichigo exhibits a latency of just 111 ms to first token generation,\nsignificantly lower than current models. Our approach not only advances the\nfield of multimodal AI but also provides a framework for smaller research teams\nto contribute effectively to open-source speech-language models.",
      "upvotes": 10
    },
    {
      "title": "Zero-shot Model-based Reinforcement Learning using Large Language Models",
      "url": "https://huggingface.co/papers/2410.11711",
      "authors": [
        "Youssef Attia El Hili",
        "Maurizio Filippone",
        "Balázs Kégl"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11711.pdf",
      "abstract": "The emerging zero-shot capabilities of Large Language Models (LLMs) have led\nto their applications in areas extending well beyond natural language\nprocessing tasks. In reinforcement learning, while LLMs have been extensively\nused in text-based environments, their integration with continuous state spaces\nremains understudied. In this paper, we investigate how pre-trained LLMs can be\nleveraged to predict in context the dynamics of continuous Markov decision\nprocesses. We identify handling multivariate data and incorporating the control\nsignal as key challenges that limit the potential of LLMs' deployment in this\nsetup and propose Disentangled In-Context Learning (DICL) to address them. We\npresent proof-of-concept applications in two reinforcement learning settings:\nmodel-based policy evaluation and data-augmented off-policy reinforcement\nlearning, supported by theoretical analysis of the proposed methods. Our\nexperiments further demonstrate that our approach produces well-calibrated\nuncertainty estimates. We release the code at\nhttps://github.com/abenechehab/dicl.",
      "upvotes": 8
    },
    {
      "title": "Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement",
      "url": "https://huggingface.co/papers/2410.15633",
      "authors": [
        "Haozhe Zhao",
        "Gang Chen",
        "Chuancheng Lv",
        "Baobao Chang",
        "Maosong Sun"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15633.pdf",
      "abstract": "The expansion of large language models to effectively handle instructions\nwith extremely long contexts has yet to be fully investigated. The primary\nobstacle lies in constructing a high-quality long instruction-following dataset\ndevised for long context alignment. Existing studies have attempted to scale up\nthe available data volume by synthesizing long instruction-following samples.\nHowever, indiscriminately increasing the quantity of data without a\nwell-defined strategy for ensuring data quality may introduce low-quality\nsamples and restrict the final performance. To bridge this gap, we aim to\naddress the unique challenge of long-context alignment, i.e., modeling the\nlong-range dependencies for handling instructions and lengthy input contexts.\nWe propose GATEAU, a novel framework designed to identify the influential and\nhigh-quality samples enriched with long-range dependency relations by utilizing\ncrafted Homologous Models' Guidance (HMG) and Contextual Awareness Measurement\n(CAM). Specifically, HMG attempts to measure the difficulty of generating\ncorresponding responses due to the long-range dependencies, using the\nperplexity scores of the response from two homologous models with different\ncontext windows. Also, the role of CAM is to measure the difficulty of\nunderstanding the long input contexts due to long-range dependencies by\nevaluating whether the model's attention is focused on important segments.\nBuilt upon both proposed methods, we select the most challenging samples as the\ninfluential data to effectively frame the long-range dependencies, thereby\nachieving better performance of LLMs. Comprehensive experiments indicate that\nGATEAU effectively identifies samples enriched with long-range dependency\nrelations and the model trained on these selected samples exhibits better\ninstruction-following and long-context understanding capabilities.",
      "upvotes": 7
    },
    {
      "title": "How Many Van Goghs Does It Take to Van Gogh? Finding the Imitation Threshold",
      "url": "https://huggingface.co/papers/2410.15002",
      "authors": [
        "Royi Rassin",
        "Arnav Das",
        "Gantavya Bhatt",
        "Preethi Seshadri",
        "Chirag Shah",
        "Jeff Bilmes",
        "Hannaneh Hajishirzi",
        "Yanai Elazar"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15002.pdf",
      "abstract": "Text-to-image models are trained using large datasets collected by scraping\nimage-text pairs from the internet. These datasets often include private,\ncopyrighted, and licensed material. Training models on such datasets enables\nthem to generate images with such content, which might violate copyright laws\nand individual privacy. This phenomenon is termed imitation -- generation of\nimages with content that has recognizable similarity to its training images. In\nthis work we study the relationship between a concept's frequency in the\ntraining dataset and the ability of a model to imitate it. We seek to determine\nthe point at which a model was trained on enough instances to imitate a concept\n-- the imitation threshold. We posit this question as a new problem: Finding\nthe Imitation Threshold (FIT) and propose an efficient approach that estimates\nthe imitation threshold without incurring the colossal cost of training\nmultiple models from scratch. We experiment with two domains -- human faces and\nart styles -- for which we create four datasets, and evaluate three\ntext-to-image models which were trained on two pretraining datasets. Our\nresults reveal that the imitation threshold of these models is in the range of\n200-600 images, depending on the domain and the model. The imitation threshold\ncan provide an empirical basis for copyright violation claims and acts as a\nguiding principle for text-to-image model developers that aim to comply with\ncopyright and privacy laws. We release the code and data at\nhttps://github.com/vsahil/MIMETIC-2.git and the project's website is\nhosted at https://how-many-van-goghs-does-it-take.github.io.",
      "upvotes": 6
    },
    {
      "title": "Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos",
      "url": "https://huggingface.co/papers/2410.16259",
      "authors": [
        "Gengshan Yang",
        "Andrea Bajcsy",
        "Shunsuke Saito",
        "Angjoo Kanazawa"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16259.pdf",
      "abstract": "We present Agent-to-Sim (ATS), a framework for learning interactive behavior\nmodels of 3D agents from casual longitudinal video collections. Different from\nprior works that rely on marker-based tracking and multiview cameras, ATS\nlearns natural behaviors of animal and human agents non-invasively through\nvideo observations recorded over a long time-span (e.g., a month) in a single\nenvironment. Modeling 3D behavior of an agent requires persistent 3D tracking\n(e.g., knowing which point corresponds to which) over a long time period. To\nobtain such data, we develop a coarse-to-fine registration method that tracks\nthe agent and the camera over time through a canonical 3D space, resulting in a\ncomplete and persistent spacetime 4D representation. We then train a generative\nmodel of agent behaviors using paired data of perception and motion of an agent\nqueried from the 4D reconstruction. ATS enables real-to-sim transfer from video\nrecordings of an agent to an interactive behavior simulator. We demonstrate\nresults on pets (e.g., cat, dog, bunny) and human given monocular RGBD videos\ncaptured by a smartphone.",
      "upvotes": 5
    },
    {
      "title": "CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy",
      "url": "https://huggingface.co/papers/2410.13218",
      "authors": [
        "Travis Labrum",
        "Jamie C. Chiu",
        "Shaun M. Eack",
        "Fei Fang",
        "Zhiyu Zoey Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.13218.pdf",
      "abstract": "There is a significant gap between patient needs and available mental health\nsupport today. In this paper, we aim to thoroughly examine the potential of\nusing Large Language Models (LLMs) to assist professional psychotherapy. To\nthis end, we propose a new benchmark, CBT-BENCH, for the systematic evaluation\nof cognitive behavioral therapy (CBT) assistance. We include three levels of\ntasks in CBT-BENCH: I: Basic CBT knowledge acquisition, with the task of\nmultiple-choice questions; II: Cognitive model understanding, with the tasks of\ncognitive distortion classification, primary core belief classification, and\nfine-grained core belief classification; III: Therapeutic response generation,\nwith the task of generating responses to patient speech in CBT therapy\nsessions. These tasks encompass key aspects of CBT that could potentially be\nenhanced through AI assistance, while also outlining a hierarchy of capability\nrequirements, ranging from basic knowledge recitation to engaging in real\ntherapeutic conversations. We evaluated representative LLMs on our benchmark.\nExperimental results indicate that while LLMs perform well in reciting CBT\nknowledge, they fall short in complex real-world scenarios requiring deep\nanalysis of patients' cognitive structures and generating effective responses,\nsuggesting potential future work.",
      "upvotes": 4
    },
    {
      "title": "In-context learning and Occam's razor",
      "url": "https://huggingface.co/papers/2410.14086",
      "authors": [
        "Sarthak Mittal",
        "Dhanya Sridhar"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.14086.pdf",
      "abstract": "The goal of machine learning is generalization. While the No Free Lunch\nTheorem states that we cannot obtain theoretical guarantees for generalization\nwithout further assumptions, in practice we observe that simple models which\nexplain the training data generalize best: a principle called Occam's razor.\nDespite the need for simple models, most current approaches in machine learning\nonly minimize the training error, and at best indirectly promote simplicity\nthrough regularization or architecture design. Here, we draw a connection\nbetween Occam's razor and in-context learning: an emergent ability of certain\nsequence models like Transformers to learn at inference time from past\nobservations in a sequence. In particular, we show that the next-token\nprediction loss used to train in-context learners is directly equivalent to a\ndata compression technique called prequential coding, and that minimizing this\nloss amounts to jointly minimizing both the training error and the complexity\nof the model that was implicitly learned from context. Our theory and the\nempirical experiments we use to support it not only provide a normative account\nof in-context learning, but also elucidate the shortcomings of current\nin-context learning methods, suggesting ways in which they can be improved. We\nmake our code available at https://github.com/3rdCore/PrequentialCode.",
      "upvotes": 2
    },
    {
      "title": "Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers",
      "url": "https://huggingface.co/papers/2410.13184",
      "authors": [
        "Tao Ge",
        "Ang Li",
        "Dong Yu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.13184.pdf",
      "abstract": "Traditional transformer models often allocate a fixed amount of computational\nresources to every input token, leading to inefficient and unnecessary\ncomputation. To address this, the Mixture of Depths (MoD) was introduced to\ndynamically adjust the computational depth by skipping less important layers.\nDespite its promise, current MoD approaches remain under-explored and face two\nmain challenges: (1) high training costs due to the need to train the\nentire model along with the routers that determine which layers to skip, and\n(2) the risk of performance degradation when important layers are\nbypassed. In response to the first issue, we propose Router-Tuning, a method\nthat fine-tunes only the router on a small dataset, drastically reducing the\ncomputational overhead associated with full model training. For the second\nchallenge, we propose MindSkip, which deploys Attention with Dynamic\nDepths. This method preserves the model's performance while significantly\nenhancing computational and memory efficiency. Extensive experiments\ndemonstrate that our approach delivers competitive results while dramatically\nimproving the computation efficiency, e.g., 21\\% speedup and only a 0.2\\%\nperformance drop. The code is released at\nhttps://github.com/CASE-Lab-UMD/Router-Tuning.",
      "upvotes": 2
    },
    {
      "title": "Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training",
      "url": "https://huggingface.co/papers/2410.15460",
      "authors": [
        "Marco Bonizzato",
        "Reihaneh Rabbany",
        "Golnoosh Farnadi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15460.pdf",
      "abstract": "As large language models (LLMs) become increasingly deployed across various\nindustries, concerns regarding their reliability, particularly due to\nhallucinations-outputs that are factually inaccurate or irrelevant to user\ninput-have grown. Our research investigates the relationship between the\ntraining process and the emergence of hallucinations to address a key gap in\nexisting research that focuses primarily on post hoc detection and mitigation\nstrategies. Using models from the Pythia suite (70M-12B parameters) and several\nhallucination detection metrics, we analyze hallucination trends throughout\ntraining and explore LLM internal dynamics. We introduce SEnsitive Neuron\nDropout (SeND), a novel training protocol designed to mitigate hallucinations\nby reducing variance during training. SeND achieves this by deterministically\ndropping neurons with significant variability on a dataset, referred to as\nSensitive Neurons. In addition, we develop an unsupervised hallucination\ndetection metric, Efficient EigenScore (EES), which approximates the\ntraditional EigenScore in 2x speed. This efficient metric is integrated into\nour protocol, allowing SeND to be both computationally scalable and effective\nat reducing hallucinations. Our empirical evaluation demonstrates that our\napproach improves LLM reliability at test time by up to 40% compared to normal\ntraining while also providing an efficient method to improve factual accuracy\nwhen adapting LLMs to domains such as Wikipedia and Medical datasets.",
      "upvotes": 1
    },
    {
      "title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs",
      "url": "https://huggingface.co/papers/2410.13394",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.13394.pdf",
      "abstract": "Evaluating machine-generated text remains a significant challenge in NLP,\nespecially for non-English languages. Current methodologies, including\nautomated metrics, human assessments, and LLM-based evaluations, predominantly\nfocus on English, revealing a significant gap in multilingual evaluation\nframeworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an\nextensible framework that includes evaluator LLMs (Hercule) and a novel test\nset (Recon) specifically designed for multilingual evaluation. Our test set\nfeatures 500 human-annotated instructions spanning various task capabilities\nalong with human judgment scores across six languages. This would enable\nbenchmarking of general-purpose multilingual LLMs and facilitate\nmeta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a\ncross-lingual evaluation model that addresses the scarcity of reference answers\nin the target language by learning to assign scores to responses based on\neasily available reference answers in English. Our experiments demonstrate that\nHercule aligns more closely with human judgments compared to proprietary\nmodels, demonstrating the effectiveness of such cross-lingual evaluation in low\nresource scenarios. Further, it is also effective in zero-shot evaluation on\nunseen languages. This study is the first comprehensive examination of\ncross-lingual evaluation using LLMs, presenting a scalable and effective\napproach for multilingual assessment. All code, datasets, and models will be\npublicly available to enable further research in this important area.",
      "upvotes": 1
    },
    {
      "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
      "url": "https://huggingface.co/papers/2410.15017",
      "authors": [
        "Md Fahim",
        "A K M Mahbubur Rahman",
        "Tariq Iqbal",
        "M Ashraful Amin",
        "Md Mofijul Islam",
        "Amin Ahsan Ali"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15017.pdf",
      "abstract": "Recent advancements in speech-language models have yielded significant\nimprovements in speech tokenization and synthesis. However, effectively mapping\nthe complex, multidimensional attributes of speech into discrete tokens remains\nchallenging. This process demands acoustic, semantic, and contextual\ninformation for precise speech representations. Existing speech representations\ngenerally fall into two categories: acoustic tokens from audio codecs and\nsemantic tokens from speech self-supervised learning models. Although recent\nefforts have unified acoustic and semantic tokens for improved performance,\nthey overlook the crucial role of contextual representation in comprehensive\nspeech modeling. Our empirical investigations reveal that the absence of\ncontextual representations results in elevated Word Error Rate (WER) and Word\nInformation Lost (WIL) scores in speech transcriptions. To address these\nlimitations, we propose two novel distillation approaches: (1) a language model\n(LM)-guided distillation method that incorporates contextual information, and\n(2) a combined LM and self-supervised speech model (SM)-guided distillation\ntechnique that effectively distills multimodal representations (acoustic,\nsemantic, and contextual) into a comprehensive speech tokenizer, termed\nDM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder\nframework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM\nduring the training process. Experiments show DM-Codec significantly\noutperforms state-of-the-art speech tokenization models, reducing WER by up to\n13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility\nby 1.85% on the LibriSpeech benchmark dataset. The code, samples, and model\ncheckpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
      "upvotes": 1
    }
  ]
}