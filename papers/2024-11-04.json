{
  "date": "2024-11-04",
  "papers": [
    {
      "title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents",
      "url": "https://huggingface.co/papers/2410.23218",
      "authors": [
        "Zhenyu Wu",
        "Yian Wang",
        "Paul Pu Liang",
        "Yu Qiao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.23218.pdf",
      "abstract": "Existing efforts in building GUI agents heavily rely on the availability of\nrobust commercial Vision-Language Models (VLMs) such as GPT-4o and\nGeminiProVision. Practitioners are often reluctant to use open-source VLMs due\nto their significant performance lag compared to their closed-source\ncounterparts, particularly in GUI grounding and Out-Of-Distribution (OOD)\nscenarios. To facilitate future research in this area, we developed OS-Atlas -\na foundational GUI action model that excels at GUI grounding and OOD agentic\ntasks through innovations in both data and modeling. We have invested\nsignificant engineering effort in developing an open-source toolkit for\nsynthesizing GUI grounding data across multiple platforms, including Windows,\nLinux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing\nthe largest open-source cross-platform GUI grounding corpus to date, which\ncontains over 13 million GUI elements. This dataset, combined with innovations\nin model training, provides a solid foundation for OS-Atlas to understand GUI\nscreenshots and generalize to unseen interfaces. Through extensive evaluation\nacross six benchmarks spanning three different platforms (mobile, desktop, and\nweb), OS-Atlas demonstrates significant performance improvements over previous\nstate-of-the-art models. Our evaluation also uncovers valuable insights into\ncontinuously improving and scaling the agentic capabilities of open-source\nVLMs.",
      "upvotes": 44
    },
    {
      "title": "Personalization of Large Language Models: A Survey",
      "url": "https://huggingface.co/papers/2411.00027",
      "authors": [
        "Branislav Kveton",
        "Diyi Yang",
        "Hamed Zamani",
        "Tong Yu",
        "Tyler Derr",
        "Hongjie Chen",
        "Nedim Lipka",
        "Yu Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00027.pdf",
      "abstract": "Personalization of Large Language Models (LLMs) has recently become\nincreasingly important with a wide range of applications. Despite the\nimportance and recent progress, most existing works on personalized LLMs have\nfocused either entirely on (a) personalized text generation or (b) leveraging\nLLMs for personalization-related downstream applications, such as\nrecommendation systems. In this work, we bridge the gap between these two\nseparate main directions for the first time by introducing a taxonomy for\npersonalized LLM usage and summarizing the key differences and challenges. We\nprovide a formalization of the foundations of personalized LLMs that\nconsolidates and expands notions of personalization of LLMs, defining and\ndiscussing novel facets of personalization, usage, and desiderata of\npersonalized LLMs. We then unify the literature across these diverse fields and\nusage scenarios by proposing systematic taxonomies for the granularity of\npersonalization, personalization techniques, datasets, evaluation methods, and\napplications of personalized LLMs. Finally, we highlight challenges and\nimportant open problems that remain to be addressed. By unifying and surveying\nrecent research using the proposed taxonomies, we aim to provide a clear guide\nto the existing literature and different facets of personalization in LLMs,\nempowering both researchers and practitioners.",
      "upvotes": 30
    },
    {
      "title": "Constant Acceleration Flow",
      "url": "https://huggingface.co/papers/2411.00322",
      "authors": [
        "Sihyeon Kim",
        "Youngjoon Hong",
        "Hyunwoo J. Kim"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00322.pdf",
      "abstract": "Rectified flow and reflow procedures have significantly advanced fast\ngeneration by progressively straightening ordinary differential equation (ODE)\nflows. They operate under the assumption that image and noise pairs, known as\ncouplings, can be approximated by straight trajectories with constant velocity.\nHowever, we observe that modeling with constant velocity and using reflow\nprocedures have limitations in accurately learning straight trajectories\nbetween pairs, resulting in suboptimal performance in few-step generation. To\naddress these limitations, we introduce Constant Acceleration Flow (CAF), a\nnovel framework based on a simple constant acceleration equation. CAF\nintroduces acceleration as an additional learnable variable, allowing for more\nexpressive and accurate estimation of the ODE flow. Moreover, we propose two\ntechniques to further improve estimation accuracy: initial velocity\nconditioning for the acceleration model and a reflow process for the initial\nvelocity. Our comprehensive studies on toy datasets, CIFAR-10, and ImageNet\n64x64 demonstrate that CAF outperforms state-of-the-art baselines for one-step\ngeneration. We also show that CAF dramatically improves few-step coupling\npreservation and inversion over Rectified flow. Code is available at\nhttps://github.com/mlvlab/CAF{https://github.com/mlvlab/CAF}.",
      "upvotes": 22
    },
    {
      "title": "TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models",
      "url": "https://huggingface.co/papers/2410.23266",
      "authors": [
        "Yuxuan Ding",
        "Yanan Zheng",
        "Yilun Zhao",
        "Tesca Fitzgerald",
        "Arman Cohan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.23266.pdf",
      "abstract": "Existing benchmarks often highlight the remarkable performance achieved by\nstate-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal\ncontext for video understanding. However, how well do the models truly perform\nvisual temporal reasoning? Our study of existing benchmarks shows that this\ncapability of MFMs is likely overestimated as many questions can be solved by\nusing a single, few, or out-of-order frames. To systematically examine current\nvisual temporal reasoning tasks, we propose three principles with corresponding\nmetrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame\nInformation Disparity. Following these principles, we introduce TOMATO,\nTemporal Reasoning Multimodal Evaluation, a novel benchmark crafted to\nrigorously assess MFMs' temporal reasoning capabilities in video understanding.\nTOMATO comprises 1,484 carefully curated, human-annotated questions spanning\nsix tasks (i.e., action count, direction, rotation, shape & trend, velocity &\nfrequency, and visual cues), applied to 1,417 videos, including 805\nself-recorded and -generated videos, that encompass human-centric, real-world,\nand simulated scenarios. Our comprehensive evaluation reveals a human-model\nperformance gap of 57.3% with the best-performing model. Moreover, our in-depth\nanalysis uncovers more fundamental limitations beyond this gap in current MFMs.\nWhile they can accurately recognize events in isolated frames, they fail to\ninterpret these frames as a continuous sequence. We believe TOMATO will serve\nas a crucial testbed for evaluating the next-generation MFMs and as a call to\nthe community to develop AI systems capable of comprehending human world\ndynamics through the video modality.",
      "upvotes": 19
    },
    {
      "title": "Randomized Autoregressive Visual Generation",
      "url": "https://huggingface.co/papers/2411.00776",
      "authors": [
        "Liang-Chieh Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00776.pdf",
      "abstract": "This paper presents Randomized AutoRegressive modeling (RAR) for visual\ngeneration, which sets a new state-of-the-art performance on the image\ngeneration task while maintaining full compatibility with language modeling\nframeworks. The proposed RAR is simple: during a standard autoregressive\ntraining process with a next-token prediction objective, the input\nsequence-typically ordered in raster form-is randomly permuted into different\nfactorization orders with a probability r, where r starts at 1 and linearly\ndecays to 0 over the course of training. This annealing training strategy\nenables the model to learn to maximize the expected likelihood over all\nfactorization orders and thus effectively improve the model's capability of\nmodeling bidirectional contexts. Importantly, RAR preserves the integrity of\nthe autoregressive modeling framework, ensuring full compatibility with\nlanguage modeling while significantly improving performance in image\ngeneration. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48,\nnot only surpassing prior state-of-the-art autoregressive image generators but\nalso outperforming leading diffusion-based and masked transformer-based\nmethods. Code and models will be made available at\nhttps://github.com/bytedance/1d-tokenizer",
      "upvotes": 17
    },
    {
      "title": "Physics in Next-token Prediction",
      "url": "https://huggingface.co/papers/2411.00660",
      "authors": [
        "Yiliang Song",
        "Xuelong Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00660.pdf",
      "abstract": "We discovered the underlying physics in Next-token Prediction (NTP). We\nidentified the law of information conservation within NTP and proposed the\nFirst Law of Information Capacity (IC-1), demonstrating that the essence of\nintelligence emergence in auto-regressive models is fundamentally a process of\ninformation transfer. We also introduced Landauer's Principle into NTP,\nformulating the Second Law of Information Capacity (IC-2), which establishes\nthe relationship between auto-regressive model training and energy consumption.\nAdditionally, we presented several corollaries, which hold practical\nsignificance for production practices. Finally, we validated the compatibility\nand complementarity of our findings with existing theories.",
      "upvotes": 14
    },
    {
      "title": "GPT or BERT: why not both?",
      "url": "https://huggingface.co/papers/2410.24159",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2410.24159.pdf",
      "abstract": "We present a simple way to merge masked language modeling with causal\nlanguage modeling. This hybrid training objective results in a model that\ncombines the strengths of both modeling paradigms within a single transformer\nstack: GPT-BERT can be transparently used like any standard causal or masked\nlanguage model. We test the pretraining process that enables this flexible\nbehavior on the BabyLM Challenge 2024. The results show that the hybrid\npretraining outperforms masked-only or causal-only models. We openly release\nthe models, training corpora and code.",
      "upvotes": 11
    },
    {
      "title": "Survey of User Interface Design and Interaction Techniques in Generative AI Applications",
      "url": "https://huggingface.co/papers/2410.22370",
      "authors": [
        "Reuben Luera",
        "Ryan A. Rossi",
        "Alexa Siu",
        "Tong Yu",
        "Xiang Chen",
        "Hanieh Salehy",
        "Jian Zhao",
        "Samyadeep Basu",
        "Nedim Lipka"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.22370.pdf",
      "abstract": "The applications of generative AI have become extremely impressive, and the\ninterplay between users and AI is even more so. Current human-AI interaction\nliterature has taken a broad look at how humans interact with generative AI,\nbut it lacks specificity regarding the user interface designs and patterns used\nto create these applications. Therefore, we present a survey that\ncomprehensively presents taxonomies of how a human interacts with AI and the\nuser interaction patterns designed to meet the needs of a variety of relevant\nuse cases. We focus primarily on user-guided interactions, surveying\ninteractions that are initiated by the user and do not include any implicit\nsignals given by the user. With this survey, we aim to create a compendium of\ndifferent user-interaction patterns that can be used as a reference for\ndesigners and developers alike. In doing so, we also strive to lower the entry\nbarrier for those attempting to learn more about the design of generative AI\napplications.",
      "upvotes": 11
    },
    {
      "title": "In-Context LoRA for Diffusion Transformers",
      "url": "https://huggingface.co/papers/2410.23775",
      "authors": [
        "Chen Liang",
        "Yu Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.23775.pdf",
      "abstract": "Recent research arXiv:2410.15027 has explored the use of diffusion\ntransformers (DiTs) for task-agnostic image generation by simply concatenating\nattention tokens across images. However, despite substantial computational\nresources, the fidelity of the generated images remains suboptimal. In this\nstudy, we reevaluate and streamline this framework by hypothesizing that\ntext-to-image DiTs inherently possess in-context generation capabilities,\nrequiring only minimal tuning to activate them. Through diverse task\nexperiments, we qualitatively demonstrate that existing text-to-image DiTs can\neffectively perform in-context generation without any tuning. Building on this\ninsight, we propose a remarkably simple pipeline to leverage the in-context\nabilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint\ncaptioning of multiple images, and (3) apply task-specific LoRA tuning using\nsmall datasets (e.g., 20sim 100 samples) instead of full-parameter tuning\nwith large datasets. We name our models In-Context LoRA (IC-LoRA). This\napproach requires no modifications to the original DiT models, only changes to\nthe training data. Remarkably, our pipeline generates high-fidelity image sets\nthat better adhere to prompts. While task-specific in terms of tuning data, our\nframework remains task-agnostic in architecture and pipeline, offering a\npowerful tool for the community and providing valuable insights for further\nresearch on product-level task-agnostic generation systems. We release our\ncode, data, and models at https://github.com/ali-vilab/In-Context-LoRA",
      "upvotes": 10
    },
    {
      "title": "CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes",
      "url": "https://huggingface.co/papers/2411.00771",
      "authors": [
        "Zhongkai Mao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00771.pdf",
      "abstract": "Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field\nreconstruction, manifesting efficient and high-fidelity novel view synthesis.\nHowever, accurately representing surfaces, especially in large and complex\nscenarios, remains a significant challenge due to the unstructured nature of\n3DGS. In this paper, we present CityGaussianV2, a novel approach for\nlarge-scale scene reconstruction that addresses critical challenges related to\ngeometric accuracy and efficiency. Building on the favorable generalization\ncapabilities of 2D Gaussian Splatting (2DGS), we address its convergence and\nscalability issues. Specifically, we implement a decomposed-gradient-based\ndensification and depth regression technique to eliminate blurry artifacts and\naccelerate convergence. To scale up, we introduce an elongation filter that\nmitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we\noptimize the CityGaussian pipeline for parallel training, achieving up to\n10times compression, at least 25% savings in training time, and a 50%\ndecrease in memory usage. We also established standard geometry benchmarks\nunder large-scale scenes. Experimental results demonstrate that our method\nstrikes a promising balance between visual quality, geometric accuracy, as well\nas storage and training costs. The project page is available at\nhttps://dekuliutesla.github.io/CityGaussianV2/.",
      "upvotes": 9
    },
    {
      "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation",
      "url": "https://huggingface.co/papers/2411.00412",
      "authors": [
        "Yadi Cao",
        "Duncan Watson-Parris"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00412.pdf",
      "abstract": "Large Language Models (LLMs) demonstrate promising capabilities in solving\nsimple scientific problems but often produce hallucinations for complex ones.\nWhile integrating LLMs with tools can increase reliability, this approach\ntypically results in over-reliance on tools, diminishing the model's ability to\nsolve simple problems through basic reasoning. In contrast, human experts first\nassess problem complexity using domain knowledge before choosing an appropriate\nsolution approach. Inspired by this human problem-solving process, we propose a\nnovel two-component fine-tuning method. In the first component World Knowledge\nDistillation (WKD), LLMs learn directly from solutions generated using tool's\ninformation to internalize domain knowledge. In the second component Tool Usage\nAdaptation (TUA), we partition problems into easy and hard categories based on\nthe model's direct answering accuracy. While maintaining the same alignment\ntarget for easy problems as in WKD, we train the model to intelligently switch\nto tool usage for more challenging problems. We validate our method on six\nscientific benchmark datasets, spanning mathematics, climate science and\nepidemiology. On average, our models demonstrate a 28.18% improvement in answer\naccuracy and a 13.89% increase in tool usage precision across all datasets,\nsurpassing state-of-the-art models including GPT-4o and Claude-3.5.",
      "upvotes": 9
    },
    {
      "title": "Zipfian Whitening",
      "url": "https://huggingface.co/papers/2411.00680",
      "authors": [
        "Han Bao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00680.pdf",
      "abstract": "The word embedding space in neural models is skewed, and correcting this can\nimprove task performance. We point out that most approaches for modeling,\ncorrecting, and measuring the symmetry of an embedding space implicitly assume\nthat the word frequencies are uniform; in reality, word frequencies follow a\nhighly non-uniform distribution, known as Zipf's law. Surprisingly, simply\nperforming PCA whitening weighted by the empirical word frequency that follows\nZipf's law significantly improves task performance, surpassing established\nbaselines. From a theoretical perspective, both our approach and existing\nmethods can be clearly categorized: word representations are distributed\naccording to an exponential family with either uniform or Zipfian base\nmeasures. By adopting the latter approach, we can naturally emphasize\ninformative low-frequency words in terms of their vector norm, which becomes\nevident from the information-geometric perspective, and in terms of the loss\nfunctions for imbalanced classification. Additionally, our theory corroborates\nthat popular natural language processing methods, such as skip-gram negative\nsampling, WhiteningBERT, and headless language models, work well just because\ntheir word embeddings encode the empirical word frequency into the underlying\nprobabilistic model.",
      "upvotes": 8
    },
    {
      "title": "Fashion-VDM: Video Diffusion Model for Virtual Try-On",
      "url": "https://huggingface.co/papers/2411.00225",
      "authors": [
        "Yingwei Li",
        "Luyang Zhu",
        "Innfarn Yoo",
        "Andreas Lugmayr",
        "Chris Lee",
        "Ira Kemelmacher-Shlizerman"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00225.pdf",
      "abstract": "We present Fashion-VDM, a video diffusion model (VDM) for generating virtual\ntry-on videos. Given an input garment image and person video, our method aims\nto generate a high-quality try-on video of the person wearing the given\ngarment, while preserving the person's identity and motion. Image-based virtual\ntry-on has shown impressive results; however, existing video virtual try-on\n(VVT) methods are still lacking garment details and temporal consistency. To\naddress these issues, we propose a diffusion-based architecture for video\nvirtual try-on, split classifier-free guidance for increased control over the\nconditioning inputs, and a progressive temporal training strategy for\nsingle-pass 64-frame, 512px video generation. We also demonstrate the\neffectiveness of joint image-video training for video try-on, especially when\nvideo data is limited. Our qualitative and quantitative experiments show that\nour approach sets the new state-of-the-art for video virtual try-on. For\nadditional results, visit our project page:\nhttps://johannakarras.github.io/Fashion-VDM.",
      "upvotes": 7
    },
    {
      "title": "SambaMixer: State of Health Prediction of Li-ion Batteries using Mamba State Space Models",
      "url": "https://huggingface.co/papers/2411.00233",
      "authors": [
        "Clara Pérez-Molina",
        "Sergio Martin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00233.pdf",
      "abstract": "The state of health (SOH) of a Li-ion battery is a critical parameter that\ndetermines the remaining capacity and the remaining lifetime of the battery. In\nthis paper, we propose SambaMixer a novel structured state space model (SSM)\nfor predicting the state of health of Li-ion batteries. The proposed SSM is\nbased on the MambaMixer architecture, which is designed to handle multi-variate\ntime signals. We evaluate our model on the NASA battery discharge dataset and\nshow that our model outperforms the state-of-the-art on this dataset. We\nfurther introduce a novel anchor-based resampling method which ensures time\nsignals are of the expected length while also serving as augmentation\ntechnique. Finally, we condition prediction on the sample time and the cycle\ntime difference using positional encodings to improve the performance of our\nmodel and to learn recuperation effects. Our results proof that our model is\nable to predict the SOH of Li-ion batteries with high accuracy and robustness.",
      "upvotes": 7
    },
    {
      "title": "HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models",
      "url": "https://huggingface.co/papers/2410.22901",
      "authors": [
        "Nianhong Jiao",
        "Tian Li",
        "Chaojie Yang",
        "Chenhui Xue",
        "Boya Niu",
        "Jun Gao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.22901.pdf",
      "abstract": "We propose an effective method for inserting adapters into text-to-image\nfoundation models, which enables the execution of complex downstream tasks\nwhile preserving the generalization ability of the base model. The core idea of\nthis method is to optimize the attention mechanism related to 2D feature maps,\nwhich enhances the performance of the adapter. This approach was validated on\nthe task of meme video generation and achieved significant results. We hope\nthis work can provide insights for post-training tasks of large text-to-image\nmodels. Additionally, as this method demonstrates good compatibility with SD1.5\nderivative models, it holds certain value for the open-source community.\nTherefore, we will release the related code\n(https://songkey.github.io/hellomeme).",
      "upvotes": 7
    },
    {
      "title": "Face Anonymization Made Simple",
      "url": "https://huggingface.co/papers/2411.00762",
      "authors": [
        "Tuomas Varanka",
        "Terence Sim",
        "Nicu Sebe"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00762.pdf",
      "abstract": "Current face anonymization techniques often depend on identity loss\ncalculated by face recognition models, which can be inaccurate and unreliable.\nAdditionally, many methods require supplementary data such as facial landmarks\nand masks to guide the synthesis process. In contrast, our approach uses\ndiffusion models with only a reconstruction loss, eliminating the need for\nfacial landmarks or masks while still producing images with intricate,\nfine-grained details. We validated our results on two public benchmarks through\nboth quantitative and qualitative evaluations. Our model achieves\nstate-of-the-art performance in three key areas: identity anonymization, facial\nattribute preservation, and image quality. Beyond its primary function of\nanonymization, our model can also perform face swapping tasks by incorporating\nan additional facial image as input, demonstrating its versatility and\npotential for diverse applications. Our code and models are available at\nhttps://github.com/hanweikung/face_anon_simple .",
      "upvotes": 6
    },
    {
      "title": "GRS-QA -- Graph Reasoning-Structured Question Answering Dataset",
      "url": "https://huggingface.co/papers/2411.00369",
      "authors": [
        "Khin S. Yone",
        "Samyak Rajesh Jain",
        "Yu Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00369.pdf",
      "abstract": "Large Language Models (LLMs) have excelled in multi-hop question-answering\n(M-QA) due to their advanced reasoning abilities. However, the impact of the\ninherent reasoning structures on LLM M-QA performance remains unclear, largely\ndue to the absence of QA datasets that provide fine-grained reasoning\nstructures. To address this gap, we introduce the Graph Reasoning-Structured\nQuestion Answering Dataset (GRS-QA), which includes both semantic contexts and\nreasoning structures for QA pairs. Unlike existing M-QA datasets, where\ndifferent reasoning structures are entangled together, GRS-QA explicitly\ncaptures intricate reasoning pathways by constructing reasoning graphs, where\nnodes represent textual contexts and edges denote logical flows. These\nreasoning graphs of different structures enable a fine-grained evaluation of\nLLM reasoning capabilities across various reasoning structures. Our empirical\nanalysis reveals that LLMs perform differently when handling questions with\nvarying reasoning structures. This finding facilitates the exploration of\ntextual structures as compared with semantics.",
      "upvotes": 6
    },
    {
      "title": "M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation",
      "url": "https://huggingface.co/papers/2410.21157",
      "authors": [
        "Ken Deng",
        "Congnan Liu",
        "Jian Yang",
        "He Zhu",
        "Peng Zhao",
        "Ke Jin",
        "Zekun Wang",
        "Guoan Zhang",
        "Wenbo Su"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.21157.pdf",
      "abstract": "Repository-level code completion has drawn great attention in software\nengineering, and several benchmark datasets have been introduced. However,\nexisting repository-level code completion benchmarks usually focus on a limited\nnumber of languages (<5), which cannot evaluate the general code intelligence\nabilities across different languages for existing code Large Language Models\n(LLMs). Besides, the existing benchmarks usually report overall average scores\nof different languages, where the fine-grained abilities in different\ncompletion scenarios are ignored. Therefore, to facilitate the research of code\nLLMs in multilingual scenarios, we propose a massively multilingual\nrepository-level code completion benchmark covering 18 programming languages\n(called M2RC-EVAL), and two types of fine-grained annotations (i.e.,\nbucket-level and semantic-level) on different completion scenarios are\nprovided, where we obtain these annotations based on the parsed abstract syntax\ntree. Moreover, we also curate a massively multilingual instruction corpora\nM2RC- INSTRUCT dataset to improve the repository-level code completion\nabilities of existing code LLMs. Comprehensive experimental results demonstrate\nthe effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.",
      "upvotes": 6
    },
    {
      "title": "WikiNER-fr-gold: A Gold-Standard NER Corpus",
      "url": "https://huggingface.co/papers/2411.00030",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2411.00030.pdf",
      "abstract": "We address in this article the the quality of the WikiNER corpus, a\nmultilingual Named Entity Recognition corpus, and provide a consolidated\nversion of it. The annotation of WikiNER was produced in a semi-supervised\nmanner i.e. no manual verification has been carried out a posteriori. Such\ncorpus is called silver-standard. In this paper we propose WikiNER-fr-gold\nwhich is a revised version of the French proportion of WikiNER. Our corpus\nconsists of randomly sampled 20% of the original French sub-corpus (26,818\nsentences with 700k tokens). We start by summarizing the entity types included\nin each category in order to define an annotation guideline, and then we\nproceed to revise the corpus. Finally we present an analysis of errors and\ninconsistency observed in the WikiNER-fr corpus, and we discuss potential\nfuture work directions.",
      "upvotes": 4
    }
  ]
}