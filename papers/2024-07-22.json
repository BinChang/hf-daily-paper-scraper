{
  "date": "2024-07-22",
  "papers": [
    {
      "title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
      "url": "https://huggingface.co/papers/2407.14507",
      "authors": [
        "Xun Liang",
        "Rong-Hua Li",
        "Feiyu Xiong",
        "Zhiyu Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.14507.pdf",
      "abstract": "Large language models (LLMs) are expected to respond accurately but often\nexhibit deficient reasoning or generate hallucinatory content. To address\nthese, studies prefixed with ``Self-'' such as Self-Consistency, Self-Improve,\nand Self-Refine have been initiated. They share a commonality: involving LLMs\nevaluating and updating itself to mitigate the issues. Nonetheless, these\nefforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization without examining the motivations behind\nthese works.\n  In this paper, we summarize a theoretical framework, termed Internal\nConsistency, which offers unified explanations for phenomena such as the lack\nof reasoning and the presence of hallucinations. Internal Consistency assesses\nthe coherence among LLMs' latent layer, decoding layer, and response layer\nbased on sampling methodologies. Expanding upon the Internal Consistency\nframework, we introduce a streamlined yet effective theoretical framework\ncapable of mining Internal Consistency, named Self-Feedback. The Self-Feedback\nframework consists of two modules: Self-Evaluation and Self-Update. This\nframework has been employed in numerous studies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, ``Does Self-Feedback Really Work?'' We propose several critical\nviewpoints, including the ``Hourglass Evolution of Internal Consistency'',\n``Consistency Is (Almost) Correctness'' hypothesis, and ``The Paradox of Latent\nand Explicit Reasoning''. Furthermore, we outline promising directions for\nfuture research. We have open-sourced the experimental code, reference list,\nand statistical data, available at\nhttps://github.com/IAAR-Shanghai/ICSFSurvey.",
      "upvotes": 46
    },
    {
      "title": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference",
      "url": "https://huggingface.co/papers/2407.14057",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2407.14057.pdf",
      "abstract": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.",
      "upvotes": 44
    },
    {
      "title": "EVLM: An Efficient Vision-Language Model for Visual Understanding",
      "url": "https://huggingface.co/papers/2407.14177",
      "authors": [
        "Wei Yuan",
        "Bin Wen",
        "Changyi Liu",
        "Dewen Fan",
        "Fan Yang",
        "Di Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.14177.pdf",
      "abstract": "In the field of multi-modal language models, the majority of methods are\nbuilt on an architecture similar to LLaVA. These models use a single-layer ViT\nfeature as a visual prompt, directly feeding it into the language models\nalongside textual tokens. However, when dealing with long sequences of visual\nsignals or inputs such as videos, the self-attention mechanism of language\nmodels can lead to significant computational overhead. Additionally, using\nsingle-layer ViT features makes it challenging for large language models to\nperceive visual signals fully. This paper proposes an efficient multi-modal\nlanguage model to minimize computational costs while enabling the model to\nperceive visual signals as comprehensively as possible. Our method primarily\nincludes: (1) employing cross-attention to image-text interaction similar to\nFlamingo. (2) utilize hierarchical ViT features. (3) introduce the Mixture of\nExperts (MoE) mechanism to enhance model effectiveness. Our model achieves\ncompetitive scores on public multi-modal benchmarks and performs well in tasks\nsuch as image captioning and video captioning.",
      "upvotes": 42
    },
    {
      "title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities",
      "url": "https://huggingface.co/papers/2407.14482",
      "authors": [
        "Peng Xu",
        "Wei Ping"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.14482.pdf",
      "abstract": "In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge\nthe gap between open-access LLMs and leading proprietary models (e.g.,\nGPT-4-Turbo) in long-context understanding and retrieval-augmented generation\n(RAG) capabilities. These two capabilities are essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt and are\ncomplementary to each other, depending on the downstream tasks and\ncomputational budgets. We present a detailed continued training recipe to\nextend the context window of Llama3-70B-base from 8K to 128K tokens, along with\na three-stage instruction tuning process to enhance the model's\ninstruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\nachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context\nunderstanding tasks and surpasses it on the RAG benchmark. Interestingly, we\nfind that the state-of-the-art long-context retriever can alleviate the top-k\ncontext fragmentation issue in RAG, further improving RAG-based results for\nlong-context understanding tasks. We also provide extensive comparisons between\nRAG and long-context solutions using state-of-the-art long-context LLMs.",
      "upvotes": 25
    },
    {
      "title": "Stable Audio Open",
      "url": "https://huggingface.co/papers/2407.14358",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2407.14358.pdf",
      "abstract": "Open generative models are vitally important for the community, allowing for\nfine-tunes and serving as baselines when presenting new models. However, most\ncurrent text-to-audio models are private and not accessible for artists and\nresearchers to build upon. Here we describe the architecture and training\nprocess of a new open-weights text-to-audio model trained with Creative Commons\ndata. Our evaluation shows that the model's performance is competitive with the\nstate-of-the-art across various metrics. Notably, the reported FDopenl3 results\n(measuring the realism of the generations) showcase its potential for\nhigh-quality stereo sound synthesis at 44.1kHz.",
      "upvotes": 23
    },
    {
      "title": "VisFocus: Prompt-Guided Vision Encoders for OCR-Free Dense Document Understanding",
      "url": "https://huggingface.co/papers/2407.12594",
      "authors": [
        "Niv Nayman",
        "Sharon Fogel",
        "Inbal Lavi",
        "Shahar Tsiper",
        "Srikar Appalaraju",
        "Shai Mazor",
        "R. Manmatha"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12594.pdf",
      "abstract": "In recent years, notable advancements have been made in the domain of visual\ndocument understanding, with the prevailing architecture comprising a cascade\nof vision and language models. The text component can either be extracted\nexplicitly with the use of external OCR models in OCR-based approaches, or\nalternatively, the vision model can be endowed with reading capabilities in\nOCR-free approaches. Typically, the queries to the model are input exclusively\nto the language component, necessitating the visual features to encompass the\nentire document. In this paper, we present VisFocus, an OCR-free method\ndesigned to better exploit the vision encoder's capacity by coupling it\ndirectly with the language prompt. To do so, we replace the down-sampling\nlayers with layers that receive the input prompt and allow highlighting\nrelevant parts of the document, while disregarding others. We pair the\narchitecture enhancements with a novel pre-training task, using language\nmasking on a snippet of the document text fed to the visual encoder in place of\nthe prompt, to empower the model with focusing capabilities. Consequently,\nVisFocus learns to allocate its attention to text patches pertinent to the\nprovided prompt. Our experiments demonstrate that this prompt-guided visual\nencoding approach significantly improves performance, achieving\nstate-of-the-art results on various benchmarks.",
      "upvotes": 19
    },
    {
      "title": "Qalam : A Multimodal LLM for Arabic Optical Character and Handwriting Recognition",
      "url": "https://huggingface.co/papers/2407.13559",
      "authors": [
        "El Moatez Billah Nagoudi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.13559.pdf",
      "abstract": "Arabic Optical Character Recognition (OCR) and Handwriting Recognition (HWR)\npose unique challenges due to the cursive and context-sensitive nature of the\nArabic script. This study introduces Qalam, a novel foundation model designed\nfor Arabic OCR and HWR, built on a SwinV2 encoder and RoBERTa decoder\narchitecture. Our model significantly outperforms existing methods, achieving a\nWord Error Rate (WER) of just 0.80% in HWR tasks and 1.18% in OCR tasks. We\ntrain Qalam on a diverse dataset, including over 4.5 million images from Arabic\nmanuscripts and a synthetic dataset comprising 60k image-text pairs. Notably,\nQalam demonstrates exceptional handling of Arabic diacritics, a critical\nfeature in Arabic scripts. Furthermore, it shows a remarkable ability to\nprocess high-resolution inputs, addressing a common limitation in current OCR\nsystems. These advancements underscore Qalam's potential as a leading solution\nfor Arabic script recognition, offering a significant leap in accuracy and\nefficiency.",
      "upvotes": 13
    },
    {
      "title": "SciCode: A Research Coding Benchmark Curated by Scientists",
      "url": "https://huggingface.co/papers/2407.13168",
      "authors": [
        "Xinan Chen",
        "Cunwei Fan",
        "Xuefei Guo",
        "Roland Haas",
        "Pan Ji",
        "Kittithat Krongchon",
        "Yao Li",
        "Di Luo",
        "Hao Tong",
        "Kha Trinh",
        "Zihan Wang",
        "Minhui Zhu",
        "Kilian Lieret"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.13168.pdf",
      "abstract": "Since language models (LMs) now outperform average humans on many challenging\ntasks, it has become increasingly difficult to develop challenging,\nhigh-quality, and realistic evaluations. We address this issue by examining\nLMs' capabilities to generate code for solving real scientific research\nproblems. Incorporating input from scientists and AI researchers in 16 diverse\nnatural science sub-fields, including mathematics, physics, chemistry, biology,\nand materials science, we created a scientist-curated coding benchmark,\nSciCode. The problems in SciCode naturally factorize into multiple subproblems,\neach involving knowledge recall, reasoning, and code synthesis. In total,\nSciCode contains 338 subproblems decomposed from 80 challenging main problems.\nIt offers optional descriptions specifying useful scientific background\ninformation and scientist-annotated gold-standard solutions and test cases for\nevaluation. Claude3.5-Sonnet, the best-performing model among those tested, can\nsolve only 4.6% of the problems in the most realistic setting. We believe that\nSciCode demonstrates both contemporary LMs' progress towards becoming helpful\nscientific assistants and sheds light on the development and evaluation of\nscientific AI in the future.",
      "upvotes": 13
    },
    {
      "title": "The Vision of Autonomic Computing: Can LLMs Make It a Reality?",
      "url": "https://huggingface.co/papers/2407.14402",
      "authors": [
        "Fangkai Yang",
        "Gong Cheng",
        "Saravan Rajmohan",
        "Qi Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.14402.pdf",
      "abstract": "The Vision of Autonomic Computing (ACV), proposed over two decades ago,\nenvisions computing systems that self-manage akin to biological organisms,\nadapting seamlessly to changing environments. Despite decades of research,\nachieving ACV remains challenging due to the dynamic and complex nature of\nmodern computing systems. Recent advancements in Large Language Models (LLMs)\noffer promising solutions to these challenges by leveraging their extensive\nknowledge, language understanding, and task automation capabilities. This paper\nexplores the feasibility of realizing ACV through an LLM-based multi-agent\nframework for microservice management. We introduce a five-level taxonomy for\nautonomous service maintenance and present an online evaluation benchmark based\non the Sock Shop microservice demo project to assess our framework's\nperformance. Our findings demonstrate significant progress towards achieving\nLevel 3 autonomy, highlighting the effectiveness of LLMs in detecting and\nresolving issues within microservice architectures. This study contributes to\nadvancing autonomic computing by pioneering the integration of LLMs into\nmicroservice management frameworks, paving the way for more adaptive and\nself-managing computing systems. The code will be made available at\nhttps://aka.ms/ACV-LLM.",
      "upvotes": 13
    },
    {
      "title": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs",
      "url": "https://huggingface.co/papers/2407.10960",
      "authors": [
        "William Brandon",
        "Yoon Kim"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.10960.pdf",
      "abstract": "The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.",
      "upvotes": 11
    },
    {
      "title": "Phi-3 Safety Post-Training: Aligning Language Models with a \"Break-Fix\" Cycle",
      "url": "https://huggingface.co/papers/2407.13833",
      "authors": [
        "Daniel Perez-Becker",
        "Amit Garg",
        "Wen Wen",
        "Ziyi Yang",
        "Hiteshi Sharma",
        "Martin Pouliot",
        "Amanda Minnich",
        "Solianna Herrera",
        "Shahed Warreth",
        "Nina Chikanov",
        "Raja Sekhar Rao Dheekonda",
        "Bolor-Erdene Jagdagdorj"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.13833.pdf",
      "abstract": "Recent innovations in language model training have demonstrated that it is\npossible to create highly performant models that are small enough to run on a\nsmartphone. As these models are deployed in an increasing number of domains, it\nis critical to ensure that they are aligned with human preferences and safety\nconsiderations. In this report, we present our methodology for safety aligning\nthe Phi-3 series of language models. We utilized a \"break-fix\" cycle,\nperforming multiple rounds of dataset curation, safety post-training,\nbenchmarking, red teaming, and vulnerability identification to cover a variety\nof harm areas in both single and multi-turn scenarios. Our results indicate\nthat this approach iteratively improved the performance of the Phi-3 models\nacross a wide range of responsible AI benchmarks.",
      "upvotes": 11
    },
    {
      "title": "Visual Text Generation in the Wild",
      "url": "https://huggingface.co/papers/2407.14138",
      "authors": [
        "Feiyu Gao",
        "Wenyu Liu",
        "Peng Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.14138.pdf",
      "abstract": "Recently, with the rapid advancements of generative models, the field of\nvisual text generation has witnessed significant progress. However, it is still\nchallenging to render high-quality text images in real-world scenarios, as\nthree critical criteria should be satisfied: (1) Fidelity: the generated text\nimages should be photo-realistic and the contents are expected to be the same\nas specified in the given conditions; (2) Reasonability: the regions and\ncontents of the generated text should cohere with the scene; (3) Utility: the\ngenerated text images can facilitate related tasks (e.g., text detection and\nrecognition). Upon investigation, we find that existing methods, either\nrendering-based or diffusion-based, can hardly meet all these aspects\nsimultaneously, limiting their application range. Therefore, we propose in this\npaper a visual text generator (termed SceneVTG), which can produce high-quality\ntext images in the wild. Following a two-stage paradigm, SceneVTG leverages a\nMultimodal Large Language Model to recommend reasonable text regions and\ncontents across multiple scales and levels, which are used by a conditional\ndiffusion model as conditions to generate text images. Extensive experiments\ndemonstrate that the proposed SceneVTG significantly outperforms traditional\nrendering-based methods and recent diffusion-based methods in terms of fidelity\nand reasonability. Besides, the generated images provide superior utility for\ntasks involving text detection and text recognition. Code and datasets are\navailable at AdvancedLiterateMachinery.",
      "upvotes": 8
    },
    {
      "title": "Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders",
      "url": "https://huggingface.co/papers/2407.14435",
      "authors": [
        "Nicolas Sonnerat"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.14435.pdf",
      "abstract": "Sparse autoencoders (SAEs) are a promising unsupervised approach for\nidentifying causally relevant and interpretable linear features in a language\nmodel's (LM) activations. To be useful for downstream tasks, SAEs need to\ndecompose LM activations faithfully; yet to be interpretable the decomposition\nmust be sparse -- two objectives that are in tension. In this paper, we\nintroduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity\nat a given sparsity level on Gemma 2 9B activations, compared to other recent\nadvances such as Gated and TopK SAEs. We also show that this improvement does\nnot come at the cost of interpretability through manual and automated\ninterpretability studies. JumpReLU SAEs are a simple modification of vanilla\n(ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU\nactivation function -- and are similarly efficient to train and run. By\nutilising straight-through-estimators (STEs) in a principled manner, we show\nhow it is possible to train JumpReLU SAEs effectively despite the discontinuous\nJumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs\nto directly train L0 to be sparse, instead of training on proxies such as L1,\navoiding problems like shrinkage.",
      "upvotes": 6
    },
    {
      "title": "SparseCraft: Few-Shot Neural Reconstruction through Stereopsis Guided Geometric Linearization",
      "url": "https://huggingface.co/papers/2407.14257",
      "authors": [
        "Amine Ouasfi",
        "Adnane Boukhayma"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.14257.pdf",
      "abstract": "We present a novel approach for recovering 3D shape and view dependent\nappearance from a few colored images, enabling efficient 3D reconstruction and\nnovel view synthesis. Our method learns an implicit neural representation in\nthe form of a Signed Distance Function (SDF) and a radiance field. The model is\ntrained progressively through ray marching enabled volumetric rendering, and\nregularized with learning-free multi-view stereo (MVS) cues. Key to our\ncontribution is a novel implicit neural shape function learning strategy that\nencourages our SDF field to be as linear as possible near the level-set, hence\nrobustifying the training against noise emanating from the supervision and\nregularization signals. Without using any pretrained priors, our method, called\nSparseCraft, achieves state-of-the-art performances both in novel-view\nsynthesis and reconstruction from sparse views in standard benchmarks, while\nrequiring less than 10 minutes for training.",
      "upvotes": 5
    },
    {
      "title": "PlacidDreamer: Advancing Harmony in Text-to-3D Generation",
      "url": "https://huggingface.co/papers/2407.13976",
      "authors": [
        "Shuo Huang",
        "Shikun Sun",
        "Zixuan Wang",
        "Xiaoyu Qin",
        "Yanmin Xiong",
        "Yuan Zhang",
        "Di Zhang",
        "Jia Jia"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.13976.pdf",
      "abstract": "Recently, text-to-3D generation has attracted significant attention,\nresulting in notable performance enhancements. Previous methods utilize\nend-to-end 3D generation models to initialize 3D Gaussians, multi-view\ndiffusion models to enforce multi-view consistency, and text-to-image diffusion\nmodels to refine details with score distillation algorithms. However, these\nmethods exhibit two limitations. Firstly, they encounter conflicts in\ngeneration directions since different models aim to produce diverse 3D assets.\nSecondly, the issue of over-saturation in score distillation has not been\nthoroughly investigated and solved. To address these limitations, we propose\nPlacidDreamer, a text-to-3D framework that harmonizes initialization,\nmulti-view generation, and text-conditioned generation with a single multi-view\ndiffusion model, while simultaneously employing a novel score distillation\nalgorithm to achieve balanced saturation. To unify the generation direction, we\nintroduce the Latent-Plane module, a training-friendly plug-in extension that\nenables multi-view diffusion models to provide fast geometry reconstruction for\ninitialization and enhanced multi-view images to personalize the text-to-image\ndiffusion model. To address the over-saturation problem, we propose to view\nscore distillation as a multi-objective optimization problem and introduce the\nBalanced Score Distillation algorithm, which offers a Pareto Optimal solution\nthat achieves both rich details and balanced saturation. Extensive experiments\nvalidate the outstanding capabilities of our PlacidDreamer. The code is\navailable at https://github.com/HansenHuang0823/PlacidDreamer.",
      "upvotes": 5
    },
    {
      "title": "Efficient Audio Captioning with Encoder-Level Knowledge Distillation",
      "url": "https://huggingface.co/papers/2407.14329",
      "authors": [
        "Mengyue Wu",
        "Mark D. Plumbley"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.14329.pdf",
      "abstract": "Significant improvement has been achieved in automated audio captioning (AAC)\nwith recent models. However, these models have become increasingly large as\ntheir performance is enhanced. In this work, we propose a knowledge\ndistillation (KD) framework for AAC. Our analysis shows that in the\nencoder-decoder based AAC models, it is more effective to distill knowledge\ninto the encoder as compared with the decoder. To this end, we incorporate\nencoder-level KD loss into training, in addition to the standard supervised\nloss and sequence-level KD loss. We investigate two encoder-level KD methods,\nbased on mean squared error (MSE) loss and contrastive loss, respectively.\nExperimental results demonstrate that contrastive KD is more robust than MSE\nKD, exhibiting superior performance in data-scarce situations. By leveraging\naudio-only data into training in the KD framework, our student model achieves\ncompetitive performance, with an inference speed that is 19 times\nfasterAn online demo is available at\n\\url{https://huggingface.co/spaces/wsntxxn/efficient_audio_captioning}.",
      "upvotes": 4
    }
  ]
}