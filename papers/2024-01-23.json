{
  "date": "2024-01-23",
  "papers": [
    {
      "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text",
      "url": "https://huggingface.co/papers/2401.12070",
      "authors": [
        "Micah Goldblum"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.12070.pdf",
      "abstract": "Detecting text generated by modern large language models is thought to be\nhard, as both LLMs and humans can exhibit a wide range of complex behaviors.\nHowever, we find that a score based on contrasting two closely related language\nmodels is highly accurate at separating human-generated and machine-generated\ntext. Based on this mechanism, we propose a novel LLM detector that only\nrequires simple calculations using a pair of pre-trained LLMs. The method,\ncalled Binoculars, achieves state-of-the-art accuracy without any training\ndata. It is capable of spotting machine text from a range of modern LLMs\nwithout any model-specific modifications. We comprehensively evaluate\nBinoculars on a number of text sources and in varied situations. Over a wide\nrange of document types, Binoculars detects over 90% of generated samples from\nChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being\ntrained on any ChatGPT data.",
      "upvotes": 43
    },
    {
      "title": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",
      "url": "https://huggingface.co/papers/2401.11708",
      "authors": [
        "Zhaochen Yu",
        "Chenlin Meng",
        "Bin Cui"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.11708.pdf",
      "abstract": "Diffusion models have exhibit exceptional performance in text-to-image\ngeneration and editing. However, existing methods often face challenges when\nhandling complex text prompts that involve multiple objects with multiple\nattributes and relationships. In this paper, we propose a brand new\ntraining-free text-to-image generation/editing framework, namely Recaption,\nPlan and Generate (RPG), harnessing the powerful chain-of-thought reasoning\nability of multimodal LLMs to enhance the compositionality of text-to-image\ndiffusion models. Our approach employs the MLLM as a global planner to\ndecompose the process of generating complex images into multiple simpler\ngeneration tasks within subregions. We propose complementary regional diffusion\nto enable region-wise compositional generation. Furthermore, we integrate\ntext-guided image generation and editing within the proposed RPG in a\nclosed-loop fashion, thereby enhancing generalization ability. Extensive\nexperiments demonstrate our RPG outperforms state-of-the-art text-to-image\ndiffusion models, including DALL-E 3 and SDXL, particularly in multi-category\nobject composition and text-image semantic alignment. Notably, our RPG\nframework exhibits wide compatibility with various MLLM architectures (e.g.,\nMiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available\nat: https://github.com/YangLing0818/RPG-DiffusionMaster",
      "upvotes": 30
    },
    {
      "title": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark",
      "url": "https://huggingface.co/papers/2401.11944",
      "authors": [
        "Xinrun Du",
        "Yiming Liang",
        "Yuyang Cheng",
        "Shuyue Guo",
        "Haoran Zhang",
        "Xingwei Qu",
        "Yudong Liu",
        "Yu-Hsuan Tsai"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.11944.pdf",
      "abstract": "As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU.\n  CMMMU includes 12k manually collected multimodal questions from college\nexams, quizzes, and textbooks, covering six core disciplines: Art & Design,\nBusiness, Science, Health & Medicine, Humanities & Social Science, and Tech &\nEngineering, like its companion, MMMU. These questions span 30 subjects and\ncomprise 39 highly heterogeneous image types, such as charts, diagrams, maps,\ntables, music sheets, and chemical structures.\n  CMMMU focuses on complex perception and reasoning with domain-specific\nknowledge in the Chinese context. We evaluate 11 open-source LLMs and one\nproprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%,\nindicating a large space for improvement. CMMMU will boost the community to\nbuild the next-generation LMMs towards expert artificial intelligence and\npromote the democratization of LMMs by providing diverse language contexts.",
      "upvotes": 27
    },
    {
      "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities",
      "url": "https://huggingface.co/papers/2401.12168",
      "authors": [
        "Zhuo Xu",
        "Danny Driess",
        "Pete Florence",
        "Dorsa Sadigh",
        "Leonidas Guibas"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.12168.pdf",
      "abstract": "Understanding and reasoning about spatial relationships is a fundamental\ncapability for Visual Question Answering (VQA) and robotics. While Vision\nLanguage Models (VLM) have demonstrated remarkable performance in certain VQA\nbenchmarks, they still lack capabilities in 3D spatial reasoning, such as\nrecognizing quantitative relationships of physical objects like distances or\nsize differences. We hypothesize that VLMs' limited spatial reasoning\ncapability is due to the lack of 3D spatial knowledge in training data and aim\nto solve this problem by training VLMs with Internet-scale spatial reasoning\ndata. To this end, we present a system to facilitate this approach. We first\ndevelop an automatic 3D spatial VQA data generation framework that scales up to\n2 billion VQA examples on 10 million real-world images. We then investigate\nvarious factors in the training recipe, including data quality, training\npipeline, and VLM architecture. Our work features the first internet-scale 3D\nspatial reasoning dataset in metric space. By training a VLM on such data, we\nsignificantly enhance its ability on both qualitative and quantitative spatial\nVQA. Finally, we demonstrate that this VLM unlocks novel downstream\napplications in chain-of-thought spatial reasoning and robotics due to its\nquantitative estimation capability. Project website:\nhttps://spatial-vlm.github.io/",
      "upvotes": 26
    },
    {
      "title": "Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers",
      "url": "https://huggingface.co/papers/2401.11605",
      "authors": [
        "Alex Birch",
        "Tanishq Mathew Abraham",
        "Daniel Z. Kaplan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.11605.pdf",
      "abstract": "We present the Hourglass Diffusion Transformer (HDiT), an image generative\nmodel that exhibits linear scaling with pixel count, supporting training at\nhigh-resolution (e.g. 1024 times 1024) directly in pixel-space. Building on\nthe Transformer architecture, which is known to scale to billions of\nparameters, it bridges the gap between the efficiency of convolutional U-Nets\nand the scalability of Transformers. HDiT trains successfully without typical\nhigh-resolution training techniques such as multiscale architectures, latent\nautoencoders or self-conditioning. We demonstrate that HDiT performs\ncompetitively with existing models on ImageNet 256^2, and sets a new\nstate-of-the-art for diffusion models on FFHQ-1024^2.",
      "upvotes": 22
    },
    {
      "title": "CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation",
      "url": "https://huggingface.co/papers/2401.12208",
      "authors": [
        "Emily B. Tsai",
        "Andrew Johnston",
        "Cameron Olsen",
        "Sergios Gatidis"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.12208.pdf",
      "abstract": "Chest X-rays (CXRs) are the most frequently performed imaging test in\nclinical practice. Recent advances in the development of vision-language\nfoundation models (FMs) give rise to the possibility of performing automated\nCXR interpretation, which can assist physicians with clinical decision-making\nand improve patient outcomes. However, developing FMs that can accurately\ninterpret CXRs is challenging due to the (1) limited availability of\nlarge-scale vision-language datasets in the medical image domain, (2) lack of\nvision and language encoders that can capture the complexities of medical data,\nand (3) absence of evaluation frameworks for benchmarking the abilities of FMs\non CXR interpretation. In this work, we address these challenges by first\nintroducing CheXinstruct - a large-scale instruction-tuning dataset\ncurated from 28 publicly-available datasets. We then present CheXagent -\nan instruction-tuned FM capable of analyzing and summarizing CXRs. To build\nCheXagent, we design a clinical large language model (LLM) for parsing\nradiology reports, a vision encoder for representing CXR images, and a network\nto bridge the vision and language modalities. Finally, we introduce\nCheXbench - a novel benchmark designed to systematically evaluate FMs\nacross 8 clinically-relevant CXR interpretation tasks. Extensive quantitative\nevaluations and qualitative reviews with five expert radiologists demonstrate\nthat CheXagent outperforms previously-developed general- and medical-domain FMs\non CheXbench tasks. Furthermore, in an effort to improve model transparency, we\nperform a fairness evaluation across factors of sex, race and age to highlight\npotential performance disparities. Our project is at\nhttps://stanford-aimi.github.io/chexagent.html.",
      "upvotes": 22
    },
    {
      "title": "DITTO: Diffusion Inference-Time T-Optimization for Music Generation",
      "url": "https://huggingface.co/papers/2401.12179",
      "authors": [
        "Julian McAuley",
        "Taylor Berg-Kirkpatrick"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.12179.pdf",
      "abstract": "We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose\nframe-work for controlling pre-trained text-to-music diffusion models at\ninference-time via optimizing initial noise latents. Our method can be used to\noptimize through any differentiable feature matching loss to achieve a target\n(stylized) output and leverages gradient checkpointing for memory efficiency.\nWe demonstrate a surprisingly wide-range of applications for music generation\nincluding inpainting, outpainting, and looping as well as intensity, melody,\nand musical structure control - all without ever fine-tuning the underlying\nmodel. When we compare our approach against related training, guidance, and\noptimization-based methods, we find DITTO achieves state-of-the-art performance\non nearly all tasks, including outperforming comparable approaches on\ncontrollability, audio quality, and computational efficiency, thus opening the\ndoor for high-quality, flexible, training-free control of diffusion models.\nSound examples can be found at https://DITTO-Music.github.io/web/.",
      "upvotes": 20
    },
    {
      "title": "WARM: On the Benefits of Weight Averaged Reward Models",
      "url": "https://huggingface.co/papers/2401.12187",
      "authors": [
        "Nino Vieillard",
        "LÃ©onard Hussenot",
        "Robert Dadashi",
        "Geoffrey Cideron",
        "Olivier Bachem"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.12187.pdf",
      "abstract": "Aligning large language models (LLMs) with human preferences through\nreinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit\nfailures in the reward model (RM) to achieve seemingly high rewards without\nmeeting the underlying objectives. We identify two primary challenges when\ndesigning RMs to mitigate reward hacking: distribution shifts during the RL\nprocess and inconsistencies in human preferences. As a solution, we propose\nWeight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then\naveraging them in the weight space. This strategy follows the observation that\nfine-tuned weights remain linearly mode connected when sharing the same\npre-training. By averaging weights, WARM improves efficiency compared to the\ntraditional ensembling of predictions, while improving reliability under\ndistribution shifts and robustness to preference inconsistencies. Our\nexperiments on summarization tasks, using best-of-N and RL methods, shows that\nWARM improves the overall quality and alignment of LLM predictions; for\nexample, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy\nRL fine-tuned with a single RM.",
      "upvotes": 18
    },
    {
      "title": "EmerDiff: Emerging Pixel-level Semantic Knowledge in Diffusion Models",
      "url": "https://huggingface.co/papers/2401.11739",
      "authors": [
        "Sanja Fidler"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.11739.pdf",
      "abstract": "Diffusion models have recently received increasing research attention for\ntheir remarkable transfer abilities in semantic segmentation tasks. However,\ngenerating fine-grained segmentation masks with diffusion models often requires\nadditional training on annotated datasets, leaving it unclear to what extent\npre-trained diffusion models alone understand the semantic relations of their\ngenerated images. To address this question, we leverage the semantic knowledge\nextracted from Stable Diffusion (SD) and aim to develop an image segmentor\ncapable of generating fine-grained segmentation maps without any additional\ntraining. The primary difficulty stems from the fact that semantically\nmeaningful feature maps typically exist only in the spatially lower-dimensional\nlayers, which poses a challenge in directly extracting pixel-level semantic\nrelations from these feature maps. To overcome this issue, our framework\nidentifies semantic correspondences between image pixels and spatial locations\nof low-dimensional feature maps by exploiting SD's generation process and\nutilizes them for constructing image-resolution segmentation maps. In extensive\nexperiments, the produced segmentation maps are demonstrated to be well\ndelineated and capture detailed parts of the images, indicating the existence\nof highly accurate pixel-level semantic knowledge in diffusion models.",
      "upvotes": 17
    },
    {
      "title": "Make-A-Shape: a Ten-Million-scale 3D Shape Model",
      "url": "https://huggingface.co/papers/2401.11067",
      "authors": [
        "Aditya Sanghi",
        "Kamal Rahimi Malekshan",
        "Hooman Shayani",
        "Chi-Wing Fu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.11067.pdf",
      "abstract": "Significant progress has been made in training large generative models for\nnatural language and images. Yet, the advancement of 3D generative models is\nhindered by their substantial resource demands for training, along with\ninefficient, non-compact, and less expressive representations. This paper\nintroduces Make-A-Shape, a new 3D generative model designed for efficient\ntraining on a vast scale, capable of utilizing 10 millions publicly-available\nshapes. Technical-wise, we first innovate a wavelet-tree representation to\ncompactly encode shapes by formulating the subband coefficient filtering scheme\nto efficiently exploit coefficient relations. We then make the representation\ngeneratable by a diffusion model by devising the subband coefficients packing\nscheme to layout the representation in a low-resolution grid. Further, we\nderive the subband adaptive training strategy to train our model to effectively\nlearn to generate coarse and detail wavelet coefficients. Last, we extend our\nframework to be controlled by additional input conditions to enable it to\ngenerate shapes from assorted modalities, e.g., single/multi-view images, point\nclouds, and low-resolution voxels. In our extensive set of experiments, we\ndemonstrate various applications, such as unconditional generation, shape\ncompletion, and conditional generation on a wide range of modalities. Our\napproach not only surpasses the state of the art in delivering high-quality\nresults but also efficiently generates shapes within a few seconds, often\nachieving this in just 2 seconds for most conditions.",
      "upvotes": 16
    },
    {
      "title": "StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion",
      "url": "https://huggingface.co/papers/2401.11053",
      "authors": [
        "Zhichao Wang",
        "Yuanzhe Chen",
        "Xinsheng Wang",
        "Zhuo Chen",
        "Lei Xie",
        "Yuping Wang",
        "Yuxuan Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.11053.pdf",
      "abstract": "Recent language model (LM) advancements have showcased impressive zero-shot\nvoice conversion (VC) performance. However, existing LM-based VC models usually\napply offline conversion from source semantics to acoustic features, demanding\nthe complete source speech, and limiting their deployment to real-time\napplications. In this paper, we introduce StreamVoice, a novel streaming\nLM-based model for zero-shot VC, facilitating real-time conversion given\narbitrary speaker prompts and source speech. Specifically, to enable streaming\ncapability, StreamVoice employs a fully causal context-aware LM with a\ntemporal-independent acoustic predictor, while alternately processing semantic\nand acoustic features at each time step of autoregression which eliminates the\ndependence on complete source speech. To address the potential performance\ndegradation from the incomplete context in streaming processing, we enhance the\ncontext-awareness of the LM through two strategies: 1) teacher-guided context\nforesight, using a teacher model to summarize the present and future semantic\ncontext during training to guide the model's forecasting for missing context;\n2) semantic masking strategy, promoting acoustic prediction from preceding\ncorrupted semantic and acoustic input, enhancing context-learning ability.\nNotably, StreamVoice is the first LM-based streaming zero-shot VC model without\nany future look-ahead. Experimental results demonstrate StreamVoice's streaming\nconversion capability while maintaining zero-shot performance comparable to\nnon-streaming VC systems.",
      "upvotes": 10
    },
    {
      "title": "OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics",
      "url": "https://huggingface.co/papers/2401.12202",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2401.12202.pdf",
      "abstract": "Remarkable progress has been made in recent years in the fields of vision,\nlanguage, and robotics. We now have vision models capable of recognizing\nobjects based on language queries, navigation systems that can effectively\ncontrol mobile systems, and grasping models that can handle a wide range of\nobjects. Despite these advancements, general-purpose applications of robotics\nstill lag behind, even though they rely on these fundamental capabilities of\nrecognition, navigation, and grasping. In this paper, we adopt a systems-first\napproach to develop a new Open Knowledge-based robotics framework called\nOK-Robot. By combining Vision-Language Models (VLMs) for object detection,\nnavigation primitives for movement, and grasping primitives for object\nmanipulation, OK-Robot offers a integrated solution for pick-and-drop\noperations without requiring any training. To evaluate its performance, we run\nOK-Robot in 10 real-world home environments. The results demonstrate that\nOK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks,\nrepresenting a new state-of-the-art in Open Vocabulary Mobile Manipulation\n(OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered\nenvironments, OK-Robot's performance increases to 82%. However, the most\nimportant insight gained from OK-Robot is the critical role of nuanced details\nwhen combining Open Knowledge systems like VLMs with robotic modules. Videos of\nour experiments are available on our website: https://ok-robot.github.io",
      "upvotes": 10
    },
    {
      "title": "UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures",
      "url": "https://huggingface.co/papers/2401.11078",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2401.11078.pdf",
      "abstract": "Recent advances in 3D avatar generation have gained significant attentions.\nThese breakthroughs aim to produce more realistic animatable avatars, narrowing\nthe gap between virtual and real-world experiences. Most of existing works\nemploy Score Distillation Sampling (SDS) loss, combined with a differentiable\nrenderer and text condition, to guide a diffusion model in generating 3D\navatars. However, SDS often generates oversmoothed results with few facial\ndetails, thereby lacking the diversity compared with ancestral sampling. On the\nother hand, other works generate 3D avatar from a single image, where the\nchallenges of unwanted lighting effects, perspective views, and inferior image\nquality make them difficult to reliably reconstruct the 3D face meshes with the\naligned complete textures. In this paper, we propose a novel 3D avatar\ngeneration approach termed UltrAvatar with enhanced fidelity of geometry, and\nsuperior quality of physically based rendering (PBR) textures without unwanted\nlighting. To this end, the proposed approach presents a diffuse color\nextraction model and an authenticity guided texture diffusion model. The former\nremoves the unwanted lighting effects to reveal true diffuse colors so that the\ngenerated avatars can be rendered under various lighting conditions. The latter\nfollows two gradient-based guidances for generating PBR textures to render\ndiverse face-identity features and details better aligning with 3D mesh\ngeometry. We demonstrate the effectiveness and robustness of the proposed\nmethod, outperforming the state-of-the-art methods by a large margin in the\nexperiments.",
      "upvotes": 7
    },
    {
      "title": "Single-View 3D Human Digitalization with Large Reconstruction Models",
      "url": "https://huggingface.co/papers/2401.12175",
      "authors": [
        "Jingyuan Liu",
        "Hao Tan",
        "Zhan Xu",
        "Yang Zhou",
        "Serena Yeung-Levy"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.12175.pdf",
      "abstract": "In this paper, we introduce Human-LRM, a single-stage feed-forward Large\nReconstruction Model designed to predict human Neural Radiance Fields (NeRF)\nfrom a single image. Our approach demonstrates remarkable adaptability in\ntraining using extensive datasets containing 3D scans and multi-view capture.\nFurthermore, to enhance the model's applicability for in-the-wild scenarios\nespecially with occlusions, we propose a novel strategy that distills\nmulti-view reconstruction into single-view via a conditional triplane diffusion\nmodel. This generative extension addresses the inherent variations in human\nbody shapes when observed from a single view, and makes it possible to\nreconstruct the full body human from an occluded image. Through extensive\nexperiments, we show that Human-LRM surpasses previous methods by a significant\nmargin on several benchmarks.",
      "upvotes": 6
    },
    {
      "title": "Scaling Face Interaction Graph Networks to Real World Scenes",
      "url": "https://huggingface.co/papers/2401.11985",
      "authors": [
        "Tatiana Lopez-Guevara",
        "Yulia Rubanova",
        "William F. Whitney",
        "Tobias Pfaff",
        "Kimberly Stachenfeld",
        "Kelsey R. Allen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.11985.pdf",
      "abstract": "Accurately simulating real world object dynamics is essential for various\napplications such as robotics, engineering, graphics, and design. To better\ncapture complex real dynamics such as contact and friction, learned simulators\nbased on graph networks have recently shown great promise. However, applying\nthese learned simulators to real scenes comes with two major challenges: first,\nscaling learned simulators to handle the complexity of real world scenes which\ncan involve hundreds of objects each with complicated 3D shapes, and second,\nhandling inputs from perception rather than 3D state information. Here we\nintroduce a method which substantially reduces the memory required to run\ngraph-based learned simulators. Based on this memory-efficient simulation\nmodel, we then present a perceptual interface in the form of editable NeRFs\nwhich can convert real-world scenes into a structured representation that can\nbe processed by graph network simulator. We show that our method uses\nsubstantially less memory than previous graph-based simulators while retaining\ntheir accuracy, and that the simulators learned in synthetic environments can\nbe applied to real world scenes captured from multiple camera angles. This\npaves the way for expanding the application of learned simulators to settings\nwhere only perceptual information is available at inference time.",
      "upvotes": 3
    },
    {
      "title": "Fast Registration of Photorealistic Avatars for VR Facial Animation",
      "url": "https://huggingface.co/papers/2401.11002",
      "authors": [
        "Te-Li Wang",
        "Jason Saragih",
        "Shih-En Wei"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.11002.pdf",
      "abstract": "Virtual Reality (VR) bares promise of social interactions that can feel more\nimmersive than other media. Key to this is the ability to accurately animate a\nphotorealistic avatar of one's likeness while wearing a VR headset. Although\nhigh quality registration of person-specific avatars to headset-mounted camera\n(HMC) images is possible in an offline setting, the performance of generic\nrealtime models are significantly degraded. Online registration is also\nchallenging due to oblique camera views and differences in modality. In this\nwork, we first show that the domain gap between the avatar and headset-camera\nimages is one of the primary sources of difficulty, where a transformer-based\narchitecture achieves high accuracy on domain-consistent data, but degrades\nwhen the domain-gap is re-introduced. Building on this finding, we develop a\nsystem design that decouples the problem into two parts: 1) an iterative\nrefinement module that takes in-domain inputs, and 2) a generic avatar-guided\nimage-to-image style transfer module that is conditioned on current estimation\nof expression and head pose. These two modules reinforce each other, as image\nstyle transfer becomes easier when close-to-ground-truth examples are shown,\nand better domain-gap removal helps registration. Our system produces\nhigh-quality results efficiently, obviating the need for costly offline\nregistration to generate personalized labels. We validate the accuracy and\nefficiency of our approach through extensive experiments on a commodity\nheadset, demonstrating significant improvements over direct regression methods\nas well as offline registration.",
      "upvotes": 2
    }
  ]
}