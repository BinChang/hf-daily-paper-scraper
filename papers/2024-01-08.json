{
  "date": "2024-01-08",
  "papers": [
    {
      "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
      "url": "https://huggingface.co/papers/2401.02954",
      "authors": [
        "DeepSeek-AI",
        "Xiao Bi",
        "Deli Chen",
        "Guanting Chen",
        "Shanhuang Chen",
        "Chengqi Deng",
        "Honghui Ding",
        "Kai Dong",
        "Qiushi Du",
        "Zhe Fu",
        "Huazuo Gao",
        "Kaige Gao",
        "Wenjun Gao",
        "Ruiqi Ge",
        "Kang Guan",
        "Jianzhong Guo",
        "Guangbo Hao",
        "Zhewen Hao",
        "Ying He",
        "Wenjie Hu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.02954.pdf",
      "abstract": "The rapid development of open-source large language models (LLMs) has been\ntruly remarkable. However, the scaling law described in previous literature\npresents varying conclusions, which casts a dark cloud over scaling LLMs. We\ndelve into the study of scaling laws and present our distinctive findings that\nfacilitate scaling of large scale models in two commonly used open-source\nconfigurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek\nLLM, a project dedicated to advancing open-source language models with a\nlong-term perspective. To support the pre-training phase, we have developed a\ndataset that currently consists of 2 trillion tokens and is continuously\nexpanding. We further conduct supervised fine-tuning (SFT) and Direct\nPreference Optimization (DPO) on DeepSeek LLM Base models, resulting in the\ncreation of DeepSeek Chat models. Our evaluation results demonstrate that\nDeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in\nthe domains of code, mathematics, and reasoning. Furthermore, open-ended\nevaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance\ncompared to GPT-3.5.",
      "upvotes": 41
    },
    {
      "title": "DocGraphLM: Documental Graph Language Model for Information Extraction",
      "url": "https://huggingface.co/papers/2401.02823",
      "authors": [
        "Kang Gu",
        "Sameena Shah"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.02823.pdf",
      "abstract": "Advances in Visually Rich Document Understanding (VrDU) have enabled\ninformation extraction and question answering over documents with complex\nlayouts. Two tropes of architectures have emerged -- transformer-based models\ninspired by LLMs, and Graph Neural Networks. In this paper, we introduce\nDocGraphLM, a novel framework that combines pre-trained language models with\ngraph semantics. To achieve this, we propose 1) a joint encoder architecture to\nrepresent documents, and 2) a novel link prediction approach to reconstruct\ndocument graphs. DocGraphLM predicts both directions and distances between\nnodes using a convergent joint loss function that prioritizes neighborhood\nrestoration and downweighs distant node detection. Our experiments on three\nSotA datasets show consistent improvement on IE and QA tasks with the adoption\nof graph features. Moreover, we report that adopting the graph features\naccelerates convergence in the learning process during training, despite being\nsolely constructed through link prediction.",
      "upvotes": 35
    },
    {
      "title": "Denoising Vision Transformers",
      "url": "https://huggingface.co/papers/2401.02957",
      "authors": [
        "Katie Z Luo",
        "Yue Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.02957.pdf",
      "abstract": "We delve into a nuanced but significant challenge inherent to Vision\nTransformers (ViTs): feature maps of these models exhibit grid-like artifacts,\nwhich detrimentally hurt the performance of ViTs in downstream tasks. Our\ninvestigations trace this fundamental issue down to the positional embeddings\nat the input stage. To address this, we propose a novel noise model, which is\nuniversally applicable to all ViTs. Specifically, the noise model dissects ViT\noutputs into three components: a semantics term free from noise artifacts and\ntwo artifact-related terms that are conditioned on pixel locations. Such a\ndecomposition is achieved by enforcing cross-view feature consistency with\nneural fields in a per-image basis. This per-image optimization process\nextracts artifact-free features from raw ViT outputs, providing clean features\nfor offline applications. Expanding the scope of our solution to support online\nfunctionality, we introduce a learnable denoiser to predict artifact-free\nfeatures directly from unprocessed ViT outputs, which shows remarkable\ngeneralization capabilities to novel data without the need for per-image\noptimization. Our two-stage approach, termed Denoising Vision Transformers\n(DVT), does not require re-training existing pre-trained ViTs and is\nimmediately applicable to any Transformer-based architecture. We evaluate our\nmethod on a variety of representative ViTs (DINO, MAE, DeiT-III, EVA02, CLIP,\nDINOv2, DINOv2-reg). Extensive evaluations demonstrate that our DVT\nconsistently and significantly improves existing state-of-the-art\ngeneral-purpose models in semantic and geometric tasks across multiple datasets\n(e.g., +3.84 mIoU). We hope our study will encourage a re-evaluation of ViT\ndesign, especially regarding the naive use of positional embeddings.",
      "upvotes": 28
    },
    {
      "title": "Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss",
      "url": "https://huggingface.co/papers/2401.02677",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2401.02677.pdf",
      "abstract": "Stable Diffusion XL (SDXL) has become the best open source text-to-image\nmodel (T2I) for its versatility and top-notch image quality. Efficiently\naddressing the computational demands of SDXL models is crucial for wider reach\nand applicability. In this work, we introduce two scaled-down variants, Segmind\nStable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter\nUNets, respectively, achieved through progressive removal using layer-level\nlosses focusing on reducing the model size while preserving generative quality.\nWe release these models weights at https://hf.co/Segmind. Our methodology\ninvolves the elimination of residual networks and transformer blocks from the\nU-Net structure of SDXL, resulting in significant reductions in parameters, and\nlatency. Our compact models effectively emulate the original SDXL by\ncapitalizing on transferred knowledge, achieving competitive results against\nlarger multi-billion parameter SDXL. Our work underscores the efficacy of\nknowledge distillation coupled with layer-level losses in reducing model size\nwhile preserving the high-quality generative capabilities of SDXL, thus\nfacilitating more accessible deployment in resource-constrained environments.",
      "upvotes": 22
    },
    {
      "title": "Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively",
      "url": "https://huggingface.co/papers/2401.02955",
      "authors": [
        "Kai Chen",
        "Chen Change Loy"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.02955.pdf",
      "abstract": "The CLIP and Segment Anything Model (SAM) are remarkable vision foundation\nmodels (VFMs). SAM excels in segmentation tasks across diverse domains, while\nCLIP is renowned for its zero-shot recognition capabilities. This paper\npresents an in-depth exploration of integrating these two models into a unified\nframework. Specifically, we introduce the Open-Vocabulary SAM, a SAM-inspired\nmodel designed for simultaneous interactive segmentation and recognition,\nleveraging two unique knowledge transfer modules: SAM2CLIP and CLIP2SAM. The\nformer adapts SAM's knowledge into the CLIP via distillation and learnable\ntransformer adapters, while the latter transfers CLIP knowledge into SAM,\nenhancing its recognition capabilities. Extensive experiments on various\ndatasets and detectors show the effectiveness of Open-Vocabulary SAM in both\nsegmentation and recognition tasks, significantly outperforming the naive\nbaselines of simply combining SAM and CLIP. Furthermore, aided with image\nclassification data training, our method can segment and recognize\napproximately 22,000 classes.",
      "upvotes": 20
    },
    {
      "title": "Pheme: Efficient and Conversational Speech Generation",
      "url": "https://huggingface.co/papers/2401.02839",
      "authors": [],
      "pdf_url": "https://arxiv.org/pdf/2401.02839.pdf",
      "abstract": "In recent years, speech generation has seen remarkable progress, now\nachieving one-shot generation capability that is often virtually\nindistinguishable from real human voice. Integrating such advancements in\nspeech generation with large language models might revolutionize a wide range\nof applications. However, certain applications, such as assistive\nconversational systems, require natural and conversational speech generation\ntools that also operate efficiently in real time. Current state-of-the-art\nmodels like VALL-E and SoundStorm, powered by hierarchical neural audio codecs,\nrequire large neural components and extensive training data to work well. In\ncontrast, MQTTS aims to build more compact conversational TTS models while\ncapitalizing on smaller-scale real-life conversational speech data. However,\nits autoregressive nature yields high inference latency and thus limits its\nreal-time usage. In order to mitigate the current limitations of the\nstate-of-the-art TTS models while capitalizing on their strengths, in this work\nwe introduce the Pheme model series that 1) offers compact yet high-performing\nmodels, 2) allows for parallel speech generation of 3) natural conversational\nspeech, and 4) it can be trained efficiently on smaller-scale conversational\ndata, cutting data demands by more than 10x but still matching the quality of\nthe autoregressive TTS models. We also show that through simple teacher-student\ndistillation we can meet significant improvements in voice quality for\nsingle-speaker setups on top of pretrained Pheme checkpoints, relying solely on\nsynthetic speech generated by much larger teacher models. Audio samples and\npretrained models are available online.",
      "upvotes": 17
    },
    {
      "title": "Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache",
      "url": "https://huggingface.co/papers/2401.02669",
      "authors": [
        "Bin Lin",
        "Tao Peng",
        "Chen Zhang",
        "Lanbo Li",
        "Wencong Xiao",
        "Qi Xu",
        "Shen Li",
        "Zhigang Ji",
        "Yong Li",
        "Wei Lin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.02669.pdf",
      "abstract": "The rapid proliferation of Large Language Models (LLMs) has been a driving\nforce in the growth of cloud-based LLM services, which are now integral to\nadvancing AI applications. However, the dynamic auto-regressive nature of LLM\nservice, along with the need to support exceptionally long context lengths,\ndemands the flexible allocation and release of substantial resources. This\npresents considerable challenges in designing cloud-based LLM service systems,\nwhere inefficient management can lead to performance degradation or resource\nwastage. In response to these challenges, this work introduces DistAttention, a\nnovel distributed attention algorithm that segments the KV Cache into smaller,\nmanageable units, enabling distributed processing and storage of the attention\nmodule. Based on that, we propose DistKV-LLM, a distributed LLM serving system\nthat dynamically manages KV Cache and effectively orchestrates all accessible\nGPU and CPU memories spanning across the data center. This ensures a\nhigh-performance LLM service on the cloud, adaptable to a broad range of\ncontext lengths. Validated in a cloud environment with 32 NVIDIA A100 GPUs in\nconfigurations from 2 to 32 instances, our system exhibited 1.03-2.4x\nend-to-end throughput improvements and supported context lengths 2-19x longer\nthan current state-of-the-art LLM service systems, as evidenced by extensive\ntesting across 18 datasets with context lengths up to 1,900K.",
      "upvotes": 14
    }
  ]
}