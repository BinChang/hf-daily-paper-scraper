{
  "date": "2024-07-12",
  "papers": [
    {
      "title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On",
      "url": "https://huggingface.co/papers/2407.08348",
      "authors": [
        "Liangjun Zhong",
        "Liu Yang",
        "Cheng Cheng",
        "Rui Hu",
        "Yang Liu",
        "Shuicheng Yan",
        "Han Fang",
        "Yahui Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08348.pdf",
      "abstract": "In this paper, we investigate the underlying factors that potentially enhance\nthe mathematical reasoning capabilities of large language models (LLMs). We\nargue that the data scaling law for math reasoning capabilities in modern LLMs\nis far from being saturated, highlighting how the model's quality improves with\nincreases in data quantity. To support this claim, we introduce the\nSkywork-Math model series, supervised fine-tuned (SFT) on common 7B LLMs using\nour proposed 2.5M-instance Skywork-MathQA dataset. Skywork-Math 7B has achieved\nimpressive accuracies of 51.2% on the competition-level MATH benchmark and\n83.9% on the GSM8K benchmark using only SFT data, outperforming an early\nversion of GPT-4 on MATH. The superior performance of Skywork-Math models\ncontributes to our novel two-stage data synthesis and model SFT pipelines,\nwhich include three different augmentation methods and a diverse seed problem\nset, ensuring both the quantity and quality of Skywork-MathQA dataset across\nvarying difficulty levels. Most importantly, we provide several practical\ntakeaways to enhance math reasoning abilities in LLMs for both research and\nindustry applications.",
      "upvotes": 50
    },
    {
      "title": "Video Diffusion Alignment via Reward Gradients",
      "url": "https://huggingface.co/papers/2407.08737",
      "authors": [
        "Russell Mendonca",
        "Katerina Fragkiadaki",
        "Deepak Pathak"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08737.pdf",
      "abstract": "We have made significant progress towards building foundational video\ndiffusion models. As these models are trained using large-scale unsupervised\ndata, it has become crucial to adapt these models to specific downstream tasks.\nAdapting these models via supervised fine-tuning requires collecting target\ndatasets of videos, which is challenging and tedious. In this work, we utilize\npre-trained reward models that are learned via preferences on top of powerful\nvision discriminative models to adapt video diffusion models. These models\ncontain dense gradient information with respect to generated RGB pixels, which\nis critical to efficient learning in complex search spaces, such as videos. We\nshow that backpropagating gradients from these reward models to a video\ndiffusion model can allow for compute and sample efficient alignment of the\nvideo diffusion model. We show results across a variety of reward models and\nvideo diffusion models, demonstrating that our approach can learn much more\nefficiently in terms of reward queries and computation than prior gradient-free\napproaches. Our code, model weights,and more visualization are available at\nhttps://vader-vid.github.io.",
      "upvotes": 47
    },
    {
      "title": "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model",
      "url": "https://huggingface.co/papers/2407.07053",
      "authors": [
        "Yongliang Shen",
        "Zeqi Tan",
        "Guiyang Hou",
        "Mingqian He",
        "Yanna Ma",
        "Weiming Lu",
        "Yueting Zhuang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.07053.pdf",
      "abstract": "Although most current large multimodal models (LMMs) can already understand\nphotos of natural scenes and portraits, their understanding of abstract images,\ne.g., charts, maps, or layouts, and visual reasoning capabilities remains quite\nrudimentary. They often struggle with simple daily tasks, such as reading time\nfrom a clock, understanding a flowchart, or planning a route using a road map.\nIn light of this, we design a multi-modal self-instruct, utilizing large\nlanguage models and their code capabilities to synthesize massive abstract\nimages and visual reasoning instructions across daily scenarios. Our strategy\neffortlessly creates a multimodal benchmark with 11,193 instructions for eight\nvisual scenarios: charts, tables, simulated maps, dashboards, flowcharts,\nrelation graphs, floor plans, and visual puzzles. This benchmark,\nconstructed with simple lines and geometric elements, exposes the shortcomings\nof most advanced LMMs like Claude-3.5-Sonnet and GPT-4o in abstract image\nunderstanding, spatial relations reasoning, and visual element induction.\nBesides, to verify the quality of our synthetic data, we fine-tune an LMM using\n62,476 synthetic chart, table and road map instructions. The results\ndemonstrate improved chart understanding and map navigation performance, and\nalso demonstrate potential benefits for other visual reasoning tasks. Our code\nis available at: https://github.com/zwq2018/Multi-modal-Self-instruct.",
      "upvotes": 41
    },
    {
      "title": "Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients",
      "url": "https://huggingface.co/papers/2407.08296",
      "authors": [
        "Lu Yin",
        "Shiwei Liu",
        "Zhangyang Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08296.pdf",
      "abstract": "Training Large Language Models (LLMs) is memory-intensive due to the large\nnumber of parameters and associated optimization states. GaLore, a recent\nmethod, reduces memory usage by projecting weight gradients into a low-rank\nsubspace without compromising performance. However, GaLore relies on\ntime-consuming Singular Value Decomposition (SVD) operations to identify the\nsubspace, and the frequent subspace updates lead to significant training time\noverhead. Moreover, GaLore offers minimal improvements in accuracy and\nefficiency compared to LoRA in more accessible fine-tuning scenarios. To\naddress these limitations, we introduce Q-Galore, a novel approach that\nsubstantially reduces memory usage by combining quantization and low-rank\nprojection, surpassing the benefits of GaLore. Our method is based on two key\nobservations: (i) the gradient subspace exhibits diverse properties, with some\nlayers converging early in training while others are subject to frequent\nchanges; (ii) the projection matrices are highly resilient to low-bit\nquantization. Leveraging these insights, Q-GaLore adaptively updates the\ngradient subspace based on its convergence statistics, achieving comparable\nperformance while significantly reducing the number of SVD operations. We\nmaintain the projection matrices in INT4 format and weights in INT8 format,\nincorporating stochastic rounding to capture accumulated gradient information.\nThis approach enables a high-precision training trajectory using only\nlow-precision weights. We demonstrate that Q-GaLore achieves highly competitive\nperformance with exceptional memory efficiency. At pre-training, Q-GaLore\nfacilitates training a LLaMA-7B model from scratch on a single NVIDIA RTX 4060\nTi with only 16 GB memory. At fine-tuning, it reduces memory consumption by up\nto 50% compared to LoRA and GaLore, while consistently outperforming QLoRA at\nthe same memory cost.",
      "upvotes": 31
    },
    {
      "title": "MAVIS: Mathematical Visual Instruction Tuning",
      "url": "https://huggingface.co/papers/2407.08739",
      "authors": [
        "Xinyu Wei",
        "Yichi Zhang",
        "Chengzhuo Tong",
        "Jiaming Liu",
        "Bin Wei",
        "Shanghang Zhang",
        "Peng Gao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08739.pdf",
      "abstract": "Multi-modal Large Language Models (MLLMs) have recently emerged as a\nsignificant focus in academia and industry. Despite their proficiency in\ngeneral multi-modal scenarios, the mathematical problem-solving capabilities in\nvisual contexts remain insufficiently explored. We identify three key areas\nwithin MLLMs that need to be improved: visual encoding of math diagrams,\ndiagram-language alignment, and mathematical reasoning skills. This draws forth\nan urgent demand for large-scale, high-quality data and training pipelines in\nvisual mathematics. In this paper, we propose MAVIS, the first MAthematical\nVISual instruction tuning paradigm for MLLMs, involving a series of\nmathematical visual datasets and specialized MLLMs. Targeting the three issues,\nMAVIS contains three progressive training stages from scratch. First, we curate\nMAVIS-Caption, consisting of 558K diagram-caption pairs, to fine-tune a\nmath-specific vision encoder (CLIP-Math) through contrastive learning, tailored\nfor improved diagram visual encoding. Second, we utilize MAVIS-Caption to align\nthe CLIP-Math with a large language model (LLM) by a projection layer,\nenhancing vision-language alignment in mathematical domains. Third, we\nintroduce MAVIS-Instruct, including 900K meticulously collected and annotated\nvisual math problems, which is adopted to finally instruct-tune the MLLM for\nrobust mathematical reasoning skills. In MAVIS-Instruct, we incorporate\ncomplete chain-of-thought (CoT) rationales for each problem, and minimize\ntextual redundancy, thereby concentrating the model towards the visual\nelements. Data and Models are released at https://github.com/ZrrSkywalker/MAVIS",
      "upvotes": 30
    },
    {
      "title": "MambaVision: A Hybrid Mamba-Transformer Vision Backbone",
      "url": "https://huggingface.co/papers/2407.08083",
      "authors": [
        "Jan Kautz"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08083.pdf",
      "abstract": "We propose a novel hybrid Mamba-Transformer backbone, denoted as MambaVision,\nwhich is specifically tailored for vision applications. Our core contribution\nincludes redesigning the Mamba formulation to enhance its capability for\nefficient modeling of visual features. In addition, we conduct a comprehensive\nablation study on the feasibility of integrating Vision Transformers (ViT) with\nMamba. Our results demonstrate that equipping the Mamba architecture with\nseveral self-attention blocks at the final layers greatly improves the modeling\ncapacity to capture long-range spatial dependencies. Based on our findings, we\nintroduce a family of MambaVision models with a hierarchical architecture to\nmeet various design criteria. For Image classification on ImageNet-1K dataset,\nMambaVision model variants achieve a new State-of-the-Art (SOTA) performance in\nterms of Top-1 accuracy and image throughput. In downstream tasks such as\nobject detection, instance segmentation and semantic segmentation on MS COCO\nand ADE20K datasets, MambaVision outperforms comparably-sized backbones and\ndemonstrates more favorable performance. Code:\nhttps://github.com/NVlabs/MambaVision.",
      "upvotes": 27
    },
    {
      "title": "Self-Recognition in Language Models",
      "url": "https://huggingface.co/papers/2407.06946",
      "authors": [
        "Giuseppe Russo",
        "Robert West"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06946.pdf",
      "abstract": "A rapidly growing number of applications rely on a small set of closed-source\nlanguage models (LMs). This dependency might introduce novel security risks if\nLMs develop self-recognition capabilities. Inspired by human identity\nverification methods, we propose a novel approach for assessing\nself-recognition in LMs using model-generated \"security questions\". Our test\ncan be externally administered to keep track of frontier models as it does not\nrequire access to internal model parameters or output probabilities. We use our\ntest to examine self-recognition in ten of the most capable open- and\nclosed-source LMs currently publicly available. Our extensive experiments found\nno empirical evidence of general or consistent self-recognition in any examined\nLM. Instead, our results suggest that given a set of alternatives, LMs seek to\npick the \"best\" answer, regardless of its origin. Moreover, we find indications\nthat preferences about which models produce the best answers are consistent\nacross LMs. We additionally uncover novel insights on position bias\nconsiderations for LMs in multiple-choice settings.",
      "upvotes": 24
    },
    {
      "title": "SEED-Story: Multimodal Long Story Generation with Large Language Model",
      "url": "https://huggingface.co/papers/2407.08683",
      "authors": [
        "Yang Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08683.pdf",
      "abstract": "With the remarkable advancements in image generation and open-form text\ngeneration, the creation of interleaved image-text content has become an\nincreasingly intriguing field. Multimodal story generation, characterized by\nproducing narrative texts and vivid images in an interleaved manner, has\nemerged as a valuable and practical task with broad applications. However, this\ntask poses significant challenges, as it necessitates the comprehension of the\ncomplex interplay between texts and images, and the ability to generate long\nsequences of coherent, contextually relevant texts and visuals. In this work,\nwe propose SEED-Story, a novel method that leverages a Multimodal Large\nLanguage Model (MLLM) to generate extended multimodal stories. Our model, built\nupon the powerful comprehension capability of MLLM, predicts text tokens as\nwell as visual tokens, which are subsequently processed with an adapted visual\nde-tokenizer to produce images with consistent characters and styles. We\nfurther propose multimodal attention sink mechanism to enable the generation of\nstories with up to 25 sequences (only 10 for training) in a highly efficient\nautoregressive manner. Additionally, we present a large-scale and\nhigh-resolution dataset named StoryStream for training our model and\nquantitatively evaluating the task of multimodal story generation in various\naspects.",
      "upvotes": 22
    },
    {
      "title": "Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist",
      "url": "https://huggingface.co/papers/2407.08733",
      "authors": [
        "Derek F. Wong",
        "Kaizhu Huang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08733.pdf",
      "abstract": "Exceptional mathematical reasoning ability is one of the key features that\ndemonstrate the power of large language models (LLMs). How to comprehensively\ndefine and evaluate the mathematical abilities of LLMs, and even reflect the\nuser experience in real-world scenarios, has emerged as a critical issue.\nCurrent benchmarks predominantly concentrate on problem-solving capabilities,\nwhich presents a substantial risk of model overfitting and fails to accurately\nrepresent genuine mathematical reasoning abilities. In this paper, we argue\nthat if a model really understands a problem, it should be robustly and readily\napplied across a diverse array of tasks. Motivated by this, we introduce\nMATHCHECK, a well-designed checklist for testing task generalization and\nreasoning robustness, as well as an automatic tool to generate checklists\nefficiently. MATHCHECK includes multiple mathematical reasoning tasks and\nrobustness test types to facilitate a comprehensive evaluation of both\nmathematical reasoning ability and behavior testing. Utilizing MATHCHECK, we\ndevelop MATHCHECK-GSM and MATHCHECK-GEO to assess mathematical textual\nreasoning and multi-modal reasoning capabilities, respectively, serving as\nupgraded versions of benchmarks including GSM8k, GeoQA, UniGeo, and Geometry3K.\nWe adopt MATHCHECK-GSM and MATHCHECK-GEO to evaluate over 20 LLMs and 11 MLLMs,\nassessing their comprehensive mathematical reasoning abilities. Our results\ndemonstrate that while frontier LLMs like GPT-4o continue to excel in various\nabilities on the checklist, many other model families exhibit a significant\ndecline. Further experiments indicate that, compared to traditional math\nbenchmarks, MATHCHECK better reflects true mathematical abilities and\nrepresents mathematical intelligence more linearly, thereby supporting our\ndesign. On our MATHCHECK, we can easily conduct detailed behavior analysis to\ndeeply investigate models.",
      "upvotes": 20
    },
    {
      "title": "DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception",
      "url": "https://huggingface.co/papers/2407.08303",
      "authors": [
        "Xiaotong Li",
        "Ling-Yu Duan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08303.pdf",
      "abstract": "Existing Multimodal Large Language Models (MLLMs) increasingly emphasize\ncomplex understanding of various visual elements, including multiple objects,\ntext information, and spatial relations. Their development for comprehensive\nvisual perception hinges on the availability of high-quality image-text\ndatasets that offer diverse visual elements and throughout image descriptions.\nHowever, the scarcity of such hyper-detailed datasets currently hinders\nprogress within the MLLM community. The bottleneck stems from the limited\nperceptual capabilities of current caption engines, which fall short in\nproviding complete and accurate annotations. To facilitate the cutting-edge\nresearch of MLLMs on comprehensive vision perception, we thereby propose\nPerceptual Fusion, using a low-budget but highly effective caption engine for\ncomplete and accurate image descriptions. Specifically, Perceptual Fusion\nintegrates diverse perception experts as image priors to provide explicit\ninformation on visual elements and adopts an efficient MLLM as a centric pivot\nto mimic advanced MLLMs' perception abilities. We carefully select 1M highly\nrepresentative images from uncurated LAION dataset and generate dense\ndescriptions using our engine, dubbed DenseFusion-1M. Extensive experiments\nvalidate that our engine outperforms its counterparts, where the resulting\ndataset significantly improves the perception and cognition abilities of\nexisting MLLMs across diverse vision-language benchmarks, especially with\nhigh-resolution images as inputs. The dataset and code are publicly available\nat https://github.com/baaivision/DenseFusion.",
      "upvotes": 17
    },
    {
      "title": "GTA: A Benchmark for General Tool Agents",
      "url": "https://huggingface.co/papers/2407.08713",
      "authors": [
        "Cailian Chen",
        "Kai Chen",
        "Xinyi Le"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08713.pdf",
      "abstract": "Significant focus has been placed on integrating large language models (LLMs)\nwith various tools in developing general-purpose agents. This poses a challenge\nto LLMs' tool-use capabilities. However, there are evident gaps between\nexisting tool-use evaluations and real-world scenarios. Current evaluations\noften use AI-generated queries, single-step tasks, dummy tools, and text-only\ninteractions, failing to reveal the agents' real-world problem-solving\nabilities effectively. To address this, we propose GTA, a benchmark for General\nTool Agents, featuring three main aspects: (i) Real user queries: human-written\nqueries with simple real-world objectives but implicit tool-use, requiring the\nLLM to reason the suitable tools and plan the solution steps. (ii) Real\ndeployed tools: an evaluation platform equipped with tools across perception,\noperation, logic, and creativity categories to evaluate the agents' actual task\nexecution performance. (iii) Real multimodal inputs: authentic image files,\nsuch as spatial scenes, web page screenshots, tables, code snippets, and\nprinted/handwritten materials, used as the query contexts to align with\nreal-world scenarios closely. We design 229 real-world tasks and executable\ntool chains to evaluate mainstream LLMs. Our findings show that real-world user\nqueries are challenging for existing LLMs, with GPT-4 completing less than 50%\nof the tasks and most LLMs achieving below 25%. This evaluation reveals the\nbottlenecks in the tool-use capabilities of current LLMs in real-world\nscenarios, which provides future direction for advancing general-purpose tool\nagents. The code and dataset are available at\nhttps://github.com/open-compass/GTA.",
      "upvotes": 14
    },
    {
      "title": "Autoregressive Speech Synthesis without Vector Quantization",
      "url": "https://huggingface.co/papers/2407.08551",
      "authors": [
        "Bing Han",
        "Sheng Zhao",
        "Xixin Wu",
        "Helen Meng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08551.pdf",
      "abstract": "We present MELLE, a novel continuous-valued tokens based language modeling\napproach for text to speech synthesis (TTS). MELLE autoregressively generates\ncontinuous mel-spectrogram frames directly from text condition, bypassing the\nneed for vector quantization, which are originally designed for audio\ncompression and sacrifice fidelity compared to mel-spectrograms. Specifically,\n(i) instead of cross-entropy loss, we apply regression loss with a proposed\nspectrogram flux loss function to model the probability distribution of the\ncontinuous-valued tokens. (ii) we have incorporated variational inference into\nMELLE to facilitate sampling mechanisms, thereby enhancing the output diversity\nand model robustness. Experiments demonstrate that, compared to the two-stage\ncodec language models VALL-E and its variants, the single-stage MELLE mitigates\nrobustness issues by avoiding the inherent flaws of sampling discrete codes,\nachieves superior performance across multiple metrics, and, most importantly,\noffers a more streamlined paradigm. See https://aka.ms/melle for demos of our\nwork.",
      "upvotes": 14
    },
    {
      "title": "Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models",
      "url": "https://huggingface.co/papers/2407.08701",
      "authors": [
        "Mohamed Elgharib",
        "Christian Theobalt",
        "Kai Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08701.pdf",
      "abstract": "Large Language Models have shown remarkable efficacy in generating streaming\ndata such as text and audio, thanks to their temporally uni-directional\nattention mechanism, which models correlations between the current token and\nprevious tokens. However, video streaming remains much less explored, despite a\ngrowing need for live video processing. State-of-the-art video diffusion models\nleverage bi-directional temporal attention to model the correlations between\nthe current frame and all the surrounding (i.e. including future) frames, which\nhinders them from processing streaming videos. To address this problem, we\npresent Live2Diff, the first attempt at designing a video diffusion model with\nuni-directional temporal attention, specifically targeting live streaming video\ntranslation. Compared to previous works, our approach ensures temporal\nconsistency and smoothness by correlating the current frame with its\npredecessors and a few initial warmup frames, without any future frames.\nAdditionally, we use a highly efficient denoising scheme featuring a KV-cache\nmechanism and pipelining, to facilitate streaming video translation at\ninteractive framerates. Extensive experiments demonstrate the effectiveness of\nthe proposed attention mechanism and pipeline, outperforming previous methods\nin terms of temporal smoothness and/or efficiency.",
      "upvotes": 10
    },
    {
      "title": "The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective",
      "url": "https://huggingface.co/papers/2407.08583",
      "authors": [
        "Wenhao Zhang",
        "Yilun Huang",
        "Bolin Ding",
        "Yaliang Li",
        "Shuiguang Deng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08583.pdf",
      "abstract": "The rapid development of large language models (LLMs) has been witnessed in\nrecent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the\nmodality from text to a broader spectrum of domains, attracting widespread\nattention due to the broader range of application scenarios. As LLMs and MLLMs\nrely on vast amounts of model parameters and data to achieve emergent\ncapabilities, the importance of data is receiving increasingly widespread\nattention and recognition. Tracing and analyzing recent data-oriented works for\nMLLMs, we find that the development of models and data is not two separate\npaths but rather interconnected. On the one hand, vaster and higher-quality\ndata contribute to better performance of MLLMs, on the other hand, MLLMs can\nfacilitate the development of data. The co-development of multi-modal data and\nMLLMs requires a clear view of 1) at which development stage of MLLMs can\nspecific data-centric approaches be employed to enhance which capabilities, and\n2) by utilizing which capabilities and acting as which roles can models\ncontribute to multi-modal data. To promote the data-model co-development for\nMLLM community, we systematically review existing works related to MLLMs from\nthe data-model co-development perspective. A regularly maintained project\nassociated with this survey is accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.",
      "upvotes": 10
    },
    {
      "title": "Gradient Boosting Reinforcement Learning",
      "url": "https://huggingface.co/papers/2407.08250",
      "authors": [
        "Chen Tessler"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08250.pdf",
      "abstract": "Neural networks (NN) achieve remarkable results in various tasks, but lack\nkey characteristics: interpretability, support for categorical features, and\nlightweight implementations suitable for edge devices. While ongoing efforts\naim to address these challenges, Gradient Boosting Trees (GBT) inherently meet\nthese requirements. As a result, GBTs have become the go-to method for\nsupervised learning tasks in many real-world applications and competitions.\nHowever, their application in online learning scenarios, notably in\nreinforcement learning (RL), has been limited. In this work, we bridge this gap\nby introducing Gradient-Boosting RL (GBRL), a framework that extends the\nadvantages of GBT to the RL domain. Using the GBRL framework, we implement\nvarious actor-critic algorithms and compare their performance with their NN\ncounterparts. Inspired by shared backbones in NN we introduce a tree-sharing\napproach for policy and value functions with distinct learning rates, enhancing\nlearning efficiency over millions of interactions. GBRL achieves competitive\nperformance across a diverse array of tasks, excelling in domains with\nstructured or categorical features. Additionally, we present a\nhigh-performance, GPU-accelerated implementation that integrates seamlessly\nwith widely-used RL libraries (available at https://github.com/NVlabs/gbrl).\nGBRL expands the toolkit for RL practitioners, demonstrating the viability and\npromise of GBT within the RL paradigm, particularly in domains characterized by\nstructured or categorical features.",
      "upvotes": 10
    },
    {
      "title": "Towards Building Specialized Generalist AI with System 1 and System 2 Fusion",
      "url": "https://huggingface.co/papers/2407.08642",
      "authors": [
        "Bowen Zhou"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08642.pdf",
      "abstract": "In this perspective paper, we introduce the concept of Specialized Generalist\nArtificial Intelligence (SGAI or simply SGI) as a crucial milestone toward\nArtificial General Intelligence (AGI). Compared to directly scaling general\nabilities, SGI is defined as AI that specializes in at least one task,\nsurpassing human experts, while also retaining general abilities. This fusion\npath enables SGI to rapidly achieve high-value areas. We categorize SGI into\nthree stages based on the level of mastery over professional skills and\ngenerality performance. Additionally, we discuss the necessity of SGI in\naddressing issues associated with large language models, such as their\ninsufficient generality, specialized capabilities, uncertainty in innovation,\nand practical applications. Furthermore, we propose a conceptual framework for\ndeveloping SGI that integrates the strengths of Systems 1 and 2 cognitive\nprocessing. This framework comprises three layers and four key components,\nwhich focus on enhancing individual abilities and facilitating collaborative\nevolution. We conclude by summarizing the potential challenges and suggesting\nfuture directions. We hope that the proposed SGI will provide insights into\nfurther research and applications towards achieving AGI.",
      "upvotes": 9
    },
    {
      "title": "Map It Anywhere (MIA): Empowering Bird's Eye View Mapping using Large-scale Public Data",
      "url": "https://huggingface.co/papers/2407.08726",
      "authors": [
        "Sai Mitheran Jagadesh Kumar",
        "Katia Sycara",
        "Sebastian Scherer"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08726.pdf",
      "abstract": "Top-down Bird's Eye View (BEV) maps are a popular representation for ground\nrobot navigation due to their richness and flexibility for downstream tasks.\nWhile recent methods have shown promise for predicting BEV maps from\nFirst-Person View (FPV) images, their generalizability is limited to small\nregions captured by current autonomous vehicle-based datasets. In this context,\nwe show that a more scalable approach towards generalizable map prediction can\nbe enabled by using two large-scale crowd-sourced mapping platforms, Mapillary\nfor FPV images and OpenStreetMap for BEV semantic maps. We introduce Map It\nAnywhere (MIA), a data engine that enables seamless curation and modeling of\nlabeled map prediction data from existing open-source map platforms. Using our\nMIA data engine, we display the ease of automatically collecting a dataset of\n1.2 million pairs of FPV images & BEV maps encompassing diverse geographies,\nlandscapes, environmental factors, camera models & capture scenarios. We\nfurther train a simple camera model-agnostic model on this data for BEV map\nprediction. Extensive evaluations using established benchmarks and our dataset\nshow that the data curated by MIA enables effective pretraining for\ngeneralizable BEV map prediction, with zero-shot performance far exceeding\nbaselines trained on existing datasets by 35%. Our analysis highlights the\npromise of using large-scale public maps for developing & testing generalizable\nBEV perception, paving the way for more robust autonomous navigation.",
      "upvotes": 8
    },
    {
      "title": "Generalizable Implicit Motion Modeling for Video Frame Interpolation",
      "url": "https://huggingface.co/papers/2407.08680",
      "authors": [
        "Wei Li",
        "Chen Change Loy"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08680.pdf",
      "abstract": "Motion modeling is critical in flow-based Video Frame Interpolation (VFI).\nExisting paradigms either consider linear combinations of bidirectional flows\nor directly predict bilateral flows for given timestamps without exploring\nfavorable motion priors, thus lacking the capability of effectively modeling\nspatiotemporal dynamics in real-world videos. To address this limitation, in\nthis study, we introduce Generalizable Implicit Motion Modeling (GIMM), a novel\nand effective approach to motion modeling for VFI. Specifically, to enable GIMM\nas an effective motion modeling paradigm, we design a motion encoding pipeline\nto model spatiotemporal motion latent from bidirectional flows extracted from\npre-trained flow estimators, effectively representing input-specific motion\npriors. Then, we implicitly predict arbitrary-timestep optical flows within two\nadjacent input frames via an adaptive coordinate-based neural network, with\nspatiotemporal coordinates and motion latent as inputs. Our GIMM can be\nsmoothly integrated with existing flow-based VFI works without further\nmodifications. We show that GIMM performs better than the current state of the\nart on the VFI benchmarks.",
      "upvotes": 8
    },
    {
      "title": "WildGaussians: 3D Gaussian Splatting in the Wild",
      "url": "https://huggingface.co/papers/2407.08447",
      "authors": [
        "Zuzana Kukelova",
        "Marc Pollefeys"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08447.pdf",
      "abstract": "While the field of 3D scene reconstruction is dominated by NeRFs due to their\nphotorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged,\noffering similar quality with real-time rendering speeds. However, both methods\nprimarily excel with well-controlled 3D scenes, while in-the-wild data -\ncharacterized by occlusions, dynamic objects, and varying illumination -\nremains challenging. NeRFs can adapt to such conditions easily through\nper-image embedding vectors, but 3DGS struggles due to its explicit\nrepresentation and lack of shared parameters. To address this, we introduce\nWildGaussians, a novel approach to handle occlusions and appearance changes\nwith 3DGS. By leveraging robust DINO features and integrating an appearance\nmodeling module within 3DGS, our method achieves state-of-the-art results. We\ndemonstrate that WildGaussians matches the real-time rendering speed of 3DGS\nwhile surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all\nwithin a simple architectural framework.",
      "upvotes": 8
    },
    {
      "title": "OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects",
      "url": "https://huggingface.co/papers/2407.08711",
      "authors": [
        "Kevis-Kokitsi Maninis",
        "James Hays",
        "Matthew Brown"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08711.pdf",
      "abstract": "We propose OmniNOCS, a large-scale monocular dataset with 3D Normalized\nObject Coordinate Space (NOCS) maps, object masks, and 3D bounding box\nannotations for indoor and outdoor scenes. OmniNOCS has 20 times more object\nclasses and 200 times more instances than existing NOCS datasets (NOCS-Real275,\nWild6D). We use OmniNOCS to train a novel, transformer-based monocular NOCS\nprediction model (NOCSformer) that can predict accurate NOCS, instance masks\nand poses from 2D object detections across diverse classes. It is the first\nNOCS model that can generalize to a broad range of classes when prompted with\n2D boxes. We evaluate our model on the task of 3D oriented bounding box\nprediction, where it achieves comparable results to state-of-the-art 3D\ndetection methods such as Cube R-CNN. Unlike other 3D detection methods, our\nmodel also provides detailed and accurate 3D object shape and segmentation. We\npropose a novel benchmark for the task of NOCS prediction based on OmniNOCS,\nwhich we hope will serve as a useful baseline for future work in this area. Our\ndataset and code will be at the project website: https://omninocs.github.io.",
      "upvotes": 5
    },
    {
      "title": "Scaling Up Personalized Aesthetic Assessment via Task Vector Customization",
      "url": "https://huggingface.co/papers/2407.07176",
      "authors": [
        "Jaegul Choo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.07176.pdf",
      "abstract": "The task of personalized image aesthetic assessment seeks to tailor aesthetic\nscore prediction models to match individual preferences with just a few\nuser-provided inputs. However, the scalability and generalization capabilities\nof current approaches are considerably restricted by their reliance on an\nexpensive curated database. To overcome this long-standing scalability\nchallenge, we present a unique approach that leverages readily available\ndatabases for general image aesthetic assessment and image quality assessment.\nSpecifically, we view each database as a distinct image score regression task\nthat exhibits varying degrees of personalization potential. By determining\noptimal combinations of task vectors, known to represent specific traits of\neach database, we successfully create personalized models for individuals. This\napproach of integrating multiple models allows us to harness a substantial\namount of data. Our extensive experiments demonstrate the effectiveness of our\napproach in generalizing to previously unseen domains-a challenge previous\napproaches have struggled to achieve-making it highly applicable to real-world\nscenarios. Our novel approach significantly advances the field by offering\nscalable solutions for personalized aesthetic assessment and establishing high\nstandards for future research.\nhttps://yeolj00.github.io/personal-projects/personalized-aesthetics/",
      "upvotes": 3
    }
  ]
}