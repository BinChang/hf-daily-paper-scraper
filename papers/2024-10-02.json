{
  "date": "2024-10-02",
  "papers": [
    {
      "title": "Law of the Weakest Link: Cross Capabilities of Large Language Models",
      "url": "https://huggingface.co/papers/2409.19951",
      "authors": [
        "Chenguang Zhu",
        "Zhengxing Chen",
        "Liang Tan",
        "Sharan Narang",
        "Melanie Kambadur"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.19951.pdf",
      "abstract": "The development and evaluation of Large Language Models (LLMs) have largely\nfocused on individual capabilities. However, this overlooks the intersection of\nmultiple abilities across different types of expertise that are often required\nfor real-world tasks, which we term cross capabilities. To systematically\nexplore this concept, we first define seven core individual capabilities and\nthen pair them to form seven common cross capabilities, each supported by a\nmanually constructed taxonomy. Building on these definitions, we introduce\nCrossEval, a benchmark comprising 1,400 human-annotated prompts, with 100\nprompts for each individual and cross capability. To ensure reliable\nevaluation, we involve expert annotators to assess 4,200 model responses,\ngathering 8,400 human ratings with detailed explanations to serve as reference\nexamples. Our findings reveal that, in both static evaluations and attempts to\nenhance specific abilities, current LLMs consistently exhibit the \"Law of the\nWeakest Link,\" where cross-capability performance is significantly constrained\nby the weakest component. Specifically, across 58 cross-capability scores from\n17 models, 38 scores are lower than all individual capabilities, while 20 fall\nbetween strong and weak, but closer to the weaker ability. These results\nhighlight the under-performance of LLMs in cross-capability tasks, making the\nidentification and improvement of the weakest capabilities a critical priority\nfor future research to optimize performance in complex, multi-dimensional\nscenarios.",
      "upvotes": 53
    },
    {
      "title": "TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices",
      "url": "https://huggingface.co/papers/2410.00531",
      "authors": [
        "Mohsen Guizani",
        "Hongfang Yu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00531.pdf",
      "abstract": "Large model inference is shifting from cloud to edge due to concerns about\nthe privacy of user interaction data. However, edge devices often struggle with\nlimited computing power, memory, and bandwidth, requiring collaboration across\nmultiple devices to run and speed up LLM inference. Pipeline parallelism, the\nmainstream solution, is inefficient for single-user scenarios, while tensor\nparallelism struggles with frequent communications. In this paper, we argue\nthat tensor parallelism can be more effective than pipeline on low-resource\ndevices, and present a compute- and memory-efficient tensor parallel inference\nsystem, named TPI-LLM, to serve 70B-scale models. TPI-LLM keeps sensitive raw\ndata local in the users' devices and introduces a sliding window memory\nscheduler to dynamically manage layer weights during inference, with disk I/O\nlatency overlapped with the computation and communication. This allows larger\nmodels to run smoothly on memory-limited devices. We analyze the communication\nbottleneck and find that link latency, not bandwidth, emerges as the main\nissue, so a star-based allreduce algorithm is implemented. Through extensive\nexperiments on both emulated and real testbeds, TPI-LLM demonstrated over 80%\nless time-to-first-token and token latency compared to Accelerate, and over 90%\ncompared to Transformers and Galaxy, while cutting the peak memory footprint of\nLlama 2-70B by 90%, requiring only 3.1 GB of memory for 70B-scale models.",
      "upvotes": 28
    },
    {
      "title": "Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect",
      "url": "https://huggingface.co/papers/2409.17912",
      "authors": [
        "Yassine Abbahaddou",
        "Sofiane Ennadir",
        "Xuguang Ren",
        "Eric Moulines",
        "Michalis Vazirgiannis"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.17912.pdf",
      "abstract": "We introduce Atlas-Chat, the first-ever collection of large language models\nspecifically developed for dialectal Arabic. Focusing on Moroccan Arabic, also\nknown as Darija, we construct our instruction dataset by consolidating existing\nDarija language resources, creating novel datasets both manually and\nsynthetically, and translating English instructions with stringent quality\ncontrol. Atlas-Chat-9B and 2B models, fine-tuned on the dataset, exhibit\nsuperior ability in following Darija instructions and performing standard NLP\ntasks. Notably, our models outperform both state-of-the-art and\nArabic-specialized LLMs like LLaMa, Jais, and AceGPT, e.g., achieving a 13%\nperformance boost over a larger 13B model on DarijaMMLU, in our newly\nintroduced evaluation suite for Darija covering both discriminative and\ngenerative tasks. Furthermore, we perform an experimental analysis of various\nfine-tuning strategies and base model choices to determine optimal\nconfigurations. All our resources are publicly accessible, and we believe our\nwork offers comprehensive design methodologies of instruction-tuning for\nlow-resource language variants, which are often neglected in favor of data-rich\nlanguages by contemporary LLMs.",
      "upvotes": 20
    },
    {
      "title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos",
      "url": "https://huggingface.co/papers/2409.19603",
      "authors": [
        "Lei Liu",
        "Zheng Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.19603.pdf",
      "abstract": "We introduce VideoLISA, a video-based multimodal large language model\ndesigned to tackle the problem of language-instructed reasoning segmentation in\nvideos. Leveraging the reasoning capabilities and world knowledge of large\nlanguage models, and augmented by the Segment Anything Model, VideoLISA\ngenerates temporally consistent segmentation masks in videos based on language\ninstructions. Existing image-based methods, such as LISA, struggle with video\ntasks due to the additional temporal dimension, which requires temporal dynamic\nunderstanding and consistent segmentation across frames. VideoLISA addresses\nthese challenges by integrating a Sparse Dense Sampling strategy into the\nvideo-LLM, which balances temporal context and spatial detail within\ncomputational constraints. Additionally, we propose a One-Token-Seg-All\napproach using a specially designed <TRK> token, enabling the model to segment\nand track objects across multiple frames. Extensive evaluations on diverse\nbenchmarks, including our newly introduced ReasonVOS benchmark, demonstrate\nVideoLISA's superior performance in video object segmentation tasks involving\ncomplex reasoning, temporal understanding, and object tracking. While optimized\nfor videos, VideoLISA also shows promising generalization to image\nsegmentation, revealing its potential as a unified foundation model for\nlanguage-instructed object segmentation. Code and model will be available at:\nhttps://github.com/showlab/VideoLISA.",
      "upvotes": 17
    },
    {
      "title": "Flex3D: Feed-Forward 3D Generation With Flexible Reconstruction Model And Input View Curation",
      "url": "https://huggingface.co/papers/2410.00890",
      "authors": [
        "Andrea Vedaldi",
        "Filippos Kokkinos"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00890.pdf",
      "abstract": "Generating high-quality 3D content from text, single images, or sparse view\nimages remains a challenging task with broad applications.Existing methods\ntypically employ multi-view diffusion models to synthesize multi-view images,\nfollowed by a feed-forward process for 3D reconstruction. However, these\napproaches are often constrained by a small and fixed number of input views,\nlimiting their ability to capture diverse viewpoints and, even worse, leading\nto suboptimal generation results if the synthesized views are of poor quality.\nTo address these limitations, we propose Flex3D, a novel two-stage framework\ncapable of leveraging an arbitrary number of high-quality input views. The\nfirst stage consists of a candidate view generation and curation pipeline. We\nemploy a fine-tuned multi-view image diffusion model and a video diffusion\nmodel to generate a pool of candidate views, enabling a rich representation of\nthe target 3D object. Subsequently, a view selection pipeline filters these\nviews based on quality and consistency, ensuring that only the high-quality and\nreliable views are used for reconstruction. In the second stage, the curated\nviews are fed into a Flexible Reconstruction Model (FlexRM), built upon a\ntransformer architecture that can effectively process an arbitrary number of\ninputs. FlemRM directly outputs 3D Gaussian points leveraging a tri-plane\nrepresentation, enabling efficient and detailed 3D generation. Through\nextensive exploration of design and training strategies, we optimize FlexRM to\nachieve superior performance in both reconstruction and generation tasks. Our\nresults demonstrate that Flex3D achieves state-of-the-art performance, with a\nuser study winning rate of over 92% in 3D generation tasks when compared to\nseveral of the latest feed-forward 3D generative models.",
      "upvotes": 17
    },
    {
      "title": "Illustrious: an Open Advanced Illustration Model",
      "url": "https://huggingface.co/papers/2409.19946",
      "authors": [
        "Junha Lee",
        "Dongha Kim",
        "Min Song"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.19946.pdf",
      "abstract": "In this work, we share the insights for achieving state-of-the-art quality in\nour text-to-image anime image generative model, called Illustrious. To achieve\nhigh resolution, dynamic color range images, and high restoration ability, we\nfocus on three critical approaches for model improvement. First, we delve into\nthe significance of the batch size and dropout control, which enables faster\nlearning of controllable token based concept activations. Second, we increase\nthe training resolution of images, affecting the accurate depiction of\ncharacter anatomy in much higher resolution, extending its generation\ncapability over 20MP with proper methods. Finally, we propose the refined\nmulti-level captions, covering all tags and various natural language captions\nas a critical factor for model development. Through extensive analysis and\nexperiments, Illustrious demonstrates state-of-the-art performance in terms of\nanimation style, outperforming widely-used models in illustration domains,\npropelling easier customization and personalization with nature of open source.\nWe plan to publicly release updated Illustrious model series sequentially as\nwell as sustainable plans for improvements.",
      "upvotes": 13
    },
    {
      "title": "SyntheOcc: Synthesize Geometric-Controlled Street View Images through 3D Semantic MPIs",
      "url": "https://huggingface.co/papers/2410.00337",
      "authors": [
        "Weichao Qiu",
        "Yingjie Cai",
        "Xu Yan",
        "Qing Lian",
        "Bingbing Liu",
        "Ying-Cong Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00337.pdf",
      "abstract": "The advancement of autonomous driving is increasingly reliant on high-quality\nannotated datasets, especially in the task of 3D occupancy prediction, where\nthe occupancy labels require dense 3D annotation with significant human effort.\nIn this paper, we propose SyntheOcc, which denotes a diffusion model that\nSynthesize photorealistic and geometric-controlled images by conditioning\nOccupancy labels in driving scenarios. This yields an unlimited amount of\ndiverse, annotated, and controllable datasets for applications like training\nperception models and simulation. SyntheOcc addresses the critical challenge of\nhow to efficiently encode 3D geometric information as conditional input to a 2D\ndiffusion model. Our approach innovatively incorporates 3D semantic multi-plane\nimages (MPIs) to provide comprehensive and spatially aligned 3D scene\ndescriptions for conditioning. As a result, SyntheOcc can generate\nphotorealistic multi-view images and videos that faithfully align with the\ngiven geometric labels (semantics in 3D voxel space). Extensive qualitative and\nquantitative evaluations of SyntheOcc on the nuScenes dataset prove its\neffectiveness in generating controllable occupancy datasets that serve as an\neffective data augmentation to perception models.",
      "upvotes": 10
    },
    {
      "title": "ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer",
      "url": "https://huggingface.co/papers/2410.00086",
      "authors": [
        "Yu Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00086.pdf",
      "abstract": "Diffusion models have emerged as a powerful generative technology and have\nbeen found to be applicable in various scenarios. Most existing foundational\ndiffusion models are primarily designed for text-guided visual generation and\ndo not support multi-modal conditions, which are essential for many visual\nediting tasks. This limitation prevents these foundational diffusion models\nfrom serving as a unified model in the field of visual generation, like GPT-4\nin the natural language processing field. In this work, we propose ACE, an\nAll-round Creator and Editor, which achieves comparable performance compared to\nthose expert models in a wide range of visual generation tasks. To achieve this\ngoal, we first introduce a unified condition format termed Long-context\nCondition Unit (LCU), and propose a novel Transformer-based diffusion model\nthat uses LCU as input, aiming for joint training across various generation and\nediting tasks. Furthermore, we propose an efficient data collection approach to\naddress the issue of the absence of available training data. It involves\nacquiring pairwise images with synthesis-based or clustering-based pipelines\nand supplying these pairs with accurate textual instructions by leveraging a\nfine-tuned multi-modal large language model. To comprehensively evaluate the\nperformance of our model, we establish a benchmark of manually annotated pairs\ndata across a variety of visual generation tasks. The extensive experimental\nresults demonstrate the superiority of our model in visual generation fields.\nThanks to the all-in-one capabilities of our model, we can easily build a\nmulti-modal chat system that responds to any interactive request for image\ncreation using a single model to serve as the backend, avoiding the cumbersome\npipeline typically employed in visual agents. Code and models will be available\non the project page: https://ali-vilab.github.io/ace-page/.",
      "upvotes": 10
    },
    {
      "title": "Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration",
      "url": "https://huggingface.co/papers/2410.00418",
      "authors": [
        "Michael Elad"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00418.pdf",
      "abstract": "Photo-realistic image restoration algorithms are typically evaluated by\ndistortion measures (e.g., PSNR, SSIM) and by perceptual quality measures\n(e.g., FID, NIQE), where the desire is to attain the lowest possible distortion\nwithout compromising on perceptual quality. To achieve this goal, current\nmethods typically attempt to sample from the posterior distribution, or to\noptimize a weighted sum of a distortion loss (e.g., MSE) and a perceptual\nquality loss (e.g., GAN). Unlike previous works, this paper is concerned\nspecifically with the optimal estimator that minimizes the MSE under a\nconstraint of perfect perceptual index, namely where the distribution of the\nreconstructed images is equal to that of the ground-truth ones. A recent\ntheoretical result shows that such an estimator can be constructed by optimally\ntransporting the posterior mean prediction (MMSE estimate) to the distribution\nof the ground-truth images. Inspired by this result, we introduce\nPosterior-Mean Rectified Flow (PMRF), a simple yet highly effective algorithm\nthat approximates this optimal estimator. In particular, PMRF first predicts\nthe posterior mean, and then transports the result to a high-quality image\nusing a rectified flow model that approximates the desired optimal transport\nmap. We investigate the theoretical utility of PMRF and demonstrate that it\nconsistently outperforms previous methods on a variety of image restoration\ntasks.",
      "upvotes": 9
    },
    {
      "title": "Visual Context Window Extension: A New Perspective for Long Video Understanding",
      "url": "https://huggingface.co/papers/2409.20018",
      "authors": [
        "Zhenzhong Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.20018.pdf",
      "abstract": "Large Multimodal Models (LMMs) have demonstrated impressive performance in\nshort video understanding tasks but face great challenges when applied to long\nvideo understanding. In contrast, Large Language Models (LLMs) exhibit\noutstanding capabilities in modeling long texts. Existing work attempts to\naddress this issue by introducing long video-text pairs during training.\nHowever, these approaches require substantial computational and data resources.\nIn this paper, we tackle the challenge of long video understanding from the\nperspective of context windows, aiming to apply LMMs to long video tasks\nwithout retraining on long video datasets. We first conduct an in-depth\nanalysis of why pretrained LMMs struggle to understand lengthy video content,\nidentifying that discrepancies between visual and language modalities lead to\ndifferent context windows for visual and language tokens, making it difficult\nto directly extend the visual tokens to match the language context window.\nBased on this, we propose to adapt LMMs for long video understanding tasks by\nextending the visual context window, eliminating the need for retraining on\nlarge scalelong video datasets. To further mitigate the significant memory\nconsumption caused by long sequences, we introduce a progressive pooling\ninference strategy that selectively adjusts the spatial resolution of frame\nembeddings, reducing the number of visual tokens while retaining important\nspatial information. Across multiple long video understanding benchmarks, our\nmethod consistently improves the performance as the number of video frames\nincreases. On the MLVU benchmark, our method outperforms GPT-4o, even though\nour model size is only 7B. Additionally, in the 256-frame setting, our method\nreduces memory usage by approximately 45% compared to the baseline, without\nintroducing any performance loss.",
      "upvotes": 8
    },
    {
      "title": "DressRecon: Freeform 4D Human Reconstruction from Monocular Video",
      "url": "https://huggingface.co/papers/2409.20563",
      "authors": [
        "Donglai Xiang",
        "Shubham Tulsiani"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.20563.pdf",
      "abstract": "We present a method to reconstruct time-consistent human body models from\nmonocular videos, focusing on extremely loose clothing or handheld object\ninteractions. Prior work in human reconstruction is either limited to tight\nclothing with no object interactions, or requires calibrated multi-view\ncaptures or personalized template scans which are costly to collect at scale.\nOur key insight for high-quality yet flexible reconstruction is the careful\ncombination of generic human priors about articulated body shape (learned from\nlarge-scale training data) with video-specific articulated \"bag-of-bones\"\ndeformation (fit to a single video via test-time optimization). We accomplish\nthis by learning a neural implicit model that disentangles body versus clothing\ndeformations as separate motion model layers. To capture subtle geometry of\nclothing, we leverage image-based priors such as human body pose, surface\nnormals, and optical flow during optimization. The resulting neural fields can\nbe extracted into time-consistent meshes, or further optimized as explicit 3D\nGaussians for high-fidelity interactive rendering. On datasets with highly\nchallenging clothing deformations and object interactions, DressRecon yields\nhigher-fidelity 3D reconstructions than prior art. Project page:\nhttps://jefftan969.github.io/dressrecon/",
      "upvotes": 7
    },
    {
      "title": "Helpful DoggyBot: Open-World Object Fetching using Legged Robots and Vision-Language Models",
      "url": "https://huggingface.co/papers/2410.00231",
      "authors": [
        "Zipeng Fu",
        "Xuxin Cheng",
        "Xiaolong Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00231.pdf",
      "abstract": "Learning-based methods have achieved strong performance for quadrupedal\nlocomotion. However, several challenges prevent quadrupeds from learning\nhelpful indoor skills that require interaction with environments and humans:\nlack of end-effectors for manipulation, limited semantic understanding using\nonly simulation data, and low traversability and reachability in indoor\nenvironments. We present a system for quadrupedal mobile manipulation in indoor\nenvironments. It uses a front-mounted gripper for object manipulation, a\nlow-level controller trained in simulation using egocentric depth for agile\nskills like climbing and whole-body tilting, and pre-trained vision-language\nmodels (VLMs) with a third-person fisheye and an egocentric RGB camera for\nsemantic understanding and command generation. We evaluate our system in two\nunseen environments without any real-world data collection or training. Our\nsystem can zero-shot generalize to these environments and complete tasks, like\nfollowing user's commands to fetch a randomly placed stuff toy after climbing\nover a queen-sized bed, with a 60% success rate. Project website:\nhttps://helpful-doggybot.github.io/",
      "upvotes": 6
    },
    {
      "title": "What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study",
      "url": "https://huggingface.co/papers/2410.00545",
      "authors": [
        "Ana Guerberof"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00545.pdf",
      "abstract": "Gender bias in machine translation (MT) is recognized as an issue that can\nharm people and society. And yet, advancements in the field rarely involve\npeople, the final MT users, or inform how they might be impacted by biased\ntechnologies. Current evaluations are often restricted to automatic methods,\nwhich offer an opaque estimate of what the downstream impact of gender\ndisparities might be. We conduct an extensive human-centered study to examine\nif and to what extent bias in MT brings harms with tangible costs, such as\nquality of service gaps across women and men. To this aim, we collect\nbehavioral data from 90 participants, who post-edited MT outputs to ensure\ncorrect gender translation. Across multiple datasets, languages, and types of\nusers, our study shows that feminine post-editing demands significantly more\ntechnical and temporal effort, also corresponding to higher financial costs.\nExisting bias measurements, however, fail to reflect the found disparities. Our\nfindings advocate for human-centered approaches that can inform the societal\nimpact of bias.",
      "upvotes": 5
    },
    {
      "title": "Embodied-RAG: General non-parametric Embodied Memory for Retrieval and Generation",
      "url": "https://huggingface.co/papers/2409.18313",
      "authors": [
        "So Yeon Min",
        "Tianyi Zhang",
        "Aarav Bajaj",
        "Ruslan Salakhutdinov",
        "Matthew Johnson-Roberson",
        "Yonatan Bisk"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.18313.pdf",
      "abstract": "There is no limit to how much a robot might explore and learn, but all of\nthat knowledge needs to be searchable and actionable. Within language research,\nretrieval augmented generation (RAG) has become the workhouse of large-scale\nnon-parametric knowledge, however existing techniques do not directly transfer\nto the embodied domain, which is multimodal, data is highly correlated, and\nperception requires abstraction.\n  To address these challenges, we introduce Embodied-RAG, a framework that\nenhances the foundational model of an embodied agent with a non-parametric\nmemory system capable of autonomously constructing hierarchical knowledge for\nboth navigation and language generation. Embodied-RAG handles a full range of\nspatial and semantic resolutions across diverse environments and query types,\nwhether for a specific object or a holistic description of ambiance. At its\ncore, Embodied-RAG's memory is structured as a semantic forest, storing\nlanguage descriptions at varying levels of detail. This hierarchical\norganization allows the system to efficiently generate context-sensitive\noutputs across different robotic platforms. We demonstrate that Embodied-RAG\neffectively bridges RAG to the robotics domain, successfully handling over 200\nexplanation and navigation queries across 19 environments, highlighting its\npromise for general-purpose non-parametric system for embodied agents.",
      "upvotes": 3
    }
  ]
}